{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer"]},"docs":[{"location":"","title":"CSCS Documentation","text":"<p>The Alps Research Infrastructure hosts multiple platforms and clusters targeting different communities. A good spot to get started is with the documentation for the platform that your project is running on.</p> <ul> <li> <p> Platforms</p> <p>Projects at CSCS are granted access to clusters, which are managed by platforms.</p> <p> HPC Platform (Daint, Eiger)</p> <p> Machine Learning Platform (Clariden, Bristen)</p> <p> Climate and Weather Platform (Santis)</p> <p>For an overview of the different platforms:</p> <p> Platforms overview</p> </li> </ul> <p>Alps is a general-purpose compute and data Research Infrastructure (RI) open to the broad community of researchers in Switzerland and the rest of the world. Find out more about Alps\u2026</p> <ul> <li> <p> Alps</p> <p>Learn more about the Alps research infrastructure</p> <p> Alps Overview</p> <p>Get detailed information about the main components of the infrastructure</p> <p> Alps Clusters</p> <p> Alps Hardware</p> <p> Alps Storage</p> </li> <li> <p> Logging In</p> <p>Once you have an account, you can set up multi factor authentication:</p> <p> Setting up MFA</p> <p>Then access CSCS services:</p> <p> Accessing CSCS Web Services</p> <p> Using SSH</p> <p> FirecREST API</p> </li> </ul> <ul> <li> <p> Accounts and Projects</p> <p>If you are a new user, or working on a project for the first time:</p> <p> Accounts and Projects</p> </li> </ul> <ul> <li><p>Visit status.cscs.ch for the status of Alps systems, and the latest announcements.</p></li> </ul>"},{"location":"#tutorials-and-guides","title":"Tutorials and Guides","text":"<p>Learn by doing with our guides and tutorials.</p> <ul> <li> <p> Tutorials</p> <p>Hands on tutorials that show how to implement workflows on Alps.</p> <p> Machine Learning</p> </li> <li> <p> Guides</p> <p>Guides with practical advice, hints and tips for key topics.</p> <p> Using storage effectively</p> <p> Accessing internet and external services</p> </li> </ul>"},{"location":"#tools-and-services","title":"Tools and Services","text":"<ul> <li> <p> Services</p> <p>CSCS provides services and software on Alps.</p> <p> Services Overview</p> <p>Learn about individual services</p> <p> Developer Portal</p> <p> CI/CD for external projects</p> </li> <li> <p> Software</p> <p>CSCS provides applications, programming environments and tools.</p> <p> Scientific applications</p> <p> Programming environments</p> <p> Development tools</p> <p>Find out how to use uenv and containers to access software</p> <p> uenv</p> <p> Container engine</p> </li> <li> <p> Data management and storage</p> <p>CSCS provides data management and storage services</p> <p> File systems</p> <p> Data transfer</p> <p> Long term storage</p> <p> Object storage</p> </li> </ul> <p></p>"},{"location":"#get-in-touch","title":"Get in Touch","text":"<p>If you cannot find the information that you need in the documentation, help is available.</p> <ul> <li> <p> Get Help</p> <p>Contact the CSCS Service Desk for help.</p> <p> Service Desk</p> </li> <li> <p> Chat</p> <p>Discuss Alps with other users and CSCS staff on Slack.</p> <p> CSCS User Slack</p> </li> </ul> <ul> <li> <p> Contribute</p> <p>The source for the documentation is hosted on GitHub.</p> <p> Contribute to the docs </p> </li> </ul>"},{"location":"access/","title":"Connecting to Alps","text":"<p>This documentation guides users through the process of accessing CSCS systems and services.</p> <p>Before accessing CSCS, you need to have an account at CSCS, and be part of a project that has been allocated resources. More information on how to get an account is available in accounts and projects.</p> <ul> <li> <p> Multi Factor Authentication</p> <p>Before signing in to CSCS\u2019 web portals or using SSH, all users have to set up multi factor authentication (MFA)</p> <p> MFA</p> </li> <li> <p> Web Services</p> <p>Before signing in to CSCS\u2019 web portals or using SSH, all users have to set up multi factor authentication (MFA)</p> <p> Accessing CSCS web services</p> </li> <li> <p> SSH Access</p> <p>Logging into Clusters on Alps</p> <p> SSH</p> </li> <li> <p> FirecREST</p> <p>FirecREST is a RESTful API for programmatically accessing High-Performance Computing resources.</p> <p> FirecREST</p> </li> <li> <p> JupyterLab</p> <p>JupyterLab is a feature-rich notebook authoring application and editing environment.</p> <p> JupyterLab</p> </li> <li> <p> VSCode</p> <p>How to connect VSCode IDE on your laptop with Alps</p> <p> SSH</p> </li> </ul>"},{"location":"access/firecrest/","title":"FirecREST","text":""},{"location":"access/firecrest/#firecrest","title":"FirecREST","text":"<p>FirecREST is a RESTful API for programmatically accessing High-Performance Computing resources, developed at CSCS.</p> <p>Users can make use of FirecREST to automate access to HPC, enabling CI/CD pipelines, workflow orchestrators, and other tools against HPC resources.</p> <p>Additionally, scientific platform developers can integrate FirecREST into web-enabled portals and web UI applications, allowing them to securely access authenticated and authorized CSCS services such as job submission and data transfer on HPC systems.</p> <p>Users can make HTTP requests to perform the following operations:</p> <ul> <li>basic system utilities like <code>ls</code>, <code>mkdir</code>, <code>mv</code>, <code>chmod</code>, <code>chown</code>, among others</li> <li>actions against the Slurm workload manager (submit, query, and cancel jobs of the user)</li> <li>internal (between CSCS systems) and external (to/from CSCS systems) data transfers</li> </ul>"},{"location":"access/firecrest/#firecrest-versions","title":"FirecREST versions","text":"<p>Starting early 2025, CSCS has introduced a new version of the API: FirecREST version 2.</p> <p>Version 2 is faster, easier to use, and more efficient in resource management than its predecessor, therefore, if you are new to FirecREST start directly using version 2.</p> <p>Deprecation notice</p> <p>If you\u2019re using version 1, we recommend you to port your applications to use the new version. We will communicate soon the exact date of the decommissioning of version 1 (not before Quarter 4 of 2025).</p> Version 2Version 1 <p>For a full feature set, have a look at the latest FirecREST version 2 API specification deployed at CSCS.</p> <p>Please refer to the FirecREST-v2 documentation for detailed documentation.</p> <p>For a full feature set, have a look at the latest FirecREST version 1 API specification deployed at CSCS.</p> <p>Please refer to the FirecREST-v1 documentation for detailed documentation.</p>"},{"location":"access/firecrest/#firecrest-deployment-on-alps","title":"FirecREST Deployment on Alps","text":"<p>FirecREST is available for all three major Alps platforms, with a different API endpoint and versions for each platform.</p> PlatformVersionAPI EndpointClusters HPC Platformv1https://api.cscs.ch/hpc/firecrest/v1Daint, Eiger v2https://api.cscs.ch/hpc/firecrest/v2 ML Platformv1https://api.cscs.ch/ml/firecrest/v1Bristen, Clariden v2https://api.cscs.ch/ml/firecrest/v2 CW Platformv1https://api.cscs.ch/cw/firecrest/v1Santis v2https://api.cscs.ch/cw/firecrest/v2"},{"location":"access/firecrest/#accessing-firecrest","title":"Accessing FirecREST","text":""},{"location":"access/firecrest/#clients-and-access-tokens","title":"Clients and access tokens","text":"<p>For authenticating requests to FirecREST, client applications use an access token instead of directly using the user\u2019s credentials. The access token is a signed JSON Web Token (JWT) which contains user information and is only valid for a short time (5 minutes). Behind the API, all commands launched by the client will use the account of the user that registered the client, inheriting their access rights.</p> <p>Every client has a client ID (Consumer Key) and a secret (Consumer Secret) that are used to get a short-lived access token with an HTTP request.</p> <code>curl</code> call to fetch the access token <pre><code>curl -s -X POST https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token \\\n     --data \"grant_type=client_credentials\" \\\n     --data \"client_id=&lt;client_id&gt;\" \\\n     --data \"client_secret=&lt;client_secret&gt;\"\n</code></pre> <p>You can manage your client application on the CSCS Developer Portal.</p> <p>To use your client credentials to access FirecREST, follow the API documentation.</p>"},{"location":"access/firecrest/#getting-started","title":"Getting Started","text":""},{"location":"access/firecrest/#using-the-python-interface","title":"Using the Python Interface","text":"<p>One way to get started is by using pyFirecREST, a Python package with a collection of wrappers for the different functionalities of FirecREST. This package simplifies the usage of FirecREST by making multiple requests in the background for more complex workflows as well as by refreshing the access token before it expires.</p> Version 2Version 1 Try FirecREST using pyFirecREST v2 <pre><code>import json\nimport firecrest as f7t\n\nclient_id = \"&lt;client_id&gt;\"\nclient_secret = \"&lt;client_secret&gt;\"\ntoken_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n# Setup the client for the specific account\n# For instance, for the Alps HPC Platform system Daint:\n\nclient = f7t.v2.Firecrest(\n    firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v2\",\n    authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n)\n\n# Status of the systems, filesystems and schedulers:\nprint(json.dumps(client.systems(), indent=2))\n\n# Output: information about systems and health status of the infrastructure\n# [\n#   {\n#     \"name\": \"daint\",\n#     \"ssh\": {                           # --&gt; SSH settings\n#       \"host\": \"daint.alps.cscs.ch\",\n#       \"port\": 22,\n#       \"maxClients\": 100,\n#       \"timeout\": {\n#         \"connection\": 5,\n#         \"login\": 5,\n#         \"commandExecution\": 5,\n#         \"idleTimeout\": 60,\n#         \"keepAlive\": 5\n#       }\n#     },\n#     \"scheduler\": {                     # --&gt; Scheduler settings\n#       \"type\": \"slurm\",\n#       \"version\": \"24.05.4\",\n#       \"apiUrl\": null,\n#       \"apiVersion\": null,\n#       \"timeout\": 10\n#     },\n#     \"servicesHealth\": [                # --&gt; Health status of services\n#       {\n#         \"serviceType\": \"scheduler\",\n#         \"lastChecked\": \"2025-03-18T23:34:51.167545Z\",\n#         \"latency\": 0.4725925922393799,\n#         \"healthy\": true,\n#         \"message\": null,\n#         \"nodes\": {\n#           \"available\": 21,\n#           \"total\": 858\n#         }\n#       },\n#       {\n#         \"serviceType\": \"ssh\",\n#         \"lastChecked\": \"2025-03-18T23:34:52.054056Z\",\n#         \"latency\": 1.358715295791626,\n#         \"healthy\": true,\n#         \"message\": null\n#       },\n#       {\n#         \"serviceType\": \"filesystem\",\n#         \"lastChecked\": \"2025-03-18T23:34:51.969350Z\",\n#         \"latency\": 1.2738196849822998,\n#         \"healthy\": true,\n#         \"message\": null,\n#         \"path\": \"/capstor/scratch/cscs\"\n#       },\n#     (...)\n#     \"fileSystems\": [                   # --&gt; Filesystem settings\n#       {\n#         \"path\": \"/capstor/scratch/cscs\",\n#         \"dataType\": \"scratch\",\n#         \"defaultWorkDir\": true\n#       },\n#       {\n#         \"path\": \"/users\",\n#         \"dataType\": \"users\",\n#         \"defaultWorkDir\": false\n#       },\n#       {\n#         \"path\": \"/capstor/store/cscs\",\n#         \"dataType\": \"store\",\n#         \"defaultWorkDir\": false\n#       }\n#     ]    \n#   }\n# ]\n\n# List content of directories\nprint(json.dumps(client.list_files(\"daint\", \"/capstor/scratch/cscs/&lt;username&gt;\"),\n                                indent=2))\n\n# [\n#   {\n#     \"name\": \"directory\",\n#     \"type\": \"d\",\n#     \"linkTarget\": null,\n#     \"user\": \"&lt;username&gt;\",\n#     \"group\": \"&lt;project&gt;\",\n#     \"permissions\": \"rwxr-x---+\",\n#     \"lastModified\": \"2024-09-02T12:34:45\",\n#     \"size\": \"4096\"\n#   },\n#   {\n#     \"name\": \"file.txt\",\n#     \"type\": \"-\",\n#     \"linkTarget\": null,\n#     \"user\": \"&lt;username&gt;\",\n#     \"group\": \"&lt;project&gt;\",\n#     \"permissions\": \"rw-r-----+\",\n#     \"lastModified\": \"2024-09-02T08:26:04\",\n#     \"size\": \"131225\"\n#   }\n# ]\n</code></pre> Try FirecREST using pyFirecREST v1 <pre><code>import json\nimport firecrest as f7t\n\nclient_id = \"&lt;client_id&gt;\"\nclient_secret = \"&lt;client_secret&gt;\"\ntoken_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n# Setup the client for the specific account\n# For instance, for the Alps HPC Platform system Daint:\n\nclient = f7t.v1.Firecrest(\n    firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v1\",\n    authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n)\n\nprint(json.dumps(client.all_systems(), indent=2))\n# Output: (one dictionary per system)\n# [{\n#      'description': 'System ready',\n#      'status': 'available',\n#      'system': 'daint'      \n# }]\n\nprint(json.dumps(client.list_files('daint', '/capstor/scratch/cscs/&lt;username&gt;'),\n                                indent=2))\n# Example output: (one dictionary per file)\n# [\n#   {\n#       'name': 'directory',\n#       'user': '&lt;username&gt;'\n#       'last_modified': '2024-04-20T11:22:41',\n#       'permissions': 'rwxr-xr-x',\n#       'size': '4096',\n#       'type': 'd',\n#       'group': '&lt;project&gt;',\n#       'link_target': '',\n#   }\n#   {\n#      'name': 'file.txt',\n#      'user': '&lt;username&gt;'\n#      'last_modified': '2024-09-02T08:26:04',\n#      'permissions': 'rw-r--r--',\n#      'size': '131225',\n#      'type': '-',\n#      'group': '&lt;project&gt;',\n#      'link_target': '',\n#   }\n# ]\n</code></pre> <p>The tutorial is written for a generic instance of FirecREST but if you have a valid user at CSCS you can test it directly with your resource allocation on the exposed systems.</p>"},{"location":"access/firecrest/#data-transfer-with-firecrest","title":"Data transfer with FirecREST","text":"<p>In addition to the external transfer methods at CSCS, FirecREST provides automated data transfer within the API.</p> <p>A staging area is used for external transfers and downloading/uploading a file from/to a CSCS filesystem.</p> <p>Note</p> <p>pyFirecREST (both v1 and v2) hides this complexity to the user. We strongly recommend to use this library for these tasks.</p> Version 2Version 1 <p>Please follow the steps below to download a file:</p> <ol> <li>Request FirecREST to move the file to the staging area: a download link will be provided</li> <li>The file will remain in the staging area for 7 days or until the link gets invalidated with a request to the <code>/storage/xfer-external/invalidate</code> endpoint or through the pyfirecrest method</li> <li>The staging area is common for all users, therefore users should invalidate the link as soon as the download has been completed</li> </ol> <p>You can see the full process in this tutorial.</p> <p>We may be forced to delete older files sooner than 7 days whenever large files are moved to the staging area and the link is not invalidated after the download, to avoid issues for other users: we will contact the user in this case.</p> <p>When uploading files through the staging area, you don\u2019t need to invalidate the link. FirecREST will do it automatically as soon as it transfers the file to the filesystem of CSCS.</p> <p>There is also a constraint on the size of a single file to transfer externally to our systems via FirecREST: 5 GB.</p> <p>If you wish to transfer data bigger than the limit mentioned above, you can check the compress and extract endpoints or follow the following example on how to split large files and download/upload them using FirecREST.</p> <p>The limit on the time and size of files that can be download/uploaded via FirecREST might change if needed.</p> <p>Checking the current values in the parameters endpoint</p> <pre><code>&gt;&gt;&gt; print(json.dumps(client.parameters(), indent = 2))\n{\n(...)\n\n\"storage\": [\n    {\n    \"description\": \"Type of object storage, like `swift`, `s3v2` or `s3v4`.\",\n    \"name\": \"OBJECT_STORAGE\",\n    \"unit\": \"\",\n    \"value\": \"s3v4\"\n    },\n    {\n    \"description\": \"Expiration time for temp URLs.\",\n    \"name\": \"STORAGE_TEMPURL_EXP_TIME\",\n    \"unit\": \"seconds\",\n    \"value\": \"604800\"  ## &lt;-------- 7 days\n    },\n    {\n    \"description\": \"Maximum file size for temp URLs.\",\n    \"name\": \"STORAGE_MAX_FILE_SIZE\",\n    \"unit\": \"MB\",\n    \"value\": \"5120\"   ## &lt;--------- 5 GB\n    }\n(...)\n}\n</code></pre> <p>Job submission through FirecREST</p> <p>FirecREST provides an abstraction for job submission using in the backend the Slurm scheduler of the vCluster.</p> <p>When submitting a job via the different endpoints, you should pass the <code>-l</code> option to the <code>/bin/bash</code> command on the batch file.</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --nodes=1\n...\n</code></pre> <p>This option ensures that the job submitted uses the same environment as your login shell to access the system-wide profile (<code>/etc/profile</code>) or to your profile (in files like <code>~/.bash_profile</code>, <code>~/.bash_login</code>, or <code>~/.profile</code>).</p>"},{"location":"access/firecrest/#upload","title":"Upload","text":"<p>Upload a large file using FirecREST-v2</p> <pre><code>import firecrest as f7t\n\n(...)\n\nsystem = \"daint\"\nsource_path = \"/path/to/local/file\"\ntarget_dir = \"/capstor/scratch/cscs/&lt;username&gt;\"\ntarget_file = \"file\"\naccount = \"&lt;project&gt;\"\n\n\nupload_task = client.upload(system,\n                            local_file=source_path,\n                            directory=target_dir,\n                            filename=target_file,\n                            account=account,\n                            blocking=True)    \n</code></pre>"},{"location":"access/firecrest/#download","title":"Download","text":"<p>Download a large file using FirecREST-v2</p> <pre><code>import firecrest as f7t\n\n(...)\n\nsystem = \"daint\"\nsource_path = \"/capstor/scratch/cscs/&lt;username&gt;/file\"\ntarget_path = \"/path/to/local/file\"\naccount = \"&lt;project&gt;\"\n\n\ndownload_task = client.download(system,\n                                source_path=source_path,\n                                target_path=target_path,\n                                account=account,\n                                blocking=True)\n</code></pre>"},{"location":"access/firecrest/#further-information","title":"Further Information","text":"<ul> <li>HPC Platform Dashboard</li> <li>ML Platform Dashboard</li> <li>C&amp;W Platform Dashboard</li> <li>FirecREST OpenAPI Specification</li> <li>FirecREST Official Docs</li> <li>Documentation of pyFirecREST</li> <li>FirecREST repository</li> <li>What are JSON Web Tokens</li> <li>Python Requests</li> <li>Python Async API Calls</li> </ul>"},{"location":"access/hpc-console/","title":"HPC Console","text":""},{"location":"access/hpc-console/#hpc-console_1","title":"HPC Console","text":"<p>HPC Console is a web application that provides users with a modern web-based environment to interact with HPC resources such as clusters, job schedulers, and filesystems.</p>"},{"location":"access/hpc-console/#features","title":"Features","text":"<ul> <li>Dashboard \u2013 shows cluster availability and basic system status.</li> <li>Jobs \u2013 allows creating, submitting, and monitoring jobs. Users can provide job name, script, and resource requirements.</li> <li>File Navigator \u2013 supports browsing directories, uploading, downloading, and deleting files. Editing files inline is not supported.</li> </ul>"},{"location":"access/hpc-console/#alps-platforms","title":"Alps platforms","text":"<p>HPC console is currently available on three Alps platforms.</p> Platform URL Clusters HPC Platform https://my.hpcp.cscs.ch/ Daint, Eiger ML Platform https://my.mlp.cscs.ch/ Bristen, Clariden CW Platform https://my.cwp.cscs.ch/ Santis"},{"location":"access/hpc-console/#access-and-prerequisites","title":"Access and prerequisites","text":"<ul> <li>Access requires a valid CSCS account and authentication through the institutional login system.</li> <li>Supported browsers: recent versions of Chrome, Firefox, and Safari are recommended.</li> <li>Sessions automatically expire after a period of inactivity.</li> </ul>"},{"location":"access/hpc-console/#feedback","title":"Feedback","text":"<p>We welcome your feedback to help us improve HPC Console. If you encounter issues or have suggestions, please let us know.</p>"},{"location":"access/hpc-console/#further-information","title":"Further information","text":"<ul> <li>FirecREST UI documentation</li> <li>FirecREST UI repository</li> <li>FirecREST documentation</li> </ul>"},{"location":"access/jupyterlab/","title":"JupyterLab","text":""},{"location":"access/jupyterlab/#jupyterlab","title":"JupyterLab","text":""},{"location":"access/jupyterlab/#access-and-setup","title":"Access and setup","text":"<p>The JupyterHub service enables the interactive execution of JupyterLab on the compute nodes of Daint, Clariden, Santis and Eiger.</p> <p>The service is accessed at\u00a0jupyter-daint.cscs.ch, jupyter-clariden.cscs.ch, jupyter-santis.cscs.ch and jupyter-eiger.cscs.ch, respectively. As the notebook servers are executed on compute nodes, you must have a project with compute resources available on the respective cluster.</p> <p>Once logged in, you will be redirected to the JupyterHub Spawner Options form, where typical job configuration options can be selected. These options might include the type and number of compute nodes, the wall time limit, and your project account.</p> <p>By default, JupyterLab servers are launched in a dedicated queue, which should ensure a start-up time of less than a few minutes. If your server is not running within 5 minutes we encourage you to first try the non-dedicated queue, and then contact us.</p> <p>When resources are granted the page redirects to the JupyterLab session, where you can browse, open and execute notebooks on the compute nodes. A new notebook with a Python 3 kernel can be created with the menu\u00a0<code>new</code>\u00a0and then\u00a0<code>Python 3</code>\u00a0. Under\u00a0<code>new</code>\u00a0it is also possible to create new text files and folders, as well as to open a terminal session on the allocated compute node.</p> <p>Debugging</p> <p>The log file of a JupyterLab server session is saved in\u00a0<code>$HOME</code>\u00a0in a file named\u00a0<code>slurm-&lt;jobid&gt;.out</code>. If you encounter problems with your JupyterLab session, the contents of this file can contain clues to debug the issue.</p> Unexpected error while saving file: disk I/O error. <p>This error message indicates that you have run out of disk quota. You can check your quota using the command <code>quota</code>.</p> <p></p>"},{"location":"access/jupyterlab/#runtime-environment","title":"Runtime environment","text":"<p>A Jupyter session can be started with either a uenv or a container as a base image. The JupyterHub Spawner form provides a set of default images such as the prgenv-gnu uenv or the NGC PyTorch container to choose from in a dropdown menu. When using uenv, the software stack will be mounted at <code>/user-environment</code>, and the specified view will be activated. For a container, the Jupyter session will launch inside the container filesystem with only a select set of paths mounted from the host. Once you have found a suitable option, you can start the session with <code>Launch JupyterLab</code>.</p> Using remote uenv for the first time. <p>If the uenv is not present in the local repository, it will be automatically fetched. As a result, JupyterLab may take slightly longer than usual to start.</p> <p>Ending your interactive session and logging out</p> <p>The Jupyter servers can be shut down through the Hub. To end a JupyterLab session, please select\u00a0<code>Hub Control Panel</code>\u00a0under the\u00a0<code>File</code>\u00a0menu and then\u00a0<code>Stop My Server</code>. By contrast, clicking\u00a0<code>Logout</code>\u00a0will log you out of the server, but the server will continue to run until the Slurm job reaches its maximum wall time.</p> <p>If the default base images do not meet your requirements, you can specify a custom environment instead. For this purpose, you supply either a custom uenv image/view or container engine (CE) TOML file under the section <code>Advanced options</code> before launching the session. The supported uenvs are compatible with the Jupyter service out of the box, whereas container images typically require the installation of some additional packages. </p> Example of a custom PyTorch container <p>A container image based on recent a NGC PyTorch release requires the installation of the following additional packages to be compatible with the Jupyter service:</p> <pre><code>FROM nvcr.io/nvidia/pytorch:25.05-py3\n\nRUN pip install --no-cache \\\n    jupyterlab \\\n    jupyterhub==4.1.6 \\\n    pyfirecrest==1.2.0 \\\n    SQLAlchemy==1.4.52 \\\n    oauthenticator==16.3.1 \\\n    notebook==7.3.3 \\\n    jupyterlab_nvdashboard==0.13.0 \\\n    git+https://github.com/eth-cscs/firecrestspawner.git\n</code></pre> <p>The package nvdashboard is also installed here, which allows to monitor system metrics at runtime.</p> <p>A corresponding TOML file can look like</p> <pre><code>image = \"/capstor/scratch/cscs/${USER}/ce-images/ngc-pytorch+25.05.sqsh\"\n\nmounts = [\n    \"/capstor\", \n    \"/iopsstor\",\n    \"/users/${USER}/.local/share/jupyter\", # (1)!\n    \"/etc/slurm\", # (2)!\n    \"/usr/lib64/libslurm-uenv-mount.so\",\n    \"/etc/container_engine_pyxis.conf\" # (3)!\n]\n\nworkdir = \"/capstor/scratch/cscs/${USER}\" # (4)!\n\nwritable = true\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\" # (5)!\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nCUDA_CACHE_DISABLE = \"1\" # (6)!\nTORCH_NCCL_ASYNC_ERROR_HANDLING = \"1\" # (7)!\nMPICH_GPU_SUPPORT_ENABLED = \"0\" # (8)!\n</code></pre> <ol> <li>Avoid mounting all of <code>$HOME</code> to avoid subtle issues with cached files, but mount Jupyter kernels</li> <li>Enable Slurm commands (together with two subsequent mounts)</li> <li>Required only for Daint and Santis; Do not use on Clariden</li> <li>Set working directory of Jupyter session (file browser root directory)</li> <li>Use environment settings for optimized communication </li> <li>Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.</li> <li>Async error handling when an exception is observed in NCCL watchdog: aborting NCCL communicator and tearing down process upon error</li> <li>Disable GPU support in MPICH, as it can lead to deadlocks when using together with NCCL</li> </ol> Accessing file systems with uenv <p>While Jupyter sessions with CE start in the directory specified with <code>workdir</code>, a uenv session always start in your\u00a0<code>$HOME</code>\u00a0folder. All non-hidden files and folders in\u00a0<code>$HOME</code>\u00a0are visible and accessible through the JupyterLab file browser. However, you can not browse directly to folders above\u00a0<code>$HOME</code>. To enable access your\u00a0<code>$SCRATCH</code>\u00a0folder, it is therefore necessary to create a symbolic link to your\u00a0<code>$SCRATCH</code>\u00a0folder. This can be done by issuing the following command in a terminal from your\u00a0<code>$HOME</code>\u00a0directory: <pre><code>ln -s $SCRATCH $HOME/scratch\n</code></pre></p>"},{"location":"access/jupyterlab/#creating-jupyter-kernels","title":"Creating Jupyter kernels","text":"<p>A kernel, in the context of Jupyter, is a program together with environment settings that runs the user code within Jupyter notebooks. In Python, Jupyter kernels make it possible to access the (system) Python installation of a uenv or container, that of a virtual environment (on top) or any other custom Python installations like Anaconda/Miniconda from Jupyter notebooks. Alternatively, a kernel can also be created for other programming languages such as Julia, allowing e.g. the execution of Julia code in notebook cells. </p> <p>As a preliminary step to running any code in Jupyter notebooks, a kernel needs to be installed, which is described in the following for both Python and Julia.</p>"},{"location":"access/jupyterlab/#using-python-in-jupyter","title":"Using Python in Jupyter","text":"<p>For Python, the recommended setup consists of a uenv or container as a base image as described above that includes the stable dependencies of the software stack. Additional packages can be installed in a virtual environment on top of the Python installation in the base image (mandatory for most uenvs). Having the base image loaded, such a virtual environment can be created with</p> Create a virtual environment on top of a base image<pre><code>python -m venv --system-site-packages venv-&lt;base-image-version&gt;\n</code></pre> <p>where <code>&lt;base-image-version&gt;</code> can be replaced by an identifier uniquely referring to the base image (such virtual environments are specific for the base image and are not portable).</p> <p>Jupyter kernels for Python are powered by\u00a0<code>ipykernel</code>. As a result,\u00a0<code>ipykernel</code>\u00a0must be installed in the target environment that will be used as a kernel. That can be done with\u00a0<code>pip install ipykernel</code> (either as part of a Dockerfile or in an activated virtual environment on top of a uenv/container image).</p> <p>A kernel can now be created from an active Python virtual environment with the following commands</p> Create an IPython Jupyter kernel<pre><code>. venv-&lt;base-image-version&gt;/bin/activate # (1)!\npython -m ipykernel install \\\n    ${VIRTUAL_ENV:+--env PATH $PATH --env VIRTUAL_ENV $VIRTUAL_ENV} \\\n    --user --name=\"&lt;kernel-name&gt;\" # (2)!\n</code></pre> <ol> <li>This step is only necessary when working with a virtual environment on top of the base image</li> <li>The expression in braces makes sure the kernel\u2019s environment is properly configured when using a virtual environment (must be activated). The flag <code>--user</code> installs the kernel to a path under <code>${HOME}/.local/share/jupyter</code>. </li> </ol> <p>The <code>&lt;kernel-name&gt;</code> can be replaced by a name specific to the base image/virtual environment.</p> Python packages from uenv shadowing those in a virtual environment <p>When using uenv with a virtual environment on top, the site-packages under <code>/user-environment</code> currently take precedence over those in the activated virtual environment. This is due to the uenv paths being included in the <code>PYTHONPATH</code> environment variable. As a consequence, despite installing a different version of a package in the virtual environment from what is available in the uenv, the uenv version will still be imported at runtime. A possible workaround is to prepend the virtual environment\u2019s site-packages to <code>PYTHONPATH</code> whenever activating the virtual environment. <pre><code>export PYTHONPATH=\"$(python -c 'import site; print(site.getsitepackages()[0])'):$PYTHONPATH\"\n</code></pre> Consequently, a modified command should be used to install the Jupyter kernel that carries over the changed <code>PYTHONPATH</code> to the Jupyter environment. This can be done as follows. <pre><code>python -m ipykernel install \\\n    ${VIRTUAL_ENV:+--env PATH $PATH --env VIRTUAL_ENV $VIRTUAL_ENV ${PYTHONPATH+--env PYTHONPATH $PYTHONPATH}} \\\n    --user --name=\"&lt;kernel-name&gt;\"\n</code></pre> It is recommended to apply this workaround if you are constrained by a Python package version installed in the uenv that you need to change for your application.</p>"},{"location":"access/jupyterlab/#using-julia-in-jupyter","title":"Using Julia in Jupyter","text":"<p>To run Julia code in Jupyter notebooks, you can use the provided uenv for this language. In particular, you need to use the following in the JupyterHub Spawner <code>Advanced options</code> forms mentioned above:</p> <p>pass a <code>julia</code> uenv and the view <code>jupyter</code>.</p> <p>When Julia is first used within Jupyter, IJulia and one or more Julia kernel need to be installed.  Type the following command in a shell within JupyterHub to install IJulia, the default Julia kernel and, on systems with Nvidia GPUs, a Julia kernel running under Nvidia Nsight Systems: <pre><code>install_ijulia\n</code></pre></p> <p>You can install additional custom Julia kernels by typing the following in a shell: <pre><code>julia\nusing IJulia\ninstallkernel(&lt;args&gt;) # (1)!\n</code></pre></p> <ol> <li>type <code>? installkernel</code> to learn about valid <code>&lt;args&gt;</code></li> </ol> <p>First time use of Julia</p> <p>If you are using Julia for the first time at all, executing <code>install_ijulia</code> will automatically first trigger the installation of <code>juliaup</code> and the latest <code>julia</code> version (it is also triggered if you execute <code>juliaup</code> or <code>julia</code>).</p>"},{"location":"access/jupyterlab/#parallel-computing","title":"Parallel computing","text":""},{"location":"access/jupyterlab/#mpi-in-the-notebook-via-ipyparallel-and-mpi4py","title":"MPI in the notebook via IPyParallel and MPI4Py","text":"<p>MPI for Python provides bindings of the Message Passing Interface (MPI) standard for Python, allowing any Python program to exploit multiple processors.</p> <p>MPI can be made available on Jupyter notebooks through\u00a0IPyParallel. This is a Python package and collection of CLI scripts for controlling clusters for Jupyter: a set of servers that act as a cluster, called engines, is created and the code in the notebook\u2019s cells will be executed within them.</p> <p>We provide the Python package\u00a0<code>ipcmagic</code>\u00a0to make easier the management of IPyParallel clusters.\u00a0<code>ipcmagic</code>\u00a0can be installed by the user with</p> <pre><code>pip install ipcmagic-cscs\n</code></pre> <p>The engines and another server that moderates the cluster, called the controller, can be started an stopped with the magic\u00a0<code>%ipcluster start -n &lt;num-engines&gt;</code>\u00a0and\u00a0<code>%ipcluster stop</code>, respectively. Before running the command, the python package\u00a0<code>ipcmagic</code>\u00a0must be imported</p> <pre><code>import ipcmagic\n</code></pre> <p>Information about the command, can be obtained with\u00a0<code>%ipcluster --help</code>.</p> <p>In order to execute MPI code on JupyterLab, it is necessary to indicate that the cells have to be run on the IPyParallel engines. This is done by adding the\u00a0IPyParallel magic command <code>%%px</code>\u00a0to the first line of each cell.</p> <p>There are two important points to keep in mind when using IPyParallel. The first one is that the code executed on IPyParallel engines has no effect on non-<code>%%px</code>\u00a0cells. For instance, a variable created on a\u00a0<code>%%px</code>-cell will not exist on a non-<code>%%px</code>-cell. The opposite is also true. A variable created on a regular cell, will be unknown to the IPyParallel engines. The second one is that the IPyParallel engines are common for all the user\u2019s notebooks. This means that variables created on a\u00a0<code>%%px</code>\u00a0cell of one notebook can be accessed or modified by a different notebook.</p> <p>The magic command\u00a0<code>%autopx</code>\u00a0can be used to make all the cells of the notebook\u00a0<code>%%px</code>-cells.\u00a0<code>%autopx</code>\u00a0acts like a switch: running it once, activates the\u00a0<code>%%px</code>\u00a0and running it again deactivates it. If\u00a0<code>%autopx</code>\u00a0is used, then there are no regular cells and all the code will be run on the IPyParallel engines.</p> <p>Examples of notebooks with\u00a0<code>ipcmagic</code>\u00a0can be found\u00a0here.</p>"},{"location":"access/jupyterlab/#distributed-training-and-inference-for-ml","title":"Distributed training and inference for ML","text":"<p>While it is generally recommended to submit long-running machine learning training and inference jobs via <code>sbatch</code>, certain use cases can benefit from an interactive Jupyter environment.</p> <p>A popular approach to run multi-GPU ML workloads is with <code>accelerate</code> and <code>torchrun</code> as demonstrated in the tutorials. In particular, the <code>accelerate launch</code> script in the LLM fine-tuning tutorial can be directly carried over to a Jupyter cell with a <code>%%bash</code> header (to run its contents interpreted by bash). For <code>torchrun</code>, one can adapt the command from the multi-node nanotron tutorial to run on a single GH200 node using the following line in a Jupyter cell</p> <pre><code>!python -m torch.distributed.run --standalone --nproc_per_node=4 run_train.py ...\n</code></pre> <p>torchrun with virtual environments</p> <p>When using a virtual environment on top of a base image with PyTorch, always replace <code>torchrun</code> with <code>python -m torch.distributed.run</code> to pick up the correct Python environment. Otherwise, the system Python environment will be used and virtual environment packages will not available. If not using virtual environments such as with a self-contained PyTorch container, <code>torchrun</code> is equivalent to <code>python -m torch.distributed.run</code>.</p> <p>Notebook structure</p> <p>In none of these scenarios any significant memory allocations or background computations are performed on the main Jupyter process. Instead, the resources are kept available for the processes launched by <code>accelerate</code> or <code>torchrun</code>, respectively.</p> <p>Alternatively to using these launchers, it is also possible to use Slurm to obtain more control over resource mappings, e.g. by launching an overlapping Slurm step onto the same node used by the Jupyter process. An example with the container engine looks like this:</p> <pre><code>!srun --overlap -ul --environment /path/to/edf.toml \\\n    --container-workdir $PWD -n 4 bash -c \"\\\n    . venv-&lt;base-image-version&gt;/bin/activate\n    MASTER_ADDR=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n    MASTER_PORT=29500 \\\n    RANK=\\$SLURM_PROCID LOCAL_RANK=\\$SLURM_LOCALID WORLD_SIZE=\\$SLURM_NPROCS \\\n    python train.py ...\"\n</code></pre> <p>where <code>/path/to/edf.toml</code> should be replaced by the TOML file and <code>venv-&lt;base-image-version&gt;</code> by the name of the virtual environment (if used). The script <code>train.py</code> is using <code>torch.distributed</code> for distributed training. This launch mechanism can be further customized with extra Slurm options.</p> <p>Concurrent usage of resources</p> <p>Subtle bugs can occur when running multiple Jupyter notebooks concurrently that each assume access to the full node. Also, some notebooks may hold on to resources such as spawned child processes or allocated memory despite having completed. In this case, resources such as a GPU may still be busy, blocking another notebook from using it. Therefore, it is good practice to only keep one such notebook running that occupies the full node and restarting a kernel once a notebook has completed. If in doubt, system monitoring with <code>htop</code> and nvdashboard can be helpful for debugging.</p> <p>Multi-GPU training from a shared Jupyter process</p> <p>Running multi-GPU training workloads directly from the shared Jupyter process is generally not recommended due to potential inefficiencies and correctness issues (cf. the PyTorch docs). However, if you need it to e.g. reproduce existing results, it is possible to do so with utilities like <code>accelerate</code>\u2019s <code>notebook_launcher</code> or <code>transformers</code>\u2019 <code>Trainer</code> class. When using these in containers, you will currently need to unset the environment variables <code>RANK</code> and <code>LOCAL_RANK</code> by adding the following in a cell at the top of the notebook:</p> <pre><code>import os; os.environ.pop(\"RANK\"); os.environ.pop(\"LOCAL_RANK\");\n</code></pre>"},{"location":"access/jupyterlab/#further-documentation","title":"Further documentation","text":"<ul> <li>Jupyter</li> <li>JupyterLab</li> <li>JupyterHub</li> </ul>"},{"location":"access/mfa/","title":"Multi Factor Authentication (MFA)","text":""},{"location":"access/mfa/#multi-factor-authentication","title":"Multi Factor Authentication","text":"<p>To access CSCS services and systems users are required to authenticate using multi-factor authentication (MFA). MFA is implemented as a two-factor authentication, where one factor is the login and password pair (\u201cthe thing you know\u201d) and the other factor is the device which generates one-time passwords (OTPs, \u201cthe thing you have\u201d). In this way security is significantly improved compared to single-factor (password only) authentication.</p> <p>The MFA workflow uses a time-based one-time password (OTP) to verify identity. An OTP is a six-digit number which changes every 30 seconds. OTPs are generated using a tool installed on a device other than the one used to access CSCS services and infrastructure. We recommend to use a smartphone with an application such as Google Authenticator to obtain the OTPs.</p> <p></p>"},{"location":"access/mfa/#getting-started","title":"Getting Started","text":"<p>When you first log in to any of the CSCS web applications such as UMP, Jupyter, etc., you will be asked to register your device.</p> <p>Firstly, you will be asked to provide a code that you received by email. After this validation step, you will need to scan a QR code with your mobile phone using an application such as Google Authenticator. Lastly, you will need to enter the OTP from the authenticator application to complete the registration of your device. From then on, two-factor authentication will be required to access CSCS services and systems. A more detailed explanation of the registration process is provided in the next section.</p> <p>Warning</p> <p>It is not possible to log in to CSCS systems using SSH without registering a device and creating certified SSH keys. See below for details on generating certified SSH keys.</p>"},{"location":"access/mfa/#authenticator-application","title":"Authenticator Application","text":"<p>CSCS supports authenticators that follow an open standard called TOTP. The recommended way to access such an authenticator is to install an application on your mobile phone. Google Authenticator and FreeOTP have been tested successfully; however, if you are using a different mobile application for OTPs, feel free to continue using it - given it supports the TOTP standard.</p> <p>You can download Google Authenticator for your phone:</p> <ul> <li> Android: on the Google Play Store.</li> <li> iOS: on the Apple Store.</li> </ul> <p></p>"},{"location":"access/mfa/#configure-the-authenticator","title":"Configure the Authenticator","text":"<p>Before starting, ensure that the following pre-requisites are satisfied</p> <ol> <li>You have an invitation email from CSCS for MFA enrollment<ul> <li>a notification email will be sent at least one week before we sent the invitation email.</li> </ul> </li> <li>You have installed an OTP Authenticator app on your mobile device (see above).</li> </ol> <p>Note</p> <p>If you try access any of our web applications without setting up MFA, you will be redirected to enroll for MFA.</p> <p>Warning</p> <p>If you try to SSH to CSCS systems without setting up MFA, you will be prompted with permission denied error, for example: <pre><code>$ ssh ela.cscs.ch\nbobsmith@ela.cscs.ch: Permission denied (publickey).\nConnection closed by UNKNOWN port 65535\n</code></pre></p> <p>Steps:</p> <ol> <li>Access any of the CSCS Web applications such as <code>account.cscs.ch</code>, Jupyter, etc., on a new browser session which will redirects you to the CSCS login page.</li> <li>Log in with your username and password.</li> <li>You will be asked to key in a code which CSCS Authentication system sent to you by email.    After successful validation of the code you will be redirected to the next page which present a QR code.</li> <li>Scan the QR code with the authenticator app that was installed on your mobile device.    After scanning the QR code the authenticator app will start generating a new 6 digit OTP every 60 seconds.</li> <li>To complete the OTP registration process, please enter the 6 digit OTP from the authenticator app at the bottom of the the same QR code page. Optionally, you can input your device name where you imported the OTP seed by scanning the QR code</li> <li>On successful registration you will be logged into the CSCS web application that you accessed in step-1</li> </ol>"},{"location":"access/mfa/#resetting-the-authenticator","title":"Resetting the Authenticator","text":"<p>In case users lose access to their mobile device/Authenticator OTP, users can reset their OTP by following the below self-service process.</p> <ol> <li>Access any CSCS web application like: account.cscs.ch which redirects you to the CSCS Login page. </li> <li>From the login screen, click the \u201cReset OTP\u201d link below the \u201cLOG IN\u201d button</li> <li>Enter your username and password.</li> <li>On successful validation of user credentials, users will receive an email with a reset credentials link like the one below, click on the link in the email</li> <li>The steps are the same as for the first time you configured the authenticator.</li> </ol> <p>Warning</p> <p>When replacing your smartphone remember to sync the authenticator app before resetting the old smartphone. Otherwise, you will have to follow this process.</p>"},{"location":"access/ssh/","title":"SSH","text":""},{"location":"access/ssh/#using-ssh","title":"Using SSH","text":"<p>Before accessing CSCS clusters using SSH, first ensure that you have created a user account that is part of a project that has access to the cluster, and have multi factor authentication configured.</p> <p></p>"},{"location":"access/ssh/#generating-keys-with-sshservice","title":"Generating Keys with SSHService","text":"<p>It is not possible to authenticate with a username/password and user-created SSH keys. Instead, it is necessary to use a certified SSH key created using the CSCS SSHService.</p> <p>Note</p> <p>Keys are valid for 24 hours, after which a new key must be generated.</p> <p>Warning</p> <p>The number of certified SSH keys is limited to five per day. Once you have reached this number you will not be able to generate new keys until at least one of these key expires or keys are revoked.</p> <p>There are two methods for generating SSH keys using the SSHService, the SSHService web app or by using a command-line script.</p>"},{"location":"access/ssh/#getting-keys-via-the-command-line","title":"Getting keys via the command line","text":"<p>On Linux and MacOS, the SSH keys can be generated and automatically installed using a command-line script. This script is provided in pure Bash and in Python. Python 3 is required together with packages listed in <code>requirements.txt</code> provided with the scripts.</p> <p>Note</p> <p>We recommend to using a virtual environment for Python.</p> <p>If this is the first time, download the ssh service from CSCS GitHub:</p> <pre><code>git clone https://github.com/eth-cscs/sshservice-cli\ncd sshservice-cli\n</code></pre> <p>The next step is to use either the bash or python scripts:</p> bashpython <p>Run the bash script in the <code>sshservice-cli</code> path:</p> <pre><code>./cscs-keygen.sh\n</code></pre> <p>The first time you use the script, you can set up a python virtual environment with the dependencies installed:</p> <pre><code>python3 -m venv mfa\nsource mfa/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Thereafter, activate the venv before using the script:</p> <pre><code>source mfa/bin/activate\npython cscs-keygen.py\n</code></pre> <p>For both approaches, follow the on screen instructions that require you to enter your username, password and the six-digit OTP from the authenticator app on your phone. The script generates the key pair (<code>cscs-key</code> and <code>cscs-key-cert.pub</code>) in your <code>~/.ssh</code> path:</p> <pre><code>&gt; ls ~/.ssh/cscs-key*\n/home/bobsmith/.ssh/cscs-key  /home/bobsmith/.ssh/cscs-key-cert.pub\n</code></pre>"},{"location":"access/ssh/#getting-keys-via-the-web-app","title":"Getting keys via the web app","text":"<p>Access the SSHService web application by accessing the URL, sshservice.cscs.ch.</p> <ol> <li>Sign in with username, password and OTP</li> <li>Select \u201cSigned key\u201d on the left tab and click on \u201cGet a signed key\u201d</li> <li>On the next page a key pair is generated and ready to be downloaded. Download or copy/paste both keys.</li> </ol> <p>Once generated, the keys need to be copied from where your browser downloaded them to your <code>~/.ssh</code> path, for example: <pre><code>mv /download/location/cscs-key-cert.pub ~/.ssh/cscs-key-cert.pub\nmv /download/location/cscs-key ~/.ssh/cscs-key\nchmod 0600 ~/.ssh/cscs-key\n</code></pre></p>"},{"location":"access/ssh/#adding-a-password-to-the-key","title":"Adding a password to the key","text":"<p>Once the key has been generated using either the CLI or web interface above, it is strongly recommended that you add a password to the generated key using the ssh-keygen tool.</p> <pre><code>ssh-keygen -f ~/.ssh/cscs-key -p\n</code></pre>"},{"location":"access/ssh/#logging-in","title":"Logging In","text":"<p>To ensure secure access, CSCS requires users to connect through the designated jump host Ela (<code>ela.cscs.ch</code>) before accessing any cluster.</p> <p>Before trying to log into your target cluster, you can first check that the SSH key generated above can be used to access Ela: <pre><code>ssh -i ~/.ssh/cscs-key ela.cscs.ch\n</code></pre></p> <p>To log into a target system at CSCS, you need to perform some additional setup to handle forwarding of SSH keys generated using the SSHService. There are two alternatives detailed below.</p> <p></p>"},{"location":"access/ssh/#adding-ela-as-a-jump-host-in-ssh-configuration","title":"Adding Ela as a jump host in SSH Configuration","text":"<p>This approach configures Ela as a jump host and creates aliases for the systems that you want to access in <code>~/.ssh/config</code> on your laptop or PC. The benefit of this approach is that once the <code>~/.ssh/config</code> file has been configured, no additional steps are required between creating a new key using MFA, and logging in.</p> <p>Below is an example <code>~/.ssh/config</code> file that facilitates directly logging into the Daint, Santis and Clariden clusters using <code>ela.cscs.ch</code> as a Jump host:</p> <pre><code>Host ela\n    HostName ela.cscs.ch\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n\nHost daint\n    HostName daint.alps.cscs.ch\n    User cscsusername\n    ProxyJump ela\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n\nHost santis\n    HostName santis.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n\nHost clariden\n    HostName clariden.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre> <p> Replace <code>cscsusername</code> with your CSCS username in the file above.</p> <p>After saving this file, one can directly log into <code>daint.alps.cscs.ch</code> from your local system using the alias <code>daint</code>:</p> <pre><code>ssh daint\n</code></pre> <p></p>"},{"location":"access/ssh/#using-ssh-agent","title":"Using SSH Agent","text":"<p>Alternatively, the SSH authentication agent can be configured to manage the keys.</p> <p>Each time a new key is generated using the SSHService, add the key to the SSH agent: <pre><code>ssh-add -t 1d ~/.ssh/cscs-key\n</code></pre></p> Could not open a connection to your authentication agent <p>If you see this error message, the ssh agent is not running. You can start it with the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p> <p>Once the key has been configured, log into Ela using the <code>-A</code> flag, and then jump to the target system: <pre><code># log in to ela.cscs.ch\nssh -A cscsusername@ela.cscs.ch\n\n# then jump to a cluster\nssh daint.cscs.ch\n</code></pre></p>"},{"location":"access/ssh/#ssh-tunnel-to-a-service-on-alps-compute-nodes-via-ela","title":"SSH tunnel to a service on Alps compute nodes via ela","text":"<p>If you have a server listening on a compute node in an Alps cluster and want to reach it from your local computer, you can do the following: allocate a node, start your server bound to <code>localhost</code>, open an SSH tunnel that jumps through <code>ela</code> to the cluster, then use <code>http://localhost:PORT</code> locally. Details on how to achieve this are below.</p> <p>Before starting, make sure you:</p> <ul> <li>Have SSH keys loaded in your agent.</li> <li>Have your CSCS username handy (replace <code>MYUSER</code> below).</li> <li>Have your server running on a compute node on Alps.   See the Slurm documentation for help on how to allocate a node and start your server on a compute node.</li> <li>Know the compute node ID (e.g., <code>nid006554</code>) and the port of your running server.</li> </ul> <p>Fast fixes when starting a server or before tunneling</p> <ul> <li>Port already in use locally: pick another PORT (e.g., 6007) in both your server and the tunnel command below.</li> <li>Auth prompts loop: verify your SSH MFA to CSCS and that your SSH agent is correctly set up and loaded with your keys.</li> </ul> <p>Binding to <code>127.0.0.1</code> ensures the service is only reachable via your tunnel</p> <p>To open the tunnel from your local computer:</p> <pre><code>MYUSER=cscsusername     # your username at CSCS\nNODE=nid006554          # obtained from salloc or srun\nPORT=6006               # example port\nCLUSTER=daint           # cluster you want to reach\n\nssh -N -J ${MYUSER}@ela.cscs.ch,${MYUSER}@${CLUSTER}.alps.cscs.ch -L ${PORT}:localhost:${PORT}   ${MYUSER}@${NODE}\n</code></pre> <p>The command blocks while the tunnel is open (that is expected).</p> <p>The first run may ask to trust the node\u2019s host key\u2014type <code>yes</code></p> <p>With the service running and the tunnel open, you can now reach your service locally:</p> <ul> <li>Browser: <code>http://localhost:PORT</code></li> <li>Terminal: <code>curl localhost:PORT</code></li> </ul> <p>Fast fix if the service doesn\u2019t respond locally</p> <ul> <li>Service not responding: ensure the server binds to 127.0.0.1 and is running on the compute node; confirm NODE matches your current Slurm allocation.</li> </ul> <p>To clean up afterwards:</p> <ul> <li>Stop the server (Ctrl-C on the compute node shell).</li> <li>End the Slurm allocation:   <pre><code>scancel $SLURM_JOB_ID\n</code></pre></li> <li>Close the tunnel (Ctrl-C in the tunnel terminal).</li> </ul> <p></p>"},{"location":"access/ssh/#frequently-encountered-issues","title":"Frequently encountered issues","text":"too many authentication failures <p>You may have too many keys in your ssh agent. Remove the unused keys from the agent or flush them all with the following command: <pre><code>ssh-add -D\n</code></pre></p> Permission denied <p>This might indicate that they key has expired.</p> Could not open a connection to your authentication agent <p>If you see this error when adding keys to the ssh-agent, please make sure the agent is up, and if not bring up the agent using the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p>"},{"location":"access/vscode/","title":"VSCode","text":""},{"location":"access/vscode/#connecting-with-vs-code","title":"Connecting with VS Code","text":"<p>Visual Studio Code provides flexible support for remote development. VS Code\u2019s remote tunnel feature starts a server on a remote system, and connects the editor to this server. There are two ways to set up the connection:</p> <ul> <li>using the code CLI: the most flexible method if using containers or uenv.</li> <li>using the VS Code interface: VS Code will connect onto the system, download and start the server</li> </ul> <p>The main challenge with using VS Code is that the most convenient method for starting a remote session is to start a remote tunnel from the VS Code GUI. This approach starts a session in the standard login environment on that node, however this won\u2019t work if you want to be developing in a container, in a uenv, or on a compute node.</p> <p>This process is also demonstrated in a webinar on Interactive computing on \u201cAlps\u201d:</p>"},{"location":"access/vscode/#flexible-method-remote-server","title":"Flexible method: remote server","text":"<p>The most flexible method for connecting VS Code is to log in to the Alps system, set up your environment (start a container or uenv, start a session on a compute node), and start the remote server in that environment pre-configured.</p> <p></p>"},{"location":"access/vscode/#installing-the-server","title":"Installing the server","text":"<p>The first step is to download the VS Code CLI tool <code>code</code>, which CSCS provides for easy download. There are two executables, one for using on systems with x86 or ARM CPUs respectively.</p> <code>aarch64</code> nodes (daint, clariden, santis)<code>x86_64</code> nodes (eiger, bristen) <pre><code>wget https://jfrog.svc.cscs.ch/artifactory/uenv-sources/vscode/vscode_cli_alpine_arm64_cli.tar.gz\ntar -xf vscode_cli_alpine_arm64_cli.tar.gz\n</code></pre> <pre><code>wget https://jfrog.svc.cscs.ch/artifactory/uenv-sources/vscode/vscode_cli_alpine_x64_cli.tar.gz\ntar -xf vscode_cli_alpine_x64_cli.tar.gz\n</code></pre> <p>After downloading, copy the <code>code</code> executable to a location in your PATH, so that it is available for future sessions.</p> <p>Clusters on Alps share a common home path <code>HOME=/users/$USER</code> that is mounted on all clusters.</p> <p>If you want to use VS Code on multiple clusters, possibly with different CPU architectures (Daint, Clariden and Santis use <code>aarch64</code> CPUs, and Eiger uses <code>x86_64</code> CPUs), you need to take some additional steps to ensure that VS Code installation and configuration is separated.</p> <p>First, install the <code>code</code> executable in an architecture-specific path.</p> <p>Installing VS Code for <code>x86_64</code> and <code>aarch64</code></p> <p>In <code>~/.bashrc</code>, add the following line (you will need to log in again for this to take effect): <pre><code>export PATH=$HOME/.local/$(uname -m)/bin:$PATH\n</code></pre> The <code>uname -m</code> command will print <code>aarch64</code> or <code>x86_64</code>, according to the microarchitecture of the node it is run on.</p> <p>Then create the path, and copy the <code>code</code> executable to the architecture-specific path: <pre><code>mkdir -p $HOME/.local/$(uname -m)/bin\ncp ./code $HOME/.local/$(uname -m)/bin\n</code></pre> Repeat this for both <code>x86_64</code> and <code>aarch64</code> binaries.</p> <p>By default VS Code will store configuration, data and executables in <code>$HOME/.vscode-server</code>. To use VS Code on multiple clusters, it is strongly recommended that you create separate <code>vscode-server</code> path for each cluster by adding the following environment variable definitions to your <code>~/.bashrc</code>:</p> <pre><code>export VSCODE_AGENT_FOLDER=\"$HOME/.vscode-server/$CLUSTER_NAME-tunnel/.vscode-server\"\nexport VSCODE_CLI_DATA_DIR=\"$VSCODE_AGENT_FOLDER/cli\"\n</code></pre> <p>Warning</p> <p>You will need to log out and back in after updating <code>$HOME/.bashrc</code>, before trying to start the VS Code server for the first time.</p> <p></p>"},{"location":"access/vscode/#updating-vs-code-server","title":"Updating VS Code server","text":"<p>VS Code is continuously being updated, and the version of VS Code on your laptop will most likely be more recent than the version provided by CSCS.</p> <p>Once you have installed the server, you can easily update it to the latest version:</p> Updating VS Code server<pre><code>$ code --version\ncode 1.97.2 (commit e54c774e0add60467559eb0d1e229c6452cf8447)\n$ code update\nSuccessfully updated to 1.101.0 (commit dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1)\n$ code --version\ncode 1.101.0 (commit dfaf44141ea9deb3b4096f7cd6d24e00c147a4b1)\n</code></pre> <p>It is good practice to periodically update code to keep it in sync with the version on your laptop.</p> <p></p>"},{"location":"access/vscode/#starting-and-configuring-the-server","title":"Starting and configuring the server","text":"<p>Note</p> <p>You need to have a GitHub account to connect a remote tunnel to VS Code.</p> <p>To set up a remote server on the target system, run the <code>code</code> executable that you downloaded with the <code>tunnel</code> argument. You will be asked to choose whether to log in to Microsoft or GitHub (we have tested with GitHub):</p> <pre><code>$ code tunnel --name=$CLUSTER_NAME-tunnel\n...\n? How would you like to log in to Visual Studio Code? \u203a\n  Microsoft Account\n\u276f GitHub Account\n</code></pre> <p>Tip</p> <p>Give the tunnel a unique name using the <code>--name</code> flag, which will later be listed on the VS Code UI.</p> <p>You will be requested to go to github.com/login/device and enter an 8-digit code. Once you have finished registering the service with GitHub, in VS Code on your PC/laptop open the \u201cremote explorer\u201d pane on the left hand side of the main window, and the connection will be visible under REMOTES (TUNNELS/SSH) -&gt; Tunnels.</p> <p>First time setting up a remote service</p> <p>If this is the first time you have followed this procedure, you may have to sign in to GitHub in VS Code.</p> <p>Click on the Remote Explorer button on the left hand side, and then find the following option:</p> <pre><code>REMOTES(TUNNELS/SSH)\n Tunnels\n    Sign in to tunnels registered with GitHub\n</code></pre> <p>If you have not signed in to GitHub with VS Code editor, you will be redirected to the browser to sign in.</p> <p>After signing in and authorizing VS Code, the open tunnel should be visible under REMOTES (TUNNELS/SSH) -&gt; Tunnels.</p> <p></p>"},{"location":"access/vscode/#using-with-uenv","title":"Using with uenv","text":"<p>To use a uenv with VS Code, the uenv must be started before calling <code>code tunnel</code>. Log into the target system and start the uenv, then start the remote server, for example: <pre><code># log into daint (this could be any other Alps cluster)\nssh daint\n# start a uenv session on the login node\nuenv start --view=default prgenv-gnu/24.11:v1\n# then start the tunnel\ncode tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>Alternatively, you can execute <code>code tunnel</code> directly in the environment: <pre><code>ssh daint\nuenv run --view=default prgenv-gnu/24.11:v1 -- code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>Once the tunnel is configured, you can access it from VS Code.</p> <p>Warning</p> <p>If you plan to do any intensive work: repeated compilation of large projects or running python code in Jupyter, please see the guide to running on a compute node below. Running intensive workloads on login nodes, which are shared resources between all users, is against CSCS fair usage of Shared Resources policy.</p> <p></p>"},{"location":"access/vscode/#running-on-a-compute-node","title":"Running on a compute node","text":"<p>If you plan to do computation using your VS Code, then you should first allocate resources on a compute node and set up your environment there.</p> <p>directly create the tunnel using srun</p> <p>You can directly execute the <code>code tunnel</code> command using srun: <pre><code>ssh daint\nsrun --uenv=prgenv-gnu/24.11:v1 --view=default -t120 -n1 --pty code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <ul> <li><code>--uenv</code> and <code>--view</code> set up the uenv</li> <li><code>-t120</code> requests a 2 hour (120 minute) reservation</li> <li><code>-n1</code> requests a single rank - only one rank/process is required for VS Code</li> <li><code>--pty</code> allows forwarding of terminal I/O, required to sign in to Github</li> </ul> <p>Once the job allocation is granted, you will be prompted to log into GitHub, the same as starting a session on the login node. If you don\u2019t want to use a uenv, the command is even simpler: <pre><code>ssh daint\nsrun -t120 -n1 --pty code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>log into a node before starting</p> <p>It is also possible to log into a compute node before executing the <code>code tunnel</code> command, if that suits your workflow: <pre><code># log into daint\nssh daint\n\n# start an interactive shell session\nsrun -t120 -n1 --pty bash\n\n# set up the environment before starting the tunnel\nuenv start prgenv-gnu/24.11:v1 --view=default\ncode tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <ul> <li><code>-t120</code> requests a 2 hour (120 minute) reservation</li> <li><code>-n1</code> requests a single rank - only one rank/process is required for VS Code</li> <li><code>--pty</code> allows forwarding of terminal I/O, for bash to work interactively</li> </ul> <p></p>"},{"location":"access/vscode/#using-with-containers","title":"Using with containers","text":"<p>This will use CSCS\u2019s Container Engine, to launch the container on a compute node and start the VS Code server.</p> EDF file with image and mount paths<pre><code>image = \"nvcr.io#nvidia/pytorch:24.01-py3\" # example of PyTorch NGC image\nwritable = true\nmounts = [\"/paths/on/scratch/or/home:path/on/the/container\",\n          \"/path/if/same/on/both\"\n          \"/path/of/code/executable:/path/for/code/executable/in/container\"]\nworkdir = \"default/working/dir/path\"\n</code></pre> <p>Note</p> <p>Ensure that the <code>code</code> executable is accessible in the container. It can either be contained in the image, or you can install and update the server in a path that you mount inside the container in the <code>mounts</code> field of the EDF file.</p> <p>Log into the target system, and launch an interactive session with the container image: <pre><code># launch container on compute node\n$ srun -N 1 --environment=/absolute/path/to/tomlfile.toml --pty bash\n</code></pre></p> <p>Then on the compute node, you can start the tunnel manually, following the prompts to log in via GitHub: <pre><code>$ cd path/for/code/executable/in/container\n$ ./code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p>"},{"location":"access/vscode/#connecting-via-vs-code-ui","title":"Connecting via VS Code UI","text":"<p>Warning</p> <p>This approach is not recommended, and is not supported by CSCS.</p> <p>It is relatively easy to connect to a log in node using the \u201cConnect to Host\u2026 (Remote-SSH)\u201d option in the VS Code GUI on your laptop. However, it is complicated and difficult to configure the connection so that the environment used by the VS Code session is in a uenv/container or on a compute node.</p>"},{"location":"access/web/","title":"Web Services","text":""},{"location":"access/web/#accessing-cscs-web-portals","title":"Accessing CSCS Web Portals","text":"<p>Most services at CSCS are connected to the CSCS Single Sign-On gate. This gives users the comfort of not having to sign in multiple times in each individual service connected to this gate and increases security. Furthermore, the Single Sign-On gate allow users to recover their forgotten passwords and authenticate using a third-party account. The login page looks like</p> <p></p>"},{"location":"access/web/#using-mfa-to-access-web-based-services","title":"Using MFA to access web-based services","text":"<p>After having completed the setup of MFA, you will be asked to enter your login/password and the OTP to access all web-based services.</p> <p>Enter username and password.</p> <p></p> <p>Then you will be prompted to enter the 6-digit code obtained from your device.</p> <p></p>"},{"location":"accounts/","title":"Index","text":""},{"location":"accounts/#getting-and-managing-accounts","title":"Getting and Managing Accounts","text":"<p>Users at CSCS have one account that can be used to access all services and systems at CSCS. To get an account you must be invited by a member of CSCS project adminstration or by a the principle investigator (PI) of a current project at CSCS.</p> <p>Getting a project at CSCS for PIs</p> <p>In order to get an account at CSCS, or to request access for the members of your team, you first need to get a project at CSCS. CSCS issues calls for proposals that are announced via the CSCS website and e-mails. More information about upcoming calls is available on the CSCS web site.</p> <p>New PIs who have successfully applied for a preparatory project will receive an invitation from CSCS to get an account at CSCS. PIs can then invite members of their groups to join their project.</p> <p>Info</p> <p>It is possible for users to be part of multiple projects by being invited separately by the PI of each project.</p> <p>Note</p> <p>Accounts are bound to projects, and accounts will be closed with the project unless the account is also part of another open project.</p>"},{"location":"accounts/#tools-for-managing-accounts-and-projects","title":"Tools for managing accounts and projects","text":"<p>The tool used to manage projects and accounts depends on the platform on which the project was granted:</p> <ul> <li>The HPC Platform and Climate and Weather Platform use the account and resources management tool at account.cscs.ch</li> <li>The Machine Learning Platform uses the project and resources management tool at portal.cscs.ch.</li> </ul> <p>Note</p> <p>The portal.cscs.ch site will be used to manage all projects in the future.</p>"},{"location":"accounts/#signing-up-for-a-new-account","title":"Signing up for a new account","text":"<p>New users who do not already have an account at CSCS, including PIs, need to provide the following information before CSCS can open their account:</p> <ul> <li>a scanned copy of your passport or recognised id card.<ul> <li>this will be deleted by CSCS immediately after the account has been created.</li> </ul> </li> <li>an institutional email address (gmail, hotmail, etc. will not be accepted)</li> <li>correct information (title, name, etc.)</li> </ul> <p>New accounts are usually opened within 48 hours.</p>"},{"location":"accounts/#using-different-accounts","title":"Using different accounts","text":"<p>In order to use a different account, log out of the Single Sign-On gate by going to the Account and Resources Tool and selecting \u201cLog out of CSCS\u201d on the upper-right profile icon with the tool used to manage your project, account.cscs.ch or portal.cscs.ch.</p>"},{"location":"accounts/#signing-in-with-a-third-part-account","title":"Signing in with a third-part account","text":"<p>All users at CSCS need to go through the standard registration process and get a CSCS account. In addition, they can also link their CSCS account to an external account, e.g. the one from their home institution. In this case, they can sign into the CSCS services using his/her home institution credentials instead of the CSCS username/password. This process happens only during the Single Sign-On procedure described above, and from that time on and for all purposes, and until the user logs out, the user identifier that presents itself to all CSCS services is the CSCS username, not the external one. The number of external institutions that are allowed to link their accounts is limited and displayed in the login page.</p> <p>Linking an external account can be done in the Profile section (upper-right corner) of your account page at the tool used to manage your project, account.cscs.ch or portal.cscs.ch.</p>"},{"location":"accounts/#regulations-and-policies","title":"Regulations and Policies","text":"<p>Please note that as soon as you receive and accept an invitation to get an account at CSCS, you agree to the CSCS/ETHZ regulations.</p>"},{"location":"accounts/account-create/","title":"Creating a new account","text":""},{"location":"accounts/account-create/#creating-an-account","title":"Creating an account","text":"<p>Warning</p> <p>The process for creating an account documented here applies only to users who are invited using the new project and resource management tool (Waldur).</p> <p>When the CSCS Account Manager, project PI or Deputy PI invites the users they will receive an invitation email if the invited user has an existing CSCS account then the user clicks on the URL from the email and log-in with a username, password, OTP, and accept the invitation whereas if the invited user is new then the user needs to follow the step-by-step instructions below to get an account</p> <p>The email contains a URL that redirects you to the registration page:</p> <p></p> <p>Clicking the \u201cCreate a new account\u201d button will lead the user to the second step where he needs to provide his personal information as shown below:</p> <p></p> <p>After submitting personal information, users have to wait for CSCS to review and approve the submission.</p> <p>Once accepted, you will receive an email with a link to set your password.</p> Acceptance email<pre><code>Dear John Doe,\n\nYour username is nchallap.\n\nPlease click here to set your password.\n\nYours sincerely,\n\nCSCS Support Team.\n</code></pre> <p>Following the link in this email will take you to a page where you set your password.</p> <p></p> <p>After your password has been set, you will be redirected to a page where you log in using your username and password</p> <p></p> <p>From here you will need to set up multi-factor authentication (MFA).</p> <p>Once MFA has been configured, you will finally be redirected to the CSCS portal as shown:</p> <p></p>"},{"location":"accounts/ump/","title":"Account and Resources Management Tool","text":""},{"location":"accounts/ump/#account-and-resources-management-tool","title":"Account and Resources Management Tool","text":"<p>The Swiss National Supercomputing Centre (CSCS) offers a web-based tool for users to manage their accounts and projects at account.cscs.ch.</p> <p>With this tool, users can:</p> <ul> <li>Access their profile, manage institutional details, or reset their password.</li> <li>List the projects they belong to, including closed ones.</li> <li>Check details on each project, quotas, and current utilization.</li> <li>Get an overview of where their files are stored at CSCS (including home directories, scratch, etc.).</li> </ul> <p>For group leaders (or PIs), the tool allows:</p> <ul> <li>Managing user membership and access control.</li> <li>Inviting users to their projects via email. Existing users can accept immediately, while new users will receive instructions to create an account and join the project.</li> <li>Removing users from their projects.</li> <li>Selecting which users can access a system (and submit jobs) and which ones can only access project data.</li> <li>Defining one or more deputies to perform such tasks. Note: The responsibility of what happens within the project still belongs to the group leader or PI.</li> </ul> <p>A short guideline on how to perform these tasks is provided below.</p>"},{"location":"accounts/ump/#usage","title":"Usage","text":"<p>The tool is designed to be intuitive and comprises the following main areas:</p> <ul> <li>A) Account selector: For users with multiple accounts (e.g., service accounts).</li> <li>B) Profile management: To view and edit the account\u2019s institutional details and change the password.</li> <li>C) Project membership: To show the selected project in detail.</li> <li>D) Storage: Where users can see where they have stored their files (home, scratch, and project areas).</li> <li>E) Main view</li> </ul> <p></p>"},{"location":"accounts/ump/#membership-management-for-group-leaders-and-deputies-only","title":"Membership Management (for Group Leaders and Deputies Only)","text":"<p>To invite users to a selected project, group leaders or their deputies need to:</p> <ol> <li>Select the project on the left menu.</li> <li>Click the \u201cMembers\u201d tab.</li> <li>Scroll down to the \u201cUsers\u201d (or \u201cDeputies\u201d to manage deputies) section.</li> <li>Use the \u201c+\u201d (plus) button on the right of the section and enter the given and family names and email address of the invitee.    The invitee will receive instructions on how to join the project. The group leader will get a confirmation on whether the invitee has accepted or rejected the invitation.    If the invitee does not have an account, they will also receive instructions on how to create one, which needs to be verified by CSCS administration staff.</li> </ol> <p>To remove users from a selected project, group leaders or their deputies need to:</p> <ol> <li>Repeat steps 1 to 3 above.</li> <li>Use the icon with the three horizontal lines (see screenshot below) that is on the right of the user and select \u201cRemove user.\u201d</li> </ol>"},{"location":"accounts/waldur/","title":"Project and Resources Management Tool","text":""},{"location":"accounts/waldur/#the-project-and-resources-management-tool","title":"The Project and Resources Management Tool","text":"<p>CSCS Account Managers, PIs and deputy PIs can invite users to the respective projects following the below steps on CSCS\u2019s new project management portal.</p> <p>Info</p> <p>The new user project management portal is currently only used by the Machine Learning Platform All other platforms use the old user management portal</p>"},{"location":"accounts/waldur/#log-in-to-the-portal","title":"log in to the portal","text":"<p>Navigate to the site project management portal portal.cscs.ch.</p>"},{"location":"accounts/waldur/#select-the-organisation","title":"Select the Organisation","text":"<p>After login to the portal, choose the corresponding organization in which the project was created.</p> <p></p> <p>In this example, the project was hosted by the CSCS organization and the project name is <code>csstaff_n</code>. From the organization dashboard navigate to Projects and click on the <code>csstaff_n</code> project</p> <p></p>"},{"location":"accounts/waldur/#invite-users","title":"Invite users","text":"<p>From the project dashboard, navigate to Team -&gt; Invitations</p> <p></p> <p>Info</p> <p>Using both the web interface and bulk invitation, the following roles can be assigned in the tool:</p> <ul> <li>Project administrator: PI</li> <li>Project manager:  deputy PI</li> <li>Project member: team member</li> </ul> invite individual usersbulk invite <p>To invite an individual user, click on the green \u201cInvite Users\u201d button on the right hand side of the tab.</p> <p>You will then be prompted to enter the invitee\u2019s email address and assign them a role (PI, deputy PI or member)</p> <p></p> <p>Role definitions</p> <p>The Waldur tool uses the following labels for the roles:</p> <ul> <li>Project administrator: PI</li> <li>Project manager: deputy PI</li> <li>Project member: member</li> </ul> <p>It is also possible to bulk invite users by preparing a CSV file and uploading it in this step.</p> <pre><code>Email,Role,Project\nCragAlvarado@example.com,Project member,prj02\nAndrease@example.com,Project member,prj02\nJoannWaters@example.com,Project administrator,prj02\nDonnaSchwartz@example.com,Project manager,prj02\n</code></pre> <p>Note</p> <p>An email will be sent to invited users:</p> <ul> <li>users who already have CSCS accounts should click on the link in the email they received, and authenticate against CSCS KeyCloak with username, password, and OTP to accept the invitation.</li> <li>new users should follow the procedure to create a CSCS account.</li> </ul>"},{"location":"alps/","title":"Index","text":""},{"location":"alps/#alps-infrastructure","title":"Alps Infrastructure","text":"<p>Alps is a general-purpose compute and data Research Infrastructure (RI) open to the broad community of researchers in Switzerland and the rest of the world. Alps provides a high impact, challenging and innovative RI that will allows Switzerland to advance science and impact society.</p> <p>Alps enables the creation of versatile clusters (vClusters) that can be tailored to the specific needs of users while maintaining confidentiality. For example, a vCluster will be dedicated to MeteoSwiss\u2019 numerical weather forecasts, another one to the User Lab and another one to Machine Learning and Artificial Intelligence.</p> <p>A key feature of Alps is multi-tenancy, where tenants are organizations, typically a research institution, that deploys, operates, or manages its platform on the Alps infrastructure. Tenants have privileged access to resource nodes, enabling them to deploy their own services and resource configurations. Additionally, network segregation ensures secure and isolated communication, with the option to connect to the tenant\u2019s private network.</p> <ul> <li> <p> Platforms</p> <p> Alps Platforms</p> </li> <li> <p> Clusters</p> <p>The resources on Alps are partitioned and configured into versatile software defined clusters.</p> <p> Alps Clusters</p> </li> <li> <p> Hardware</p> <p>Learn about the node types and networking infrastructure in Alps.</p> <p> Alps Hardware</p> </li> <li> <p> Storage</p> <p>Learn about the file systems attached to Alps.</p> <p> Alps Storage</p> </li> </ul>"},{"location":"alps/hardware/","title":"Hardware","text":""},{"location":"alps/hardware/#alps-hardware","title":"Alps Hardware","text":"<p>Alps is a HPE Cray EX3000 system, a liquid cooled blade-based, high-density system.</p> <p>Under-construction</p> <p>This page is a work in progress - contact us if you want us to prioritise documentation specific information that would be useful for your work.</p>"},{"location":"alps/hardware/#alps-cabinets","title":"Alps Cabinets","text":"<p>The basic building block of the system is a liquid-cooled cabinet. A single cabinet can accommodate up to 64 compute blade slots within 8 compute chassis. The cabinet is not configured with any cooling fans. All cooling needs for the cabinet are provided by direct liquid cooling and the CDU. This approach to cooling provides greater efficiency for the rack-level cooling, decreases power costs associated with cooling (no blowers) and utilizes a single water source per CDU One cabinet supports the following:</p> <ul> <li>8 compute chassis</li> <li>4 power shelves with a maximum of 6 rectifiers per shelf- 24 total 12.5 or 15kW rectifiers per cabinet</li> <li>4 PDUs (1 per power shelf)</li> <li>3 power input whips (3-phase)</li> <li>Maximum of 64 quad-blade compute blades</li> <li>Maximum of 64 Slingshot switch blades</li> </ul> <p></p>"},{"location":"alps/hardware/#alps-high-speed-network","title":"Alps High Speed Network","text":"<p>Todo</p> <p>information about the network.</p> <ul> <li>Details about SlingShot 11.<ul> <li>how many NICs per node</li> <li>raw feeds and speeds</li> </ul> </li> <li>Some OSU benchmark results.</li> <li>GPU-aware communication</li> <li>slingshot is not infiniband - there is no NVSwitch</li> </ul>"},{"location":"alps/hardware/#alps-nodes","title":"Alps Nodes","text":"<p>Alps was installed in phases, starting with the installation of 1024 AMD Rome dual socket CPU nodes in 2020, through to the main installation of 2,688 Grace-Hopper nodes in 2024.</p> <p>There are currently five node types in Alps:</p> type abbreviation blades nodes CPU sockets GPU devices NVIDIA GH200 gh200 1344 2688 10,752 10,752 AMD Rome zen2 256 1024 2,048 \u2013 NVIDIA A100 a100 72 144 144 576 AMD MI250x mi200 12 24 24 96 AMD MI300A mi300 64 128 512 512 <p></p>"},{"location":"alps/hardware/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>Under-construction</p> <p>The description of the GH200 nodes is a work in progress. We will add more detailed information soon. Please get in touch if there is information that you want to see here.</p> <p>There are 24 cabinets, in 4 rows with 6 cabinets per row, and each cabinet contains 112 nodes (for a total of 448 GH200):</p> <ul> <li>8 chassis per cabinet</li> <li>7 blades per chassis</li> <li>2 nodes per blade</li> </ul> <p>Why 7 blades per chassis?</p> <p>A chassis can contain up to 8 blades, however Alps\u2019 gh200 chassis are underpopulated so that we can increase the amount of power delivered to each GPU without exceeding the power limit of each cabinet.</p> <p>Each node contains four Grace-Hopper modules and four corresponding network interface cards (NICs) per blade, as illustrated below:</p> <p></p> Node xname <p>There are two boards per blade with one node per board. This is different to the <code>zen2</code> CPU-only nodes (used for example in Eiger) that have two nodes per board for a total of four nodes per blade. As such, there are no <code>n1</code> nodes in the xname list, e.g.: <pre><code>x1100c0s6b0n0\nx1100c0s6b1n0\n</code></pre></p> Core-to-core latency <p>The core-to-core latency on a Grace CPU (collected using the <code>core-to-core-latency</code> program):</p> <p></p> <p>The latencies between the first cores on each of the four Grace CPUs within a node:</p> <p></p> <p>Note the significantly higher latencies compared to within a single Grace CPU.</p> <p></p>"},{"location":"alps/hardware/#amd-rome-cpu-nodes","title":"AMD Rome CPU Nodes","text":"<p>These nodes have two AMD Epyc 7742 64-core CPU sockets, and are used primarily for the Eiger system. They come in two memory configurations:</p> <ul> <li>Standard-memory:  256 GB in 16x16 GB DDR4 DIMMs.</li> <li>Large-memory:  512 GB in 16x32 GB DDR4 DIMMs.</li> </ul> <p>Not all memory is available</p> <p>The total memory available to jobs on the nodes is roughly 245 GB and 497 GB on the standard and large memory nodes respectively.</p> <p>The amount of memory available to your job also depends on the number of MPI ranks per node\u2014each MPI rank has a memory overhead.</p> <p>A schematic of a standard memory node below illustrates the CPU cores and NUMA nodes.(1)</p> <ol> <li>Obtained with the command <code>lstopo --no-caches --no-io --no-legend eiger-topo.png</code> on Eiger.</li> </ol> <p></p> <ul> <li>The two sockets are labelled Package L#0 and Package L#1.</li> <li>Each socket has 4 NUMA nodes, with 16 cores each, for a total of 64 cores per socket.</li> </ul> <p>Each core supports simultaneous multi threading (SMT), whereby each core can execute two threads concurrently, which are presented as two processing units (PU) per physical core:</p> <ul> <li>the first PU on each core are numbered 0:63 on socket 0, and 64:127 on socket 1;</li> <li>the second PU on each core are numbered 128:191 on socket 0, and 192:256 on socket 1;</li> <li>hence, core <code>n</code> has PUs <code>n</code> and <code>n+128</code>.</li> </ul> <p>Each node has two Slingshot 11 network interface cards (NICs), which are not illustrated on the diagram.</p> <p></p>"},{"location":"alps/hardware/#nvidia-a100-gpu-nodes","title":"NVIDIA A100 GPU Nodes","text":"<p>The Grizzly Peak blades contain two nodes, where each node has:</p> <ul> <li>One 64-core Zen3 CPU socket</li> <li>512 GB DDR4 Memory</li> <li>4 NVIDIA A100 GPUs with 80 GB HBM3 memory each<ul> <li>The MCH system is the same, except the A100 have 96 GB of memory.</li> </ul> </li> <li>4 NICs\u2014one per GPU.</li> </ul> <p></p>"},{"location":"alps/hardware/#amd-mi250x-gpu-nodes","title":"AMD MI250x GPU Nodes","text":"<p>Todo</p> <p>Bard Peak</p> <p></p>"},{"location":"alps/hardware/#amd-mi300a-gpu-nodes","title":"AMD MI300A GPU Nodes","text":"<p>Todo</p> <p>Parry Peak</p>"},{"location":"alps/platforms/","title":"Platforms","text":""},{"location":"alps/platforms/#platforms-on-alps","title":"Platforms on Alps","text":"<p>A platform represents a set of scientific services along with compute and data resources hosted on the Alps research infrastructure, provided to a specific scientific community. Each platform addresses particular research needs and domains, such as climate and weather modeling, machine learning, or high-performance computing applications. A platform can consist of one or multiple clusters, and its services can be managed either by CSCS or by the scientific community itself, including access control, usage policies, and support.</p> <p>The following platforms are fully operated by CSCS:</p> <ul> <li> <p> HPC Platform</p> <p>The HPC Platform (HPCP) provides services for the HPC community in Switzerland and abroad. The majority of compute cycles are provided to the User Lab via peer-reviewed allocation schemes.</p> <p> HPCP</p> </li> <li> <p> Machine Learning Platform</p> <p>The Machine Learning Platform (MLP) hosts ML and AI researchers, particularly the SwissAI initiative.</p> <p> MLP</p> </li> <li> <p> Climate and Weather Platform</p> <p>The Climate and Weather Platform (CWP) provides resources to the climate modeling community.</p> <p> CWP</p> </li> </ul> <p>The following platforms are jointly operated by different Swiss projects and institutes and CSCS. Software and policies are tailored depending on the use case, and are documented separately.</p> <ul> <li> <p> Merlin 7</p> <p>The Merlin resources are available to all PSI staff and external collaborators.</p> <p> Merlin</p> </li> <li> <p> Materials Cloud</p> <p>Materials Cloud is built to enable the seamless sharing and dissemination of resources in computational materials science.</p> <p> Materials Cloud</p> </li> <li> <p> Numerical weather forecasting</p> <p>MeteoSwiss uses a numerical weather forecasting model for the production of regional and local forecast products in the Alpine region.</p> <p> NWP</p> </li> </ul> <p>Under-construction</p> <p>Add the WLCG, CTA and SKA platforms in here as well, with their respective external links.</p>"},{"location":"alps/storage/","title":"Storage","text":""},{"location":"alps/storage/#alps-storage","title":"Alps Storage","text":"<p>Under-construction</p> <p>The Alps infrastructure offers multiple storage solutions, each with characteristics suited to different workloads and use cases. HPC storage is provided by independent clusters, composed of servers and physical storage drives.</p> Capstor Iopsstor VAST Model HPE ClusterStor E1000D HPE ClusterStor E1000F VAST Type Lustre Lustre NFS Capacity 129 PB raw GridRAID 7.2 PB raw RAID 10 1 PB Number of Drives 8,480 16 TB HDD 240 * 30 TB NVMe SSD N/A Read Speed 1.19 TB/s 782 GB/s 38 GB/s Write Speed 1.09 TB/s 393 GB/s 11 GB/s IOPs 1.5M 8.6M read, 24M write 200k read, 768k write file create/s 374k 214k 97k <p>Capstor and Iopsstor are on the same Slingshot network as Alps, while VAST is on the CSCS Ethernet network.</p> <p>See the Lustre guide for some hints on how to get the best performance out of the filesystem.</p> <p>The mounts, and how they are used for Scratch, Store, and Home file systems that are mounted on clusters are documented in the file system docs.</p> <p></p>"},{"location":"alps/storage/#capstor","title":"Capstor","text":"<p>Capstor is the largest file system, and it is meant for storing large amounts of input and output data. It is used to provide scratch and store.</p> <p>Capstor has 80 Object Storage Servers (OSS), and 6 Metadata Servers (MDS).  Two of of these Metadata servers are dedicated for Store, and the remaining four are dedicated for Scratch.</p> <p></p>"},{"location":"alps/storage/#scratch","title":"Scratch","text":"<p>All users on Alps get their own scratch path on Alps, <code>/capstor/scratch/cscs/$USER</code>. Since Capstor OSSs are made of HDDs, Capstor is a storage well suited for jobs which perform large sequential and parallel read/write operations. See the Scratch documentation for more information.</p> <p></p>"},{"location":"alps/storage/#store","title":"Store","text":"<p>The Store mount point on Capstor provides stable storage with backups and no cleaning policy. It is mounted on clusters at the <code>/capstor/store</code> mount point, with folders created for each project.</p> <p></p>"},{"location":"alps/storage/#iopsstor","title":"Iopsstor","text":"<p>Iopsstor is a smaller filesystem compared to Capstor, but it leverages high-performance NVMe drives, which offer significantly better speed and responsiveness than traditional HDDs. It is primarily used as a scratch space, and it is optimized for IOPS-intensive workloads.  This makes it particularly well-suited for applications that involve frequent, random read and write operations within files.</p> <p>Iopsstor has has 20 OSSs, and 2 MDSs. </p> <p></p>"},{"location":"alps/storage/#vast","title":"VAST","text":"<p>The VAST storage is smaller capacity system that is designed for use as Home folders.</p> <p>Todo</p> <p>small text explaining what VAST is designed to be used for.</p>"},{"location":"build-install/","title":"Index","text":""},{"location":"build-install/#building-and-installing-software","title":"Building and Installing Software","text":"<p>CSCS provides commonly used software and tools on Alps, however many use cases will require first installing software on a system before you can start working. Modern HPC applications and software stacks are often very complicated, and there is no one-size-fits-all method for building and installing them.</p> <ul> <li> <p> Programming Environments</p> <p>Programming environments are your first option if you want to install an application (and its dependencies) from source, or set up a Python/Julia environment.</p> <p>CSCS provides the following uenv:</p> <p> prgenv-gnu</p> <p> prgenv-nvfortran</p> <p> linalg</p> <p> julia</p> <p>And containers are used to deploy:</p> <p> Cray Programming Environment</p> </li> </ul> <ul> <li> <p> Packaging and Deployment</p> <p>How to create containers or uenv, and how to share them with your colleagues and community.</p> <p> build containers with podman</p> <p> use the uenv build service</p> </li> </ul> <ul> <li> <p> Software Building Guides</p> <p>How to create containers or uenv, and how to share them with your colleagues and community.</p> <p> building software using uenv</p> <p>Coming soon: how to install Python software stacks.</p> </li> </ul>"},{"location":"build-install/containers/","title":"Creating Containers with podman","text":""},{"location":"build-install/containers/#building-container-images-on-alps","title":"Building container images on Alps","text":"<p>Building OCI container images on Alps vClusters is supported through Podman, an open-source container engine that adheres to OCI standards and supports rootless containers by leveraging Linux user namespaces. Its command-line interface (CLI) closely mirrors Docker\u2019s, providing a consistent and familiar experience for users of established container tools.</p> <p></p>"},{"location":"build-install/containers/#preliminary-step-configuring-podmans-storage","title":"Preliminary step: configuring Podman\u2019s storage","text":"<p>The first step in order to use Podman on Alps is to create a valid Container Storage configuration file in your home directory, according to the following minimal template:</p> $HOME/.config/containers/storage.conf<pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/dev/shm/$USER/runroot\"\ngraphroot = \"/dev/shm/$USER/root\"\n</code></pre> <p>Warning</p> <p>If <code>$XDG_CONFIG_HOME</code> is set, place this file at <code>$XDG_CONFIG_HOME/containers/storage.conf</code> instead. See the terminal user guide for further information about XDG variables.</p> <p>Warning</p> <p>In the above configuration, <code>/dev/shm</code> is used to store the container images. <code>/dev/shm</code> is the mount point of a tmpfs filesystem and is compatible with the user namespaces used by Podman. The limitation of this approach  is that container images created during a job allocation are deleted when the job ends. Therefore, the image needs to either be pushed to a container registry or imported by the Container Engine before the job allocation finishes.</p> <p>You can use</p> <pre><code>podman info | grep -A 2 \"store:\"\n</code></pre> <p>to check that the correct <code>storage.conf</code> file is used by Podman (<code>store:configFile</code> field).</p>"},{"location":"build-install/containers/#building-images-with-podman","title":"Building images with Podman","text":"<p>The easiest way to build a container image is to rely on a Containerfile (a more generic name for a container image recipe, but essentially equivalent to Dockerfile):</p> <pre><code># Allocate a compute node and open an interactive terminal on it\nsrun --pty --partition=&lt;partition&gt; bash\n\n# Change to the directory containing the Containerfile/Dockerfile and build the image\npodman build -t &lt;image:tag&gt; .\n</code></pre> <p>In general, <code>podman build</code> follows the Docker options convention.</p> <p>Debugging the container build</p> <p>If the container build fails, you can run an interactive shell using the image from the last successfully built layer with</p> <pre><code>podman run -it --rm -e NVIDIA_VISIBLE_DEVICES=void &lt;last-layer-hash&gt; bash # (1)!\n</code></pre> <ol> <li>Setting <code>NVIDIA_VISIBLE_DEVICES</code> in the environment is required specifically to run NGC containers with podman</li> </ol> <p>replacing <code>&lt;last-layer-hash&gt;</code> with the actual hash output in the build job and interactively test the failing command.</p>"},{"location":"build-install/containers/#importing-images-in-the-container-engine","title":"Importing images in the Container Engine","text":"<p>An image built using Podman can be easily imported as a squashfs archive in order to be used with our Container Engine solution. It is important to keep in mind that the import has to take place in the same job allocation where the image creation took place, otherwise the image is lost due to the temporary nature of <code>/dev/shm</code>.</p> <p>Preliminary configuration: Lustre settings for container images</p> <p>Container images are stored in a single SquashFS file, that is typically between 1-20 GB in size (particularly for large ML containers). To ensure good performance for jobs on multiple nodes, take the time to configure the target directory using <code>lfs setstripe</code> according to best practices for Lustre before importing the container image, or using <code>lfs migrate</code> to fix files that are already imported.</p> <p>To import the image:</p> <pre><code>enroot import -x mount -o &lt;image_name.sqsh&gt; podman://&lt;image:tag&gt;\n</code></pre> <p>The resulting <code>&lt;image_name.sqsh&gt;</code> can used directly as an explicitly pulled container image, as documented in Container Engine. An example Environment Definition File (EDF) using the imported image looks as follows:</p> <pre><code>image = \"/&lt;path to image directory&gt;/&lt;image_name.sqsh&gt;\"\nmounts = [\"/capstor/scratch/cscs/&lt;username&gt;:/capstor/scratch/cscs/&lt;username&gt;\"]\nworkdir = \"/capstor/scratch/cscs/&lt;username&gt;\"\n</code></pre>"},{"location":"build-install/containers/#pushing-images-to-a-container-registry","title":"Pushing Images to a Container Registry","text":"<p>In order to push an image to a container registry, you first need to follow three steps:</p> <ol> <li>Use your credential to login to the container registry with podman login.</li> <li>Tag the image according to the name of your container registry and the corresponding repository, using podman tag. This step can be skipped if you already provided the appropriate tag when building the image.</li> <li>Push the image using podman push.</li> </ol> <pre><code># Login to a container registry using username/password interactively\npodman login &lt;registry_url&gt;\n\n# Tag the image accordingly\npodman tag &lt;image:tag&gt; &lt;registry url&gt;/&lt;image:tag&gt;\n\n# Push the image (for docker type registries use the docker:// prefix)\npodman push docker://&lt;registry url&gt;/&lt;image:tag&gt;\n</code></pre> <p>For example, to push an image to the DockerHub container registry, the following steps have to be performed:</p> <pre><code># Login to DockerHub (Podman will ask for your credentials)\npodman login docker.io\n\n# Tag the image based on your username\npodman tag &lt;image:tag&gt; docker.io/&lt;username&gt;/myimage:latest\n\n# Push the image to the repository of your choice\npodman push docker://docker.io/&lt;username&gt;/myimage:latest\n</code></pre>"},{"location":"build-install/uenv/","title":"uenv","text":""},{"location":"build-install/uenv/#building-with-uenv","title":"Building with uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools on Alps.  This article explains how to use them to build software.</p> <p>For more documentation on how to find, download and use uenv in your workflow, see the uenv documentation.</p> <p></p>"},{"location":"build-install/uenv/#building-software-using-spack","title":"Building software using Spack","text":"<p>Each uenv is tightly coupled with Spack and can be used as an upstream Spack instance, because the software in uenv is built with Spack using the Stackinator tool.</p> <p>CSCS provides <code>uenv-spack</code> - a tool that can be used to quickly install software using the software and configuration provided inside a uenv, similarly to how <code>module load</code> loads software packages.</p>"},{"location":"build-install/uenv/#installing-uenv-spack","title":"Installing <code>uenv-spack</code>","text":"<pre><code>git clone https://github.com/eth-cscs/uenv-spack.git # (1)!\n\n(cd uenv-spack &amp;&amp; ./bootstrap) # (2)!\n\nexport PATH=$PWD/uenv-spack:$PATH\n</code></pre> <ol> <li> <p>Download the <code>uenv-spack</code> tool from GitHub.</p> </li> <li> <p>Initialize the <code>uenv-spack</code> tool.</p> </li> </ol>"},{"location":"build-install/uenv/#select-the-uenv","title":"Select the uenv","text":"<p>The next step is to choose which uenv to use. The uenv will provide the compilers, Cray MPICH, and other libraries and tools.</p> <pre><code>graph TD\n  A[/is there a uenv for the application?\\] --&gt;|yes| B[use that image, e.g. **gromacs**]\n  A --&gt; |no| C[/do I need OpenACC or CUDA Fortran?\\]\n  C --&gt; |no| D[use **prgenv-gnu**]\n  C --&gt; |yes| E[/are you _really_ sure?\\]\n  E --&gt; |yes| F[use **prgenv-nvfortran**]\n  E --&gt; |no| D</code></pre> <p>Use <code>prgenv-gnu</code> when in doubt</p> <p>If you don\u2019t know where to start, use the latest release of the <code>prgenv-gnu</code> on the system that you are targeting. It provides the latest versions of <code>gcc</code>, <code>cray-mpich</code>, <code>python</code> and commonly used libraries like <code>fftw</code> and <code>boost</code>.</p> <p>On systems that have NVIDIA GPUs (<code>gh200</code> and <code>a100</code> uarch), it also provides the latest version of <code>cuda</code> and <code>nccl</code>, and it is configured for GPU-aware MPI communication.</p> <p>To use an uenv as an upstream Spack instance, the uenv has to be started with the <code>spack</code> view:</p> <pre><code>uenv start prgenv-gnu/24.11:v1 --view=spack\n</code></pre> <p>What does the <code>spack</code> view do?</p> <p>The <code>spack</code> view sets environment variables that provide information about the version of Spack that was used to build the uenv, and where the uenv Spack configuration is stored.</p> variable example description <code>UENV_SPACK_CONFIG_PATH</code> <code>user-environment/config</code> the path of the upstream spack configuration files. <code>UENV_SPACK_REF</code> <code>releases/v0.23</code> the branch or tag used - this might be empty if a specific commit of Spack was used. <code>UENV_SPACK_URL</code> <code>https://github.com/spack/spack.git</code> The git repository for Spack - nearly always the main spack/spack repository. <code>UENV_SPACK_COMMIT</code> <code>c6d4037758140fe...0cd1547f388ae51</code> The commit of Spack that was used <p>Warning</p> <p>The environment variables set by the <code>spack</code> view are scoped by <code>UENV_</code>. Therefore, they don\u2019t change Spack-related environment variables. You can use them to consistently set Spack-related environment variables.</p> Upstream Spack version <p>It is strongly recommended that your version of Spack and the version of Spack in the uenv match when building software on top of an uenv.</p> <p>Advanced Spack users</p> <p>Advanced Spack users can use the environment variables set by the <code>spack</code> view to manually configure the uenv as a Spack upstream instance.</p> <p>Tip</p> <p>If using multiple uenvs, we recommend using a different Spack instance per uenv.</p> Setting Spack configuration path <pre><code>export SPACK_SYSTEM_CONFIG_PATH=$UENV_SPACK_CONFIG_PATH\n</code></pre>"},{"location":"build-install/uenv/#describing-what-to-build","title":"Describing what to build","text":"<p>The next step is to describe what software to build. This is done using a Spack environment file and a Spack package repository.</p> <p>The <code>uenv-spack</code> tool can be used to create a build directory with a template Spack environment file (<code>spack.yaml</code>) and a Spack package repository (<code>repo/</code> directory).</p> <p>Create a build directory with a Spack environment file and a Spack package repository</p> <pre><code>uenv-spack &lt;build-path&gt; --uarch=gh200\ncd &lt;build-path&gt;\n./build\n</code></pre> <p><code>&lt;build-path&gt;</code> is a path (typically in <code>$SCRATCH</code>, e.g. <code>$SCRATCH/builds/gromacs-24.11</code>).</p> <p><code>uenv-spack</code> creates a directory tree with the following contents:</p> <pre><code>&lt;build-path&gt;\n\u251c\u2500 build # (1)!\n\u251c\u2500 spack # (2)!\n\u251c\u2500 config # (3)!\n\u2502   \u251c\u2500 meta.json # (4)!\n\u2502   \u251c\u2500 user\n\u2502   \u2502  \u251c\u2500 config.yaml\n\u2502   \u2502  \u251c\u2500 modules.yaml\n\u2502   \u2502  \u2514\u2500 repos.yaml\n\u2502   \u2514\u2500 system\n\u2502      \u251c\u2500 compilers.yaml\n\u2502      \u251c\u2500 packages.yaml\n\u2502      \u251c\u2500 repos.yaml\n\u2502      \u2514\u2500 upstreams.yaml\n\u2514\u2500 env # (5)!\n    \u251c\u2500 spack.yaml # (6)!\n    \u2514\u2500 repo # (7)!\n       \u251c\u2500 repo.yaml\n       \u2514\u2500 packages\n</code></pre> <ol> <li>Script to build the software stack.</li> <li><code>git</code> clone of the required version of Spack.</li> <li>Spack configuration files for the software stack.</li> <li>Information about the uenv that was used to run <code>uenv-spack</code>.</li> <li>Description of the software to build.</li> <li>Template Spack environment file.</li> <li>Empty Spack package repository.</li> </ol> <p>The <code>env</code> path contains a template <code>spack.yaml</code> file, and an empty Spack package repository:</p> <pre><code>env\n\u251c\u2500 spack.yaml\n\u2514\u2500 repo\n   \u251c\u2500 repo.yaml\n   \u2514\u2500 packages\n</code></pre> <p>where the <code>spack.yaml</code> file contains an empty list of specs:</p> <pre><code>    specs: []\n</code></pre> <p>Edit this file to add the specs that you wish to build, for example:</p> <pre><code>    specs: [tree, screen, emacs +treesitter]\n</code></pre> <p>The step of adding a list of specs to the <code>spack.yaml</code> template can be skipped by providing them using the <code>--specs</code> argument to <code>uenv-spack</code>.</p> <p>Create a build path and populate the <code>spack.yaml</code> file with some Spack specs</p> <pre><code>uenv-spack $SCRATCH/install/tools --uarch=gh200 \\\n           --specs=\"tree, screen, emacs +treesitter\"\ncd $SCRATCH/install/tools\n./build\n</code></pre> <p>If you already have a directory with a complete <code>spack.yaml</code> file and custom repo, you can provide it as an argument to <code>uenv-spack</code>:</p> <p>Create a build path and use a pre-configured <code>spack.yaml</code> and <code>repo</code></p> <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-recipe&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre> Create a build path and use your own <code>spack.yaml</code> <p>NOT YET IMPLEMENTED</p> <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-spack.yaml&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre>"},{"location":"build-install/uenv/#build-the-software","title":"Build the software","text":"<p>Once specs have been added to <code>spack.yaml</code>, you can build the image using the <code>build</code> script that was generated in <code>&lt;build-path&gt;</code>:</p> <pre><code>./build\n</code></pre> <p>This process will take a while, because the version of Spack that was downloaded needs to</p> <ul> <li>bootstrap,</li> <li>concretise the environment,</li> <li>and build all of the packages.</li> </ul> <p>The duration of the build depends on the specs: some specs may require a long time to build, or require installing many dependencies.</p> <p>The build step generates multiple outputs, described below.</p>"},{"location":"build-install/uenv/#installed-packages","title":"Installed packages","text":"<p>The packages built by Spack are installed in <code>&lt;build-path&gt;/store</code>.</p>"},{"location":"build-install/uenv/#spack-view","title":"Spack view","text":"<p>A Spack view is generated in <code>&lt;build-path&gt;/view</code>.</p>"},{"location":"build-install/uenv/#modules","title":"Modules","text":"<p>Module files are generated in the <code>module</code> sub-directory of the <code>&lt;build-path&gt;</code></p> <p>To use them, add them to the module environment</p> <pre><code>module use &lt;build-path&gt;/modules # (1)!\nmodule avail # (2)!\n</code></pre> <ol> <li>Make modules available.</li> <li>Check that the modules are available.</li> </ol> <p>Note</p> <p>The generation of modules can be customised by editing the <code>&lt;build-path&gt;/config/user/modules.yaml</code> file before running <code>build</code>. See the Spack modules documentation.</p>"},{"location":"build-install/uenv/#use-the-software","title":"Use the software","text":"<p>Warning</p> <p>This step is not fully covered by the tool/workflow yet.</p> <p>Warning</p> <p>The uenv that was used to configure and build must always be loaded when using the software stack.</p> <p>To use the installed software, you have the following options:</p> <ul> <li>Loading modules</li> <li>Activate the Spack view</li> <li><code>source &lt;build-path&gt;/spack/share/spack/setup-env.sh</code> and then use Spack</li> </ul>"},{"location":"clusters/","title":"Index","text":""},{"location":"clusters/#alps-clusters","title":"Alps Clusters","text":"<p>A vCluster (versatile software-defined cluster) is a logical partition of the supercomputing resources where platform services are deployed. It serves as a dedicated environment supporting a specific platform. The composition of resources and services for each vCluster is defined in a configuration file used by an automated pipeline for deployment. Once deployed by CSCS, the vCluster becomes immutable.</p>"},{"location":"clusters/#clusters-on-alps","title":"Clusters on Alps","text":"<p>Clusters on Alps are provided as part of different platforms. The following clusters are part of the platforms that are fully operated by CSCS.</p> <ul> <li> <p> Machine Learning Platform</p> <p>Clariden is the main Grace-Hopper cluster</p> <p> Clariden</p> <p>Bristen is a small system with A100 nodes used for data processing, development, x86 workloads and ML inference services.</p> <p> Bristen</p> </li> </ul> <ul> <li> <p> HPC Platform</p> <p>Daint is the main Grace-Hopper cluster for GPU workloads</p> <p> Daint</p> <p>Eiger is a large AMD-CPU cluster for CPU workloads</p> <p> Eiger</p> </li> </ul> <ul> <li> <p> Climate and Weather Platform</p> <p>Santis is a Grace-Hopper cluster for climate and weather simulation</p> <p> Santis</p> </li> </ul>"},{"location":"clusters/#other-systems","title":"Other systems","text":"<ul> <li> <p> Porting and Development</p> <p>Besso is a small system used by some partners for development and porting with AMD and NVIDIA GPUs.</p> <p> Besso</p> </li> </ul>"},{"location":"clusters/besso/","title":"Besso","text":""},{"location":"clusters/besso/#besso","title":"Besso","text":"<p>Besso is a small Alps cluster that provides development resources for porting software for selected customers. It is provided as is, without the same level of support as the main platform clusters.</p>"},{"location":"clusters/besso/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Besso uses the HPCP filesystems and storage policies.</p>"},{"location":"clusters/besso/#getting-started","title":"Getting started","text":""},{"location":"clusters/besso/#logging-into-besso","title":"Logging into Besso","text":"<p>To connect to Besso via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to besso using <code>ssh besso</code>. <pre><code>Host besso\n    HostName besso.vc.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/besso/#software","title":"Software","text":""},{"location":"clusters/besso/#uenv","title":"uenv","text":"<p>Besso is a development and testing system, for which CSCS does not provide supported applications.</p> <p>Instead, the prgenv-gnu programming environment is provided for the both the a100 and mi200 node types.</p> <p></p>"},{"location":"clusters/besso/#containers","title":"Containers","text":"<p>Besso supports container workloads using the Container Engine.</p> <p>To build images, see the guide to building container images on Alps.</p>"},{"location":"clusters/besso/#cray-modules","title":"Cray Modules","text":"<p>Warning</p> <p>The Cray Programming Environment (CPE), loaded using <code>module load cray</code>, is no longer supported by CSCS.</p> <p>CSCS will continue to support and update uenv and the Container Engine, and users are encouraged to update their workflows to use these methods at the first opportunity.</p> <p>The CPE is still installed on Besso, however it will receive no support or updates, and will be replaced with a container in a future update.</p>"},{"location":"clusters/besso/#running-jobs-on-besso","title":"Running jobs on Besso","text":""},{"location":"clusters/besso/#slurm","title":"Slurm","text":"<p>Besso uses Slurm as the workload manager, which is used to launch and monitor workloads on compute nodes.</p> <p>There are multiple Slurm partitions on the system:</p> <ul> <li>the <code>a100</code> partition contains NVIDIA A100 GPU nodes</li> <li>the <code>mi200</code> partition contains AMD Mi250x GPU nodes</li> <li>the <code>normal</code> partition contains all of the nodes in the system.</li> </ul> name max nodes per job time limit <code>a100</code> 2 24 hours <code>mi200</code> 2 24 hours <code>normal</code> 4 24 hours <p>See the Slurm documentation for instructions on how to run jobs.</p>"},{"location":"clusters/besso/#firecrest","title":"FirecREST","text":"<p>Besso can also be accessed using FirecREST at the <code>https://api.cscs.ch/mwa/firecrest/v2</code> API endpoint.</p>"},{"location":"clusters/besso/#maintenance-and-status","title":"Maintenance and status","text":"<p>There is no regular scheduled maintenance for this system.</p>"},{"location":"clusters/bristen/","title":"Bristen","text":""},{"location":"clusters/bristen/#bristen","title":"Bristen","text":"<p>Bristen is an Alps cluster that provides GPU accelerators and filesystems designed to meet the needs of machine learning workloads in the MLP.</p>"},{"location":"clusters/bristen/#cluster-specification","title":"Cluster Specification","text":""},{"location":"clusters/bristen/#compute-nodes","title":"Compute Nodes","text":"<p>Bristen consists of 32 A100 nodes NVIDIA A100 nodes. The number of nodes can change when nodes are added or removed from other clusters on Alps.</p> node type number of nodes total CPU sockets total GPUs a100 32 32 128 <p>Nodes are in the <code>normal</code> Slurm partition.</p>"},{"location":"clusters/bristen/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Bristen uses the MLP filesystems and storage policies.</p>"},{"location":"clusters/bristen/#getting-started","title":"Getting started","text":""},{"location":"clusters/bristen/#logging-into-bristen","title":"Logging into Bristen","text":"<p>To connect to Bristen via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to bristen using <code>ssh bristen</code>. <pre><code>Host bristen\n    HostName bristen.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/bristen/#software","title":"Software","text":"<p>Users are encouraged to use containers on Bristen.</p> <ul> <li>Jobs using containers can be easily set up and submitted using the container engine.</li> <li>To build images, see the guide to building container images on Alps.</li> </ul>"},{"location":"clusters/bristen/#running-jobs-on-bristen","title":"Running Jobs on Bristen","text":""},{"location":"clusters/bristen/#slurm","title":"Slurm","text":"<p>Bristen uses Slurm as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>There is currently a single Slurm partition on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.<ul> <li>nodes in this partition are not shared.</li> </ul> </li> </ul> name nodes max nodes per job time limit <code>normal</code> 32 - 24 hours"},{"location":"clusters/bristen/#firecrest","title":"FirecREST","text":"<p>Bristen can also be accessed using FirecREST at the <code>https://api.cscs.ch/ml/firecrest/v1</code> API endpoint.</p>"},{"location":"clusters/bristen/#scheduled-maintenance","title":"Scheduled Maintenance","text":"<p>Wednesday morning 8-12 CET is reserved for periodic updates, with services potentially unavailable during this timeframe. If the queues must be drained (redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window.</p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/bristen/#change-log","title":"Change log","text":"<p>2025-03-05 container engine updated</p> <p>now supports better containers that go faster. Users do not to change their workflow to take advantage of these updates.</p>"},{"location":"clusters/bristen/#known-issues","title":"Known issues","text":""},{"location":"clusters/clariden/","title":"Clariden","text":""},{"location":"clusters/clariden/#clariden","title":"Clariden","text":"<p>Clariden is an Alps cluster that provides GPU accelerators and file systems designed to meet the needs of machine learning workloads in the MLP.</p>"},{"location":"clusters/clariden/#cluster-specification","title":"Cluster Specification","text":""},{"location":"clusters/clariden/#compute-nodes","title":"Compute Nodes","text":"<p>Clariden consists of around 1200 Grace-Hopper nodes. The number of nodes can change when nodes are added or removed from other clusters on Alps.</p> node type number of nodes total CPU sockets total GPUs gh200 1,200 4,800 4,800 <p>Most nodes are in the <code>normal</code> Slurm partition, while a few nodes are in the <code>debug</code> partition.</p>"},{"location":"clusters/clariden/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Clariden uses the MLP filesystems and storage policies.</p>"},{"location":"clusters/clariden/#getting-started","title":"Getting started","text":""},{"location":"clusters/clariden/#logging-into-clariden","title":"Logging into Clariden","text":"<p>To connect to Clariden via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to clariden using <code>ssh clariden</code>. <pre><code>Host clariden\n    HostName clariden.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/clariden/#software","title":"Software","text":"<p>Users are encouraged to use containers on Clariden.</p> <ul> <li>Jobs using containers can be easily set up and submitted using the container engine.</li> <li>To build images, see the guide to building container images on Alps.</li> <li>Base images which include the necessary libraries and compilers are for example available from the Nvidia NGC Catalog:<ul> <li>HPC NGC container</li> <li>PyTorch NGC container</li> </ul> </li> </ul> <p>Alternatively, uenv are also available on Clariden. Currently deployed on Clariden:</p> <ul> <li>prgenv-gnu</li> <li>pytorch</li> </ul> using uenv provided for other clusters <p>You can run uenv that were built for other Alps clusters using the <code>@</code> notation. For example, to use uenv images for daint: <pre><code># list all images available for daint\nuenv image find @daint\n\n# download an image for daint\nuenv image pull namd/3.0:v3@daint\n\n# start the uenv\nuenv start namd/3.0:v3@daint\n</code></pre></p> <p>For detailed instructions and best practices with ML frameworks, please refer to the dedicated pages under ML software.</p>"},{"location":"clusters/clariden/#running-jobs-on-clariden","title":"Running Jobs on Clariden","text":""},{"location":"clusters/clariden/#slurm","title":"Slurm","text":"<p>Clariden uses Slurm as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>There are two Slurm partitions on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes.</li> <li>the <code>xfer</code> partition is for internal data transfer at CSCS.</li> </ul> name nodes max nodes per job time limit <code>normal</code> 1204 - 12 hours <code>debug</code> 24 2 1.5 node-hours <code>xfer</code> 2 1 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> <li>nodes in the <code>debug</code> queue have a 1.5 node-hour time limit. This means you could for example request 2 nodes for 45 minutes each, or 1 single node for the full time limit.</li> </ul> <p>See the Slurm documentation for instructions on how to run jobs on the Grace-Hopper nodes.</p> how to check the number of nodes on the system <p>You can check the size of the system by running the following command in the terminal: <pre><code>$ sinfo --format \"| %20R | %10D | %10s | %10l | %10A |\"\n| PARTITION            | NODES      | JOB_SIZE   | TIMELIMIT  | NODES(A/I) |\n| debug                | 32         | 1-2        | 30:00      | 3/29       |\n| normal               | 1266       | 1-infinite | 1-00:00:00 | 812/371    |\n| xfer                 | 2          | 1          | 1-00:00:00 | 1/1        |\n</code></pre> The last column shows the number of nodes that have been allocated in currently running jobs (<code>A</code>) and the number of jobs that are idle (<code>I</code>).</p>"},{"location":"clusters/clariden/#firecrest","title":"FirecREST","text":"<p>Clariden can also be accessed using FirecREST at the <code>https://api.cscs.ch/ml/firecrest/v1</code> API endpoint.</p>"},{"location":"clusters/clariden/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/clariden/#scheduled-maintenance","title":"Scheduled Maintenance","text":"<p>Wednesday morning 8-12 CET is reserved for periodic updates, with services potentially unavailable during this timeframe. If the queues must be drained (redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/clariden/#change-log","title":"Change log","text":"<p>2025-03-05 container engine updated</p> <p>now supports better containers that go faster. Users do not to change their workflow to take advantage of these updates.</p> 2024-10-07 old event <p>this is an old update. Use <code>???</code> to automatically fold the update.</p>"},{"location":"clusters/clariden/#known-issues","title":"Known issues","text":""},{"location":"clusters/daint/","title":"Daint","text":""},{"location":"clusters/daint/#daint","title":"Daint","text":"<p>Daint is the main HPC Platform cluster that provides compute nodes and file systems for GPU-enabled workloads.</p>"},{"location":"clusters/daint/#cluster-specification","title":"Cluster specification","text":""},{"location":"clusters/daint/#compute-nodes","title":"Compute nodes","text":"<p>Daint consists of around 800-1000 Grace-Hopper nodes.</p> <p>The number of nodes can vary as nodes are added or removed from other clusters on Alps. See the Slurm documentation for information on how to check the number of nodes.</p> <p>There are four login nodes, <code>daint-ln00[1-4]</code>. You will be assigned to one of the four login nodes when you ssh onto the system, from where you can edit files, compile applications and launch batch jobs.</p> node type number of nodes total CPU sockets total GPUs gh200 1,022 4,088 4,088"},{"location":"clusters/daint/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Daint uses the HPCP filesystems and storage policies.</p>"},{"location":"clusters/daint/#getting-started","title":"Getting started","text":""},{"location":"clusters/daint/#logging-into-daint","title":"Logging into Daint","text":"<p>To connect to Daint via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to Daint using <code>ssh daint</code>. <pre><code>Host daint\n    HostName daint.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/daint/#software","title":"Software","text":""},{"location":"clusters/daint/#uenv","title":"uenv","text":"<p>Daint provides uenv to deliver programming environments and application software. Please refer to the uenv documentation for detailed information on how to use the uenv tools on the system.</p> <ul> <li> <p> Scientific Applications</p> <p>Provide the latest versions of scientific applications, tuned for Daint, and the tools required to build your own versions of the applications.</p> <ul> <li>CP2K</li> <li>GROMACS</li> <li>LAMMPS</li> <li>NAMD</li> <li>Quantumespresso</li> <li>VASP</li> </ul> </li> </ul> <ul> <li> <p> Programming Environments</p> <p>Provide compilers, MPI, Python, common libraries and tools used to build your own applications.</p> <ul> <li>prgenv-gnu</li> <li>prgenv-nvfortran</li> <li>linalg</li> <li>julia</li> </ul> </li> </ul> <ul> <li> <p> Tools</p> <p>Provide tools like </p> <ul> <li>Linaro Forge</li> </ul> </li> </ul> <p></p>"},{"location":"clusters/daint/#containers","title":"Containers","text":"<p>Daint supports container workloads using the container engine.</p> <p>To build images, see the guide to building container images on Alps.</p>"},{"location":"clusters/daint/#cray-modules","title":"Cray Modules","text":"<p>Warning</p> <p>The Cray Programming Environment (CPE), loaded using <code>module load cray</code>, is no longer supported by CSCS.</p> <p>CSCS will continue to support and update uenv and container engine, and users are encouraged to update their workflows to use these methods at the first opportunity.</p> <p>The CPE is still installed on Daint, however it will receive no support or updates, and will be replaced with a container in a future update.</p>"},{"location":"clusters/daint/#running-jobs-on-daint","title":"Running jobs on Daint","text":""},{"location":"clusters/daint/#slurm","title":"Slurm","text":"<p>Daint uses Slurm as the workload manager, which is used to launch and monitor compute-intensive workloads.</p> <p>There are four Slurm partitions on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes.</li> <li>the <code>xfer</code> partition is for internal data transfer.</li> <li>the <code>low</code> partition is a low-priority partition, which may be enabled for specific projects at specific times.</li> </ul> name nodes max nodes per job time limit <code>normal</code> unlimited - 24 hours <code>debug</code> 24 2 30 minutes <code>xfer</code> 2 1 24 hours <code>low</code> unlimited - 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> (and <code>low</code>) partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> </ul> <p>See the Slurm documentation for instructions on how to run jobs on the Grace-Hopper nodes.</p>"},{"location":"clusters/daint/#firecrest","title":"FirecREST","text":"<p>Daint can also be accessed using FirecREST at the <code>https://api.cscs.ch/hpc/firecrest/v2</code> API endpoint.</p> <p>The FirecREST v1 API is still available, but deprecated</p>"},{"location":"clusters/daint/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/daint/#scheduled-maintenance","title":"Scheduled maintenance","text":"<p>move this to HPCP top level docs</p> <p>Wednesday mornings 8:00-12:00 CET are reserved for periodic updates, with services potentially unavailable during this time frame. If the batch queues must be drained (for redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/daint/#change-log","title":"Change log","text":"<p>2025-05-21</p> <p>Minor enhancements to system configuration have been applied. These changes should reduce the frequency of compute nodes being marked as <code>NOT_RESPONDING</code> by the workload manager, while we continue to investigate the issue</p> <p>2025-05-14</p> Performance hotfix <p>The access-counter-based memory migration feature in the NVIDIA driver for Grace Hopper is disabled to address performance issues affecting NCCL-based workloads (e.g. LLM training)</p> NVIDIA boost slider <p>Added an option to enable the NVIDIA boost slider (vboost) via Slurm using the <code>-C nvidia_vboost_enabled</code> flag. This feature, disabled by default, may increase GPU frequency and performance while staying within the power budget</p> Enroot update <p>The container runtime is upgraded from version 2.12.0 to 2.13.0. This update includes libfabric version 1.22.0 (previously 1.15.2.0), which has demonstrated improved performance during LLM checkpointing</p> <p>2025-04-30</p> uenv is updated from v7.0.1 to v8.1.0 <p>Release notes</p> Pyxis is upgraded from v24.5.0 to v24.5.3 <ul> <li>Added image caching for Enroot</li> <li>Added support for environment variable expansion in EDFs</li> <li>Added support for relative paths expansion in EDFs</li> <li>Print a message about the experimental status of the \u2013environment option when used outside of the srun command</li> <li>Merged small features and bug fixes from upstream Pyxis releases v0.16.0 to v0.20.0</li> <li>Internal changes: various bug fixes and refactoring</li> </ul> 2025-03-12 <ol> <li>The number of compute nodes has been increased to 1018</li> <li>The restriction on the number of running jobs per project has been lifted.</li> <li>A \u201clow\u201d priority partition has been added, which allows some project types to consume up to 130% of the project\u2019s quarterly allocation</li> <li>We have increased the power cap for the GH module from 624 to 660 W. You might see increased application performance as a consequence </li> <li>Small changes in kernel tuning parameters</li> </ol>"},{"location":"clusters/daint/#known-issues","title":"Known issues","text":"<p>Todo</p> <p>Most of these issues (see original KB docs) should be consolidated in a location where they can be linked to by all clusters.</p> <p>We have some \u201cknow issues\u201d documented under communication libraries, however these might be a bit too disperse for centralised linking.</p>"},{"location":"clusters/eiger/","title":"Eiger","text":""},{"location":"clusters/eiger/#eiger","title":"Eiger","text":"<p>Eiger is an Alps cluster that provides compute nodes and file systems designed to meet the needs of CPU-only workloads for the HPC Platform.</p> <p>Note</p> <p>This documentation is for the updated cluster <code>Eiger.Alps</code> reachable at <code>eiger.alps.cscs.ch</code>, that replaced the former cluster as on July 1 2025.</p> Important changes from Eiger <p>The redeployment of <code>eiger.cscs.ch</code> as <code>eiger.alps.cscs.ch</code> has introduced changes that may affect some users.</p>"},{"location":"clusters/eiger/#breaking-changes","title":"Breaking changes","text":"<p>Sarus is replaced with the Container Engine</p> <p>The Sarus container runtime is replaced with the Container Engine.</p> <p>If you are using Sarus to run containers on Eiger, you will have to rebuild and adapt your containers for the Container Engine.</p> <p>Cray modules and EasyBuild are no longer supported</p> <p>The Cray Programming Environment (accessed via the <code>cray</code> module) is no longer supported by CSCS, along with software that CSCS provided using EasyBuild.</p> <p>The same version of the Cray modules is still available, along with software that was installed using them, however they will not receive updates or support from CSCS.</p> <p>You are strongly encouraged to start using uenv to access supported applications and to rebuild your own applications.</p> <ul> <li>The versions of compilers, <code>cray-mpich</code>, Python and libraries in uenv are up to date.</li> <li>The scientific application uenv have up to date versions of the supported applications.</li> </ul>"},{"location":"clusters/eiger/#minor-changes","title":"Minor changes","text":"<p>Slurm is updated from version 23.02.6 to 24.05.4</p>"},{"location":"clusters/eiger/#cluster-specification","title":"Cluster specification","text":""},{"location":"clusters/eiger/#compute-nodes","title":"Compute nodes","text":"<p>Eiger consists of multicore AMD Epyc Rome compute nodes: please note that the total number of available compute nodes on the system might vary over time. See the Slurm documentation for information on how to check the number of nodes.</p> <p>Additionally, there are four login nodes with host names <code>eiger-ln00[1-4]</code>.</p>"},{"location":"clusters/eiger/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Eiger uses the HPCP filesystems and storage policies.</p>"},{"location":"clusters/eiger/#getting-started","title":"Getting started","text":""},{"location":"clusters/eiger/#logging-into-eiger","title":"Logging into Eiger","text":"<p>To connect to Eiger via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to eiger using <code>ssh eiger.alps</code>. <pre><code>Host eiger.alps\n    HostName eiger.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/eiger/#software","title":"Software","text":""},{"location":"clusters/eiger/#uenv","title":"uenv","text":"<p>CSCS and the user community provide uenv software environments on Eiger.</p> <ul> <li> <p> Scientific Applications</p> <p>Provide the latest versions of scientific applications, tuned for Eiger, and the tools required to build your own version of the applications.</p> <ul> <li>CP2K</li> <li>GROMACS</li> <li>LAMMPS</li> <li>NAMD</li> <li>Quantumespresso</li> <li>VASP</li> </ul> </li> </ul> <ul> <li> <p> Programming Environments</p> <p>Provide compilers, MPI, Python, common libraries and tools used to build your own applications.</p> <ul> <li>prgenv-gnu</li> <li>linalg</li> <li>julia</li> </ul> </li> </ul> <ul> <li> <p> Tools</p> <p>Provide tools like </p> <ul> <li>Linaro Forge</li> </ul> </li> </ul> <p></p>"},{"location":"clusters/eiger/#containers","title":"Containers","text":"<p>Eiger supports container workloads using the Container Engine.</p> <p>To build images, see the guide to building container images on Alps.</p> <p>Sarus is not available</p> <p>A key change with the new Eiger deployment is that the Sarus container runtime is replaced with the Container Engine.</p> <p>If you are using Sarus to run containers on Eiger, you will have to rebuild and adapt your containers for the Container Engine.</p>"},{"location":"clusters/eiger/#cray-modules","title":"Cray Modules","text":"<p>Warning</p> <p>The Cray Programming Environment (CPE), loaded using <code>module load cray</code>, is no longer supported by CSCS.</p> <p>CSCS will continue to support and update uenv and the Container Engine, and users are encouraged to update their workflows to use these methods at the first opportunity.</p> <p>The CPE is still installed on Eiger, however it will receive no support or updates, and will be replaced with a container in a future update.</p>"},{"location":"clusters/eiger/#running-jobs-on-eiger","title":"Running jobs on Eiger","text":""},{"location":"clusters/eiger/#slurm","title":"Slurm","text":"<p>Eiger uses Slurm as the workload manager, which is used to launch and monitor workloads on compute nodes.</p> <p>There are multiple Slurm partitions on the system:</p> <ul> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes</li> <li>the <code>prepost</code> partition is meant for small high priority allocations up to 30 minutes, for pre- and post-processing jobs.</li> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>xfer</code> partition is for internal data transfer.</li> <li>the <code>low</code> partition is a low-priority partition, which may be enabled for specific projects at specific times.</li> </ul> name max nodes per job time limit <code>debug</code> 1 30 minutes <code>prepost</code> 1 30 minutes <code>normal</code> - 24 hours <code>xfer</code> 1 24 hours <code>low</code> - 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> </ul> <p>See the Slurm documentation for instructions on how to run jobs on the AMD CPU nodes.</p>"},{"location":"clusters/eiger/#jupyterhub","title":"JupyterHub","text":"<p>A JupyterHub service for Eiger is available at https://jupyter-eiger.cscs.ch.</p>"},{"location":"clusters/eiger/#firecrest","title":"FirecREST","text":"<p>Eiger can also be accessed using FirecREST at the <code>https://api.cscs.ch/hpc/firecrest/v2</code> API endpoint.</p> <p>The FirecREST v1 API is still available, but deprecated</p>"},{"location":"clusters/eiger/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/eiger/#scheduled-maintenance","title":"Scheduled maintenance","text":"<p>Wednesday mornings 8:00-12:00 CET are reserved for periodic updates, with services potentially unavailable during this time frame. If the batch queues must be drained (for redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, the CSCS Status Page and the #eiger channel of the CSCS User Slack.</p>"},{"location":"clusters/eiger/#change-log","title":"Change log","text":"<p>2025-06-05 Early access phase</p> <p>Early access phase is open</p> 2025-05-23 Creation of Eiger on Alps <p>Eiger is deployed as a vServices-enabled cluster</p>"},{"location":"clusters/eiger/#known-issues","title":"Known issues","text":""},{"location":"clusters/santis/","title":"Santis","text":""},{"location":"clusters/santis/#santis","title":"Santis","text":"<p>Santis is an Alps cluster that provides GPU accelerators and file systems designed to meet the needs of climate and weather models for the CWP.</p>"},{"location":"clusters/santis/#cluster-specification","title":"Cluster specification","text":""},{"location":"clusters/santis/#compute-nodes","title":"Compute nodes","text":"<p>Santis consists of around 430 Grace-Hopper nodes.</p> <p>The number of nodes can change when nodes are added or removed from other clusters on Alps.</p> <p>There are four login nodes, labelled <code>santis-ln00[1-4]</code>. You will be assigned to one of the four login nodes when you ssh onto the system, from where you can edit files, compile applications and start simulation jobs.</p> node type number of nodes total CPU sockets total GPUs gh200 430 1,720 1,720"},{"location":"clusters/santis/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Santis uses the CWP filesystems and storage policies.</p>"},{"location":"clusters/santis/#getting-started","title":"Getting started","text":""},{"location":"clusters/santis/#logging-into-santis","title":"Logging into Santis","text":"<p>To connect to Santis via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to santis using <code>ssh santis</code>. <pre><code>Host santis\n    HostName santis.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/santis/#software","title":"Software","text":"<p>CSCS and the user community provide software environments tailored to  uenv are also available on Santis.</p> <p>Currently, the following uenv are provided for the climate and weather community</p> <ul> <li><code>icon/25.1</code></li> <li><code>climana/25.1</code></li> </ul> <p>In addition to the climate and weather uenv, all of the</p> using uenv provided for other clusters <p>You can run uenv that were built for other Alps clusters using the <code>@</code> notation. For example, to use uenv images for daint: <pre><code># list all images available for daint\nuenv image find @daint\n\n# download an image for daint\nuenv image pull namd/3.0:v3@daint\n\n# start the uenv\nuenv start namd/3.0:v3@daint\n</code></pre></p> <p>It is also possible to use HPC containers on Santis:</p> <ul> <li>Jobs using containers can be easily set up and submitted using the container engine.</li> <li>To build images, see the guide to building container images on Alps.</li> </ul>"},{"location":"clusters/santis/#running-jobs-on-santis","title":"Running jobs on Santis","text":""},{"location":"clusters/santis/#slurm","title":"Slurm","text":"<p>Santis uses Slurm as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>There are two Slurm partitions on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes.</li> <li>the <code>xfer</code> partition is for internal data transfer at CSCS.</li> </ul> name nodes max nodes per job time limit <code>normal</code> 1266 - 24 hours <code>debug</code> 32 2 30 minutes <code>xfer</code> 2 1 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> </ul> <p>See the Slurm documentation for instructions on how to run jobs on the Grace-Hopper nodes.</p>"},{"location":"clusters/santis/#firecrest","title":"FirecREST","text":"<p>Santis can also be accessed using FirecREST at the <code>https://api.cscs.ch/ml/firecrest/v2</code> API endpoint.</p> <p>The FirecREST v1 API is still available, but deprecated</p>"},{"location":"clusters/santis/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/santis/#scheduled-maintenance","title":"Scheduled maintenance","text":"<p>Wednesday morning 8-12 CET is reserved for periodic updates, with services potentially unavailable during this timeframe. If the queues must be drained (redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/santis/#change-log","title":"Change log","text":"<p>2025-03-05 container engine updated</p> <p>now supports better containers that go faster. Users do not to change their workflow to take advantage of these updates.</p> 2024-10-07 old event <p>this is an old update. Use <code>???</code> to automatically fold the update.</p>"},{"location":"clusters/santis/#known-issues","title":"Known issues","text":""},{"location":"contributing/","title":"Index","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>This documentation is developed using the Material for MkDocs framework, and the source code for the docs is publicly available on GitHub. This means that everybody, CSCS staff and the CSCS user community can contribute to the documentation.</p>"},{"location":"contributing/#making-suggestions-or-small-changes","title":"Making suggestions or small changes","text":"<p>If you have a suggestion, comment or small change to make when reading the documentation, there are three ways to reach out.</p> <ol> <li>Edit the page inline: click on the  icon on the top right hand corner of each page, and make the change inline. When you click \u201ccommit\u201d, and create a pull request, which will then be reviewed by the CSCS docs team.</li> <li>Create a GitHub issue: create an issue on the issue page on the GitHub repository.</li> <li>Create a CSCS service desk ticket: create a ticket on the CSCS service desk.     This is useful if you don\u2019t have a GitHub account, or would prefer not to use Github.</li> </ol>"},{"location":"contributing/#before-starting","title":"Before starting","text":"<p>The CSCS documentation takes contributions from all CSCS staff, with a core team of maintainers responsible for ensuring that the overall documentation is well organised, that pages are well written and up to date, and that contributions are reviewed and merged as quickly as possible.</p> Who are the core team? <p>The docs core team are:</p> <ul> <li>Ben Cumming (@bcumming);</li> <li>Mikael Simberg (@msimberg);</li> <li>and Rocco Meli (@RMeli).</li> </ul> <p>We are volunteers for this role, who care about the quality of CSCS documentation!</p> <p>Before contributing</p> <p>Please read the guidelines and style guide before making any contribution. Consistency and common practices make it easier for users to read and navigate the documentation, make it easier for regular contributors to write, and avoid style debates. We try to strike a balance between following the guidelines and letting authors write in a style that is comfortable for them.</p> <p>To speed up the merge process and avoid lengthy style discussions, we reserve the right to make changes to pull requests to bring it into line with the guidelines. The core team will also update pages when they are out of date or when the style guidelines change.</p> <p>Before making large contributions</p> <p>If you plan to make large changes, like adding documentation for a new tool/service or refactoring existing documentation, reach out to the core team before starting.</p> <p>This will mean that the changes are consistent with other parts of the documentation, streamline the review process, and to avoid misunderstandings.</p>"},{"location":"contributing/#code-owners","title":"Code owners","text":"<p>Many sections have individual staff that follow them. This is codified in the CODEOWNERS file in the repository. The code owners are notified when there is a change to their pages, and can review the changes.</p> <p>If you want to follow changes to a page or section, add your name to the CODEOWNERS.</p> <p>Note</p> <p>Review from code owners is not required to merge, however the core team will try to get a timely review from code owners whenever possible.</p>"},{"location":"contributing/#getting-started","title":"Getting started","text":"<p>We use the GitHub fork and pull request model for development:</p> <ul> <li>First create a fork of the main GitHub repository.</li> <li>Make all proposed changes in branches on your fork - don\u2019t make branches on the main repository (we reserve the right to block creating branches on the main repository).</li> </ul> <p>Clone your fork repository on your PC/laptop: <pre><code># clone your fork of the repository\ngit clone git@github.com:${githubusername}/cscs-docs.git\ncd cscs-docs\n# create a branch for your changes (here we are fixing the ssh docs)\ngit switch -c 'fix/ssh-alias'\n# ... make your edits ...\n</code></pre></p> <p>Review your edits checking the Guidelines section below.</p> <p>Note</p> <p>Note that a simple editor markdown preview may not render all the features of the documentation.</p> <p>To properly review the docs locally, the <code>serve</code> script in the root path of the repository can be used as shown below: <pre><code>./serve\n...\nINFO    -  [08:33:34] Serving on http://127.0.0.1:8000/\n</code></pre></p> <p>Note</p> <p>To run the serve script, you need to first install uv.</p> <p>You can now open your browser at the address shown above (<code>http://127.0.0.1:8000/</code>). The documentation will be automatically rebuilt and the webpage reloaded on each file change you save.</p> <p>After your first review, commit and push your changes <pre><code>git add &lt;files&gt;\ngit commit -m 'update the ssh docs with aliases for all user lab vclusters'\ngit push origin 'fix/ssh-alias'\n</code></pre></p> <p>Then navigate to GitHub, and create a pull request.</p> <p>Tip</p> <p>If you\u2019ve already created a fork repository, make sure to keep it synced to the main CSCS repository before making further change.</p>"},{"location":"contributing/#review-process","title":"Review process","text":"<p>After you have made a pull request, a CI/CD pipeline will run the spell checker and build a copy of the docs with the PR changes. A temporary \u201cTDS\u201d copy of the docs is deployed, to allow reviewers to see the finished documentation, at the address <code>https://docs.tds.cscs.ch/$PR</code>, where <code>PR</code> is the number of the pull request.</p> <p>To make changes based on reviewer feedback, make a new commit on your branch, and push it to your fork. The PR will automatically be updated, the spell checker will run again, and the TDS documentation site will be rebuilt.</p> <p>Tip</p> <p>If you think your documentation update could affect specific stakeholders, ping them for a review. You can get some hints of whom to contact by looking at CODEOWNERS. If they don\u2019t reply in a timely manner, reach out to the core docs team to expedite the process.</p> <p>Note</p> <p>To minimise the overhead of the contributing to the documentation and speed up \u201ctime-to-published-docs\u201d we do not have a formal review process. We will start simple, and add more formality as needed.</p> <p></p>"},{"location":"contributing/#spell-checker","title":"Spell checker","text":"<p>A spell checker workflow runs on all PRs to help catch simple typos. If the spell checker finds words that it considers misspelled, it will add a comment like this to the PR, listing the words that it finds misspelled.</p> <p>The spell checker isn\u2019t always right and can be configured to ignore words. Most frequently technical terms, project names, etc. will not be in the dictionaries. There are three files used to configure words that get ignored:</p> <ul> <li><code>.github/actions/spelling/allow.txt</code>:   This is the main file for whitelisting words.   Each line of the file contains a word that is ignored by the spell checker.   All lowercase words are matched with any capitalization, while words containing at least one uppercase letter are matched with the given capitalization.   Using the capitalized word is useful if you always want to ensure the same spelling, e.g. for names.</li> </ul> <ul> <li><code>.github/actions/spelling/patterns.txt</code>:   This file is used to ignore words that match a given regular expression.   This file is useful to ignore e.g. URLs or markdown references.   Words that have unusual capitalization may also need to be added to this file to make sure they are ignored.   For example, \u201cFirecREST\u201d is normally recognized as two words: \u201cFirec\u201d and \u201cREST\u201d, and adding \u201cFirecREST\u201d to <code>allow.txt</code> will not ignore the word.   In this case it can be ignored by adding it to <code>patterns.txt</code></li> </ul> <ul> <li><code>.github/actions/spelling/block-delimiters.txt</code>:   This file can be used to ignore words between begin- and end markers.   For example, code blocks starting and ending with <code>```</code> are ignored from spell checking as they often contain unusual words not in dictionaries.   If adding words to <code>allow.txt</code> or <code>patterns.txt</code>, or ignoring blocks with <code>block-delimiters.list</code>, is not sufficient, you can as a last resort use the HTML comments <code>&lt;!--begin no spell check--&gt;</code> and <code>&lt;!--end no spell check--&gt;</code> to ignore spell checking for a larger block of text.   The comments will not be rendered in the final documentation.</li> </ul> <p>Additionally, the file <code>.github/actions/spelling/only.txt</code> contains a list of regular expressions used to match which files to check. Only markdown files under the <code>docs</code> directory are checked.</p> <p></p>"},{"location":"contributing/#guidelines","title":"Guidelines","text":""},{"location":"contributing/#links","title":"Links","text":""},{"location":"contributing/#external-links","title":"External links","text":"<p>Links to external sites use the <code>[]()</code> syntax:</p> external link syntaxresult <pre><code>[The Spack repository](https://github.com/spack/spack)\n</code></pre> <p>The Spack repository</p>"},{"location":"contributing/#internal-links","title":"Internal links","text":"<p>Note</p> <p>The CI/CD pipeline will fail if it detects broken links in your draft documentation. It is not completely foolproof - to ensure that your changes do not create broken links you should merge the most recent version of the <code>main</code> branch of the docs into your branch branch.</p> <p>Adding and maintaining links to internal pages and sections that don\u2019t break or conflict requires care. It is possible to refer to links in other files using relative links, for example <code>[the fast server](../servers.md#fast-server)</code>, however if the target file is moved, or the section title \u201cfast-server\u201d is changed, the link will break.</p> <p>Instead, we advocate adding unique references to sections.</p> adding a referencelinking to a reference <p>Add a reference above the item, in this case we want to link to the section with the title <code>## The fast server</code>:</p> <pre><code>[](){#ref-fast-server}\n## Fast server\n</code></pre> <p>Use the <code>[](){#}</code> syntax to define the reference/anchor.</p> <p>Note</p> <p>Always place the anchor above the item you are linking to.</p> <p>In any other file in the project, use the <code>[][]</code> syntax to refer to the link (note that this link type uses square braces, instead of the usual parenthesis):</p> <pre><code>[the fast server][ref-fast-server]\n</code></pre> <p>The benefits of this approach are that the link won\u2019t break if</p> <ul> <li>either the file containing the link or the file that refers to the link move,</li> <li>or if the title of the target sections changes.</li> </ul>"},{"location":"contributing/#images","title":"Images","text":"<p>A picture is worth a thousand words</p> <p>We encourage the usage of images to improve clarity and understanding. You can use screenshots or diagrams.</p> <p>Images are stored in the <code>docs/images</code> directory.</p> <ul> <li>create a new sub-directory for your images if appropriate</li> <li>choose a path and file name that hint what the image is about - neither <code>screenshot.png</code> nor <code>PX-202502025-imgx.png</code> are great names.</li> </ul> <p>Warning</p> <p>Keep the size of your images to a minimum because we want to keep an overall lightweight repository.</p>"},{"location":"contributing/#screenshots","title":"Screenshots","text":"<p>Screenshots can help readers follow steps on guides. Think if you need to show the whole screen or just part of one window. Cropping the image will decrease file size, and might also draw the readers attention to the most relevant information.</p> <p>Often, screenshots can quickly become obsolete, so you may want to complement (or maybe even replace) some with text descriptions.</p> <p>Tip</p> <p>Screen shots take up space in the git repository.</p> <p>It might be \u201conly a few hundred kilobytes\u201d for a picture, but over the lifetime of the git repository this adds up to slow down source code cloning and CI/CD pipelines.</p> <p>Tip</p> <p>Avoid using screen shots that do not directly contribute to the documentation.</p> <p>For example, showing a screen shot with markers that are used to explain non-trivial steps that a user should follow is good documentation. On the other hand, a screenshot that says \u201cthis is a screenshot of the tool\u201d adds no value, and draws the readers attention away from documentation.</p>"},{"location":"contributing/#diagrams","title":"Diagrams","text":"<p>Diagrams can help readers understand more abstract concepts like processes or architectures. We suggest you use mermaid. Such format makes diagrams easy to maintain and removes the need to commit image files in the repository.</p> Example SourceRendered <pre><code>```mermaid\ngraph TD;\n    Image(Will image add value?);\n    Image--NO--&gt;T(keep text only);\n    Image--YES--&gt;SD(What image is needed?)\n    SD--Screenshot--&gt;S(keep it lean)\n    SD--Diagram--&gt;D(keep it maintainable)\n    D--Default--&gt;M(Mermaid)\n    D--Custom--&gt;DR(Draw.io)\n```\n</code></pre> <pre><code>graph TD;\n    Image(Will image add value?);\n    Image--NO--&gt;T(keep text only);\n    Image--YES--&gt;SD(What image is needed?)\n    SD--Screenshot--&gt;S(keep it lean)\n    SD--Diagram--&gt;D(keep it maintainable)\n    D--Default--&gt;M(Mermaid)\n    D--Custom--&gt;DR(Draw.io)</code></pre> <p>If you need more hand-crafted diagrams, we suggest you use draw.io. Make sure you export the png with the source inside, typically a <code>file.drawio.png</code>, so it can be extended in the future as needed.</p>"},{"location":"contributing/#text-formatting","title":"Text formatting","text":"<p>Turn off automatic line breaks in your text editor, and stick to one sentence per line in paragraphs of text.</p> <p>See the good and bad examples below for an example of of what happens when a change to a sentence forces a line rebalance:</p> goodbad <p>Before: <pre><code>There are many different versions of MPI that can be used for communication.\nThe final choice of which to use is up to you.\n</code></pre></p> <p>After: <pre><code>There are many different versions of the popular MPI communication library that can be used for communication.\nThe final choice of which to use is up to you.\n</code></pre></p> <p>The diff in this case affects only one line.</p> <p>Before: <pre><code>There are many different versions of MPI that\ncan be used for communication. The final choice\nof which to use is up to you.\n</code></pre></p> <p>After: <pre><code>There are many different versions of the popular\nMPI communication library that can be used for\ncommunication. The final choice of which to use\nis up to you.\n</code></pre></p> <p>The diff in this case affects the original 3 lines, and creates a new one.</p> <p>This method defines a canonical representation of text, i.e. there is one and only one way to write a paragraph of text, which plays much better with git.</p> <ul> <li>changes to the text are less likely to create merge conflicts</li> <li>changing one line of text will not modify the surrounding lines (see example above)</li> <li>git diffs and git history are easier to read.</li> </ul>"},{"location":"contributing/#frequently-asked-questions","title":"Frequently asked questions","text":"<p>The documentation does not have a FAQ section, because questions are best answered by the documentation, not in a separate section. Integrating information into the main documentation requires some care to identify where the information needs to go, and edit the documentation around it. Adding the information to a FAQ is easier, but the result is information about a topic distributed between the docs and FAQ questions, which ultimately makes the documentation harder to search.</p> <p>FAQ content, such as lists of most frequently encountered error messages, is still very useful in many contexts. If you want to add such content, create a section at the bottom of a topic page, for example this section on the SSH documentation page.</p>"},{"location":"contributing/#small-contributions","title":"Small contributions","text":"<p>Small changes that only modify the contents of a single file, for example to fix some typos or add some clarifying detail to an example, it is possible to quickly create a pull request directly in the browser.</p> <p>At the top of each page there is an \u201cedit\u201d icon , which will open the markdown source for the page in the GitHub text editor.</p> <p>Once your changes are ready, click on the \u201cCommit changes\u2026\u201d button in the top right hand corner of the editor, and add at least a description commit message.</p> <p>Tip</p> <p>See the GitHub official guide on editing files for a step-by-step walkthrough.</p> <p>Note</p> <p>Use the default option Create a new branch for this commit and start a pull request. This allows others to review the change. Even for trivial changes, opening a PR creates visibility that a small fix or change has been made.</p>"},{"location":"contributing/#style-guide","title":"Style guide","text":"<p>This section contains general guidelines for how to format and present documentation in this repository. They should be followed for most cases, but as a guideline it can be broken, with good reason.</p>"},{"location":"contributing/#headings-are-written-in-sentence-case","title":"Headings are written in sentence case","text":"<p>Use sentence case for headings, meaning all words are capitalized except for minor words.</p>"},{"location":"contributing/#avoid-nesting-headings-too-deep","title":"Avoid nesting headings too deep","text":"<p>Nesting headings up to three levels is generally ok.</p>"},{"location":"contributing/#lists","title":"Lists","text":"<p>Write lists as proper sentences. Separate the items simply with commas if each item is simple, or make each item a full sentence if the items are longer and contain multiple sentences.</p> <ol> <li>The first item can look like this,</li> <li>the second like this, and</li> <li>the third item like this.</li> </ol>"},{"location":"contributing/#using-admonitions","title":"Using admonitions","text":"<p>Aim to include examples, notes, warnings using admonitions whenever appropriate. They stand out better from the main text, and can be collapsed by default if needed.</p> <p>Example one</p> <p>This is an example. The title of the example uses sentence case.</p> Collapsed note <p>This note is collapsed, because it uses <code>???</code>.</p> <p>If an admonition is collapsed by default, it should have a title.</p> <p>We provide some custom admonitions.</p>"},{"location":"contributing/#change","title":"Change","text":"<p>For adding information about a change, originally designed for recording updates to clusters.</p> RenderedMarkdown <p>2025-04-17</p> <ul> <li>Slurm was upgraded to version 25.1.</li> <li>uenv was upgraded to v0.8</li> </ul> <p>Old changes can be folded:</p> 2025-02-04 <ul> <li>The new Scratch cleanup policy was implemented</li> <li>NVIDIA driver was updated</li> </ul> <pre><code>!!! change \"2025-04-17\"\n    * Slurm was upgraded to version 25.1.\n    * uenv was upgraded to v0.8\n</code></pre> <p>Old changes can be folded:</p> <pre><code>??? change \"2025-02-04\"\n    * The new Scratch cleanup policy was implemented\n    * NVIDIA driver was updated\n</code></pre>"},{"location":"contributing/#under-construction","title":"Under construction","text":"<p>For marking incomplete sections.</p> RenderedMarkdown <p>Under-construction</p> <p>This is not finished yet!</p> <pre><code>!!! under-construction\n    This is not finished yet!\n</code></pre>"},{"location":"contributing/#todo","title":"Todo","text":"<p>As a placeholder for documentation that needs to be written.</p> RenderedMarkdown <p>Todo</p> <p>Add some common error messages and how to fix them.</p> <pre><code>!!! todo\n    Add some common error messages and how to fix them.\n</code></pre>"},{"location":"contributing/#code-blocks","title":"Code blocks","text":"<p>Use code blocks when you want to display monospace text in a programming language, terminal output, configuration files etc. The documentation uses pygments for highlighting. See list of available lexers for the languages that you can use for code blocks.</p> <p>Use <code>console</code> for interactive sessions with prompt-output pairs:</p> MarkdownRendered <pre><code>```console title=\"Hello, world!\"\n$ echo \"Hello, world!\"\nHello, world!\n```\n</code></pre> Hello, world!<pre><code>$ echo \"Hello, world!\"\nHello, world!\n</code></pre> <p>Warning</p> <p><code>terminal</code> is not a valid lexer, but MkDocs or pygments will not warn about using it as a language. The text will be rendered without highlighting.</p> <p>Warning</p> <p>Use <code>$</code> as the prompt character, optionally preceded by text. <code>&gt;</code> as the prompt character will not be highlighted correctly.</p> <p>Note the use of <code>title=...</code>, which will give the code block a heading.</p> <p>Tip</p> <p>Include a title whenever possible to describe what the code block does or is.</p> <p>If you want to display commands without output that can easily be copied, use <code>bash</code> as the language:</p> MarkdownRendered <pre><code>```bash title=\"Hello, world!\"\necho \"Hello, world!\"\n```\n</code></pre> Hello, world!<pre><code>echo \"Hello, world!\"\n</code></pre>"},{"location":"contributing/#avoiding-repetition-using-snippets","title":"Avoiding repetition using snippets","text":"<p>It can be useful to repeat information on different pages to increase visibility for users. If possible, prefer linking to a primary section describing a topic instead of fully repeating text on different pages. However, if you believe it\u2019s beneficial to actually repeat the content, consider using snippets to avoid repeated information getting out of sync on different pages. Snippets allow including the contents of a text file in multiple places of the documentation.</p> <p>For example, the recommended NCCL environment variables are defined in a text file <code>docs/software/commuinication/nccl_env_vars</code> and included on multiple pages because it\u2019s essential that users of NCCL notice and use the environment variables.</p> <p>Snippets are included with <code>--8&lt;-- path/to/snippet</code>. For example, to include the recommended NCCL environment variables, do the following:</p> MarkdownRendered <pre><code>```bash\n--8&lt;-- \"docs/software/communication/nccl_env_vars\"\n```\n</code></pre> Recommended NCCL environment variables<pre><code># This forces NCCL to use the libfabric plugin, enabling full use of the\n# Slingshot network. If the plugin can not be found, applications will fail to\n# start. With the default value, applications would instead fall back to e.g.\n# TCP, which would be significantly slower than with the plugin. More information\n# about `NCCL_NET` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net\nexport NCCL_NET=\"AWS Libfabric\"\n# Use GPU Direct RDMA when GPU and NIC are on the same NUMA node. More\n# information about `NCCL_NET_GDR_LEVEL` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net-gdr-level-formerly-nccl-ib-gdr-level\nexport NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\n# Starting with nccl 2.27 a new protocol (LL128) was enabled by default, which\n# typically performs worse on Slingshot. The following disables that protocol.\nexport NCCL_PROTO=^LL128\n# These `FI` (libfabric) environment variables have been found to give the best\n# performance on the Alps network across a wide range of applications. Specific\n# applications may perform better with other values.\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_DEFAULT_TX_SIZE=32768\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_CXI_RX_MATCH_MODE=software\nexport FI_MR_CACHE_MONITOR=userfaultfd\n</code></pre>"},{"location":"contributing/#documentation-structure","title":"Documentation structure","text":"<p>Here we describe a high-level overview of the documentation layout and organisation.</p> <p>Under-construction</p> <p>This section is mostly incomplete, and will be expanded over time.</p> <p>Note that the directory layout, where markdown files are stored in the repository, does not strictly reflect the section of the documentation where the content is displayed because:</p> <ul> <li>the URL of a page is decided by its location in the directory tree, not in the table of contents.   If a page is moved in the ToC, we are conservative about moving the file, so that urls don\u2019t break.</li> <li>pages can be included in multiple locations in the ToC (not a feature that we use very often).</li> </ul>"},{"location":"contributing/#tutorials","title":"Tutorials","text":"<p>All tutorials are stored in the <code>/docs/tutorials</code> directory. Currently we only have ML tutorials in <code>/docs/tutorials/ml</code>.</p> <p>There is no top level \u201cTutorials\u201d section, instead tutorial content can be included directly in the docs where most appropriate. The ML tutorials, for example, are alongside the PyTorch documentation in the Applications and Frameworks material.</p> <p>rationale</p> <p>Group all tutorial content together in the directory structure so that the url of specific tutorials won\u2019t change when they are moved around.</p>"},{"location":"environments/","title":"Alps environments","text":"<p>Once you have logged into a cluster on Alps, you will want to set up your environment and find the software and tools provided by CSCS. This part of the documentation will give guidance on how to set up your shell, and how to access software in uenv and container environments.</p> <ul> <li> <p> Set up your login environment</p> <p>A good spot to start is to read our guide on configuring your shell.</p> <p> Using the terminal</p> </li> </ul> <ul> <li> <p> Setting up your development and simulation environments</p> <p>CSCS provides two ways to access software, and create the environment used to develop and run.</p> <p>Scientific HPC workloads are a good fit for uenv \u2013 a tool developed by CSCS to deliver scientific software stacks on Alps.</p> <p> uenv</p> <p>The container engine runtime is recommended for machine learning workflows. It can also be a very good choice for Python environments installed using pip, uv and conda.</p> <p> container engine</p> </li> </ul> <p>CSCS provides ready to use environments in uenv and containers:</p> <ul> <li>Programming environments;</li> <li>Applications and software.</li> </ul>"},{"location":"guides/","title":"Index","text":""},{"location":"guides/#guides","title":"Guides","text":"<p>Documentation that provides best practices, practical tips, known problems and useful background information.</p> <p>The guides are grouped around top-level topics.</p>"},{"location":"guides/internet-access/","title":"Internet Access on Alps","text":""},{"location":"guides/internet-access/#internet-access-on-alps","title":"Internet Access on Alps","text":"<p>The Alps network is mostly configured with private IP addresses (<code>172.28.0.0/16</code>). Login nodes have public IP addresses which means that they can directly access the internet, while compute nodes access the internet through NAT.</p> <p>Public IPs are a shared resource</p> <p>Be aware that public IPs, whether on login nodes or through NAT, are essentially a shared resource. Many services will rate limit or block usage based on the IP address if abused. An example is pulling container images from Docker Hub. Authenticating with Docker Hub makes their rate limit apply per user instead. See also the guidelines below</p>"},{"location":"guides/internet-access/#accessing-the-public-ip-of-a-node","title":"Accessing the public IP of a node","text":"<p>When on a login node configured with a public IP address, you can retrieve the public IP address for example as follows:</p> <pre><code>$ curl api.ipify.org\n148.187.6.19\n</code></pre> <p></p>"},{"location":"guides/internet-access/#communicating-with-external-services","title":"Communicating with external services","text":"<p>Note</p> <p>Examples of the type of external communication that can trigger problems include:</p> <ul> <li>web scraping;</li> <li>bulk downloads;</li> <li>pipelines constantly pulling the same image from DockerHub.</li> </ul> <p>Communication with external services from Alps is provided by a high-capacity 400 GBit/s connection to SWITCH. SWITCH provides internet services to the research and education infrastructure in Switzerland.</p> <p>However, communication with external services is not the focus of CSCS, it is rather seen as a way to enable the use of our resources, so for example as explained below from Alps do not put load on services that do not expect it, for example through scraping.</p>"},{"location":"guides/internet-access/#shared-resources","title":"Shared resources","text":"<p>If you need to heavily interact with external systems there are some caveats that you have to keep in mind, in general some resources are shared resources, and a single user should not monopolize their use.</p> <p>To avoid abuse there are measures in place at CSCS, on the transit networks, and on the remote systems, but these measures are often very blunt and would affect the CSCS as whole, so care should be taken to avoid triggering them. We have a good relationship with SWITCH, so if we trigger some of their fail-safes (for example their anti-DDoS tools), they will contact us. External providers might take action, like blacklisting Alps, without warning or notification.</p> <p>For example a website might blacklist IPs, or whole subnets from CSCS, rendering the service unavailable for all CSCS users. Many sites use content delivery networks (CDN), like Cloudflare, Akamai, or similar, and if those blacklist CSCS we would lose access to all content provided by those CDNs. In addition, once blacklisted, it is very difficult to get removed from the blacklist.</p> <p>Info</p> <p>Sites do not publish the number of requests/queries per second that trigger blacklisting, for some obvious reason that bad-intentioned people would stay just below this limit.</p> <p>So you should be mindful of your usage, in particular of the number of requests to the DNS and the network bandwidth. Every access to a different domain will trigger a DNS request, using multiple nodes does not solve the problem, because they will still be hitting the same DNS resolver.</p> <p>CSCS has protection in place for our public DNS server, but other DNS servers might decide to blacklist the originator of all those requests. Alps uses an internal DNS, which is also used to resolve the different nodes in alps, and does not have special protections against abuse. For this reason avoid scraping from Alps, as it could lead to it being blacklisted.</p> <p>Warning</p> <p>The high-capacity of the CSCS-SWITCH connection can saturate the connection of a large provider like Google, which would affect all Swiss Google users.</p>"},{"location":"guides/internet-access/#conclusions","title":"Conclusions","text":"<p>Before any large scale sustained use of external resources think carefully about the load you are putting on the CSCS, network and target, both in number of requests and size of the request.</p> <p>Try to change the perspective: how quickly do you really need the whole data? Can you or should you use resources outside Alps, or even outside CSCS? Maybe geo-distributed?</p> <p>Also reach out to us, so that we are aware of what you are doing, and react quickly if we reach out to you. This last part worked well until now, and it is important that it continues to work well.</p> <p>Even if you did your homework and calculated that your load is acceptable it is important to understand that at the end it\u2019s the aggregated load across all users that counts, and if suddenly many users add an \u201cacceptable\u201d load it might not be so acceptable after all.</p> <p>Finally here we do not touch the legal aspect of the data collection which we expect you to clear separately: copyright/licensing issues, and storage of data that might contain private information, and consequently needs to be handled with due diligence to avoid data breaches.</p> <p>We want to support your ground breaking research, let\u2019s work together to find an acceptable solution for everybody, in the end being ethical is also about this.</p>"},{"location":"guides/storage/","title":"Storage","text":""},{"location":"guides/storage/#storage","title":"Storage","text":""},{"location":"guides/storage/#sharing-files-and-data","title":"Sharing files and data","text":"<p>Newly created user folders are not accessible by other groups or users on CSCS systems. Linux Access Control Lists (ACLs) let you grant access to one or more groups or users.</p> <p>In traditional POSIX, access permissions are granted to <code>user/group/other</code> in mode <code>read</code>/<code>write</code>/<code>execute</code>. The permissions can be checked with the <code>-l</code>  option of the command <code>ls</code>. For instance, if <code>user1</code> owns the folder <code>test</code>, the output would be the following:</p> Checking posix permissions with ls<pre><code>$ ls -lahd test/\ndrwxr-xr-x 2 user1 csstaff 4.0K Feb 23 13:46 test/\n</code></pre> <p>ACLs are an extension of these permissions to give one or more users or groups access to your data. The ACLs of the same <code>test</code> folder of <code>user1</code> can be shown with the command <code>getfacl</code>:</p> Checking permissions with getfacl<pre><code>$ getfacl test\n# file: test\n# owner: user1\n# group: csstaff\nuser::rwx\ngroup::r-x\nother::r-x\n</code></pre> <p>The command <code>setfacl</code> is used to change ACLs for a file or directory.</p> <p>To add users or groups to read/write/execute on a selected file or folder, use the <code>-M,--modify-file</code> or <code>-m,--modify</code> flags to modify the ACL of a file or directory.</p> <p>give user2 read+write access to test</p> <p>Where <code>test</code> is owned by <code>user1</code>. <pre><code>$ setfacl -m user:user2:rw test/\n\n$ getfacl test/\n# file: test\n# owner: user1\n# group: csstaff\nuser::rwx\nuser:user2:rw\ngroup::r-x\nmask::rwx\nother::r-x\n</code></pre></p> <p>The <code>-X,--remove-file</code> and  <code>-x,--remove</code> options will remove ACL entries.</p> <p>remove user2 access to test</p> <p>This reverts the access that was granted in the previous example. <pre><code>$ setfacl -x user:user2 test/\n\n$ getfacl test/\n# file: test\n# owner: user1\n# group: csstaff\nuser::rwx\ngroup::r-x\nmask::rwx\nother::r-x\n</code></pre></p> <p>Access rights can also be granted recursively to a folder and its children (if they exist) using the option <code>-R,--recursive</code>.</p> <p>Note</p> <p>This applies only to existing files - files added after this call won\u2019t inherit the permissions.</p> <p>recursively grant user2 access to test and its contents</p> <pre><code>$ setfacl -Rm user:user2 test\n\n$ getfacl test/subdir\n# file: test/subdir\n# owner: user1\n# group: csstaff\nuser::rwx\nuser:user2:rwx\ngroup::---\ngroup:csstaff:r-x\nmask::rwx\nother::---\n</code></pre> <p>To set up a default so all newly created folders and dirs inside or your desired path will inherit the permissions, use the <code>-d,--default</code> option.</p> <p>recursively grant user2 access to test and its contents</p> <p><code>user2</code> will have access to files created inside <code>test</code> after this call:</p> <pre><code>$ setfacl -dm user:user2:rw test/\n\n$ getfacl test\n# file: test\n# owner: user1\n# group: csstaff\nuser::rwx\ngroup::r-x\nmask::rwx\nother::r-x\ndefault:user::rwx\ndefault:user:user2:rw\ndefault:group::r-x\ndefault:mask::rwx\ndefault:other::r-x\n</code></pre> <p>Info</p> <p>For more information read the <code>setfacl</code> man page: <code>man setfacl</code>.</p> <p></p>"},{"location":"guides/storage/#lustre-tuning","title":"Lustre tuning","text":"<p>Capstor and Iopsstor are both Lustre filesystem.</p> <p></p> <p>As shown in the schema above, Lustre uses metadata servers to store and query metadata, which is basically what is shown by <code>ls</code>: directory structure, file permission, and modification dates. Its performance is roughly the same on Capstor and Iopsstor. This data is globally synchronized, which means Lustre is not well suited to handling many small files, see the discussion on how to handle many small files.</p> <p>The data itself is subdivided in blocks of size <code>&lt;blocksize&gt;</code> and is stored by Object Storage Servers (OSS) in one or more Object Storage Targets (OST). The block size and number of OSTs to use is defined by the striping settings, which are applied to a path, with new files and directories inheriting them from their parent directory. The <code>lfs getstripe &lt;path&gt;</code> command can be used to get information on the stripe settings of a path. For directories and empty files <code>lfs setstripe --stripe-count &lt;count&gt; --stripe-size &lt;size&gt; &lt;directory/file&gt;</code> can be used to set the layout.</p> <p>Striping settings on a directory are only applied to files added after the command is run.  Existing files retain their original layout unless explicitly changed using <code>lfs migrate &lt;striping settings&gt;</code>, which takes the same arguments as <code>lfs setstripe</code>. The simplest way to have the correct layout is to copy to a directory with the correct layout.</p> <p>A block size of 4MB gives good throughput, without being overly big\u2026</p> <p>\u2026 so it is a good choice when reading a file sequentially or in large chunks, but if one reads shorter chunks in random order it might be better to reduce the size, the performance will be smaller, but the performance of your application might actually increase. See the Lustre documentation for more information.</p> <p>Settings for large files</p> <p>Remember: Settings only apply to files added to the directory after this command. <pre><code>lfs setstripe --stripe-count -1 --stripe-size 4M &lt;big_files_dir&gt;`\n</code></pre></p> <p>Lustre also supports composite layouts, switching from one layout to another at a given size <code>--component-end</code> (<code>-E</code>). With it it is possible to create a Progressive file layout switching <code>--stripe-count</code> (<code>-c</code>), <code>--stripe-size</code> (<code>-S</code>), so that fewer locks are required for smaller files, but load is distributed for larger files.</p> <p>Good default settings</p> <pre><code>lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -S 4M &lt;base_dir&gt;\n</code></pre>"},{"location":"guides/storage/#iopsstor-vs-capstor","title":"Iopsstor vs Capstor","text":"<p>Iopsstor uses SSD as OST, thus random access is quick, and the performance of the single OST is high. Capstor on another hand uses hard disks, it has a larger capacity, and  it also have many more OSS, thus the total bandwidth is larger. See for example the ML filesystem guide.</p> <p></p>"},{"location":"guides/storage/#many-small-files-vs-hpc-file-systems","title":"Many small files vs. HPC File Systems","text":"<p>Workloads that read or create many small files are not well-suited to parallel file systems, which are designed for parallel and distributed I/O.</p> <p>In some cases, and if enough memory is available it might be worth to unpack/repack the small files to in-memory filesystems like <code>/dev/shm/$USER</code> or <code>/tmp</code>, which are much faster, or to use a squashfs filesystem that is stored as a single large file on Lustre.</p> <p>Workloads that do not play nicely with Lustre include:</p> <ul> <li>Configuration and compiling applications.</li> <li>Using Python virtual environments</li> </ul> <p>At first it can seem strange that a \u201chigh-performance\u201d file system is significantly slower than a laptop drive for a \u201csimple\u201d task like compilation or loading Python modules, however Lustre is designed for high-bandwidth parallel file access from many nodes at the same time, with the attendant trade offs this implies.</p> <p>Meta data lookups on Lustre are expensive compared to your laptop, where the local file system is able to aggressively cache meta data.</p> <p></p>"},{"location":"guides/storage/#python-virtual-environments-with-uenv","title":"Python virtual environments with uenv","text":"<p>Python virtual environments can be very slow on Lustre, for example a simple <code>import numpy</code> command run on Lustre might take seconds, compared to milliseconds on your laptop.</p> <p>The main reasons for this include:</p> <ul> <li>Python virtual environments contain many small files, on which Python performs <code>stat()</code>, <code>open()</code> and <code>read()</code> commands when loading a module.</li> <li>Python pre-compiles <code>.pyc</code> files for each <code>.py</code> file in a project.</li> <li>All of these operations create a lot of meta-data lookups.</li> </ul> <p>As a result, using virtual environments can be slow, and these problems are only exacerbated when the virtual environment is loaded simultaneously by many ranks in an MPI job.</p> <p>One solution is to use the tool <code>mksquashfs</code> to compresses the contents of a directory - files, inodes and sub-directories - into a single file. This file can be mounted as a read-only Squashfs file system, which is much faster because a single file is accessed instead of the many small files that were in the original environment.</p>"},{"location":"guides/storage/#step-1-create-the-virtual-environment","title":"Step 1: create the virtual environment","text":"<p>The first step is to create the virtual environment using the usual workflow.</p> uvvenv <p>The recommended way to create a new virtual environment is to use the uv tool, which supports relocatable virtual environments and asynchronous package downloads. The main benefit of a relocatable virtual environment is that it does not need to be created in the final path from where it will be used. This allows the use of shared memory to speed up the creation and initialization of the virtual environment and, since the virtual environment can be used from any location, the resulting squashfs image can be safely shared across projects.</p> <pre><code># start the uenv\n# in this case the \"default\" view of prgenv-gnu provides python, cray-mpich,\n# and other useful tools\nuenv start prgenv-gnu/24.11:v1 --view=default\n\n# create and activate a new relocatable venv using uv\n# in this case we explicitly select python 3.12\nuv venv -p 3.12 --relocatable --link-mode=copy /dev/shm/sqfs-demo/.venv\n# You can also point to the uenv python with `uv venv -p $(which python) ...`\n# which, among other things, enables user portability of the venv\ncd /dev/shm/sqfs-demo\nsource .venv/bin/activate\n\n# install software in the virtual environment using uv\n# in this case we install install pytorch\nuv pip install --link-mode=copy torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\n\n# optionally, to reduce the import times, precompile all\n# python modules to bytecode before creating the squashfs image\npython -m compileall -j 8 -o 0 -o 1 -o 2 .venv/lib/python3.12/site-packages\n</code></pre> <p>A new virtual environment can also be created using the standard <code>venv</code> module. However, virtual environments created by <code>venv</code> are not relocatable, and thus they need to be created and initialized in the path from where they will be used. This implies that the installation process can not be optimized for file system performance and will still be slow on Lustre filesystems.</p> <pre><code># start the uenv\n# in this case the \"default\" view of prgenv-gnu provides python, cray-mpich,\n# and other useful tools\nuenv start prgenv-gnu/24.11:v1 --view=default\n\n# for the example create a working path on SCRATCH\nmkdir $SCRATCH/sqfs-demo\ncd $SCRATCH/sqfs-demo\n\n# create and activate the empty venv\npython -m venv ./.venv\nsource ./.venv/bin/activate\n\n# install software in the virtual environment\n# in this case we install install pytorch\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\n</code></pre> how many files did that create? <p>An inode is created for every file, directory and symlink on a file system. In order to optimise performance, we want to reduce the number of inodes (i.e. the number of files and directories).</p> <p>The following command can be used to count the number of inodes: <pre><code>find $SCRATCH/sqfs-demo/.venv -exec stat --format=\"%i\" {} + | sort -u | wc -l\n</code></pre> <code>find</code> is used to list every path and file, and <code>stat</code> is called on each of these to get the inode, and then <code>sort</code> and <code>wc</code> are used to count the number of unique inodes.</p> <p>In our \u201csimple\u201d pytorch example, I counted 22806 inodes!</p>"},{"location":"guides/storage/#step-2-make-a-squashfs-image-of-the-virtual-environment","title":"Step 2: make a squashfs image of the virtual environment","text":"<p>The next step is to create a single squashfs file that contains the whole virtual environment folder (i.e. <code>/dev/shm/sqfs-demo/.venv</code> or <code>$SCRATCH/sqfs-demo/.venv</code>).</p> <p>This is performed using the <code>mksquashfs</code> command, that is installed on all Alps clusters.</p> uvvenv <pre><code>mksquashfs /dev/shm/sqfs-demo/.venv py_venv.squashfs \\\n    -no-recovery -noappend -Xcompression-level 3\n</code></pre> <pre><code>mksquashfs $SCRATCH/sqfs-demo/.venv py_venv.squashfs \\\n    -no-recovery -noappend -Xcompression-level 3\n</code></pre> <p>Hint</p> <p>The <code>-Xcompression-level</code> flag sets the compression level to a value between 1 and 9, with 9 being the most compressed. We find that level 3 provides a good trade off between the size of the compressed image and performance: both uenv and the container engine use level 3.</p> I am seeing errors of the form <code>Unrecognised xattr prefix...</code> <p>You can safely ignore the (possibly many) warning messages of the form: <pre><code>Unrecognised xattr prefix lustre.lov\nUnrecognised xattr prefix system.posix_acl_access\nUnrecognised xattr prefix lustre.lov\nUnrecognised xattr prefix system.posix_acl_default\n</code></pre></p> <p>Tip</p> <p>The default installed version of <code>mksquashfs</code> on Alps does not support the best <code>zstd</code> compression method. Every uenv contains a better version of <code>mksquashfs</code>, which is used by the uenv to compress itself when it is built.</p> <p>The exact location inside the uenv depends on the target architecture, and version, and will be of the form: <pre><code>/user-environment/linux-sles15-${arch}/gcc-7.5.0/squashfs-${version}-${hash}/bin/mksquashfs\n</code></pre> Use this version for the best results, though it is also perfectly fine to use the system version.</p>"},{"location":"guides/storage/#step-3-use-the-squashfs","title":"Step 3: use the squashfs","text":"<p>To use the optimised virtual environment, mount the squashfs image at the location of the original virtual environment when starting the uenv.</p> uvvenv <pre><code>cd $SCRATCH/sqfs-demo\nuenv start --view=default \\\n    prgenv-gnu/24.11:v1,$PWD/py_venv.squashfs:$SCRATCH/sqfs-demo/.venv\nsource .venv/bin/activate\n</code></pre> <p>Remember that virtual environments created by <code>uv</code> are relocatable only if the <code>--relocatable</code> option flag is passed to the <code>uv venv</code> command as mentioned in step 1. In that case, the generated environment is relocatable and thus it is possible to mount it in multiple locations without problems.</p> <pre><code>cd $SCRATCH/sqfs-demo\nuenv start --view=default \\\n    prgenv-gnu/24.11:v1,$PWD/py_venv.squashfs:$SCRATCH/sqfs-demo/.venv\nsource .venv/bin/activate\n</code></pre> <p>Note that the original virtual environment is still installed in <code>$SCRATCH/sqfs-demo/.venv</code>, however the squashfs image has been mounted on top of it, so the single squashfs file is being accessed instead of the many files in the original version.</p> <p>A benefit of this approach is that the squashfs file can be copied to a location that is not subject to the Scratch cleaning policy.</p> <p>Warning</p> <p>Virtual environments created by <code>venv</code> are not relocatable as they contain symlinks to absolute locations inside the virtual environment. This means that the squashfs file must be mounted in the exact same location where the virtual environment was created.</p>"},{"location":"guides/storage/#step-4-optional-regenerate-the-virtual-environment","title":"Step 4: (optional) regenerate the virtual environment","text":"<p>The squashfs file is immutable - it is not possible to modify the contents of <code>.venv</code> while it is mounted. This means that it is not possible to <code>pip install</code> more packages in the virtual environment.</p> <p>If you need to modify the virtual environment, run the original uenv without the squashfs file mounted, make changes to the virtual environment, and run step 2 again to generate a new image.</p> <p>Hint</p> <p>If you save the updated copy in a different file, you can now \u201croll back\u201d to the old version of the environment by mounting the old image.</p>"},{"location":"guides/terminal/","title":"Getting started in the terminal","text":""},{"location":"guides/terminal/#terminal-usage-on-alps","title":"Terminal usage on Alps","text":"<p>This documentation is a collection of guides, hints, and tips for setting up your terminal environment on Alps.</p> <p></p>"},{"location":"guides/terminal/#shells","title":"Shells","text":"<p>Every user has a shell that will be used when they log in, with bash as the default shell for new users at CSCS.</p> <p>Which shell am I using?</p> <p>Run the following command after logging in:</p> <pre><code>$ echo $SHELL\n/usr/local/bin/bash\n</code></pre> <p>Tip</p> <p>If you would like to change your shell, for example to zsh, you have to open a service desk ticket to request the change. You can\u2019t make the change yourself.</p> <p>Warning</p> <p>If you are comfortable with another shell (like Zsh or Fish), you are welcome to switch. Just keep in mind that some tools and instructions might not work the same way outside of <code>bash</code>. Since our support and documentation are based on the default setup, using a different shell might make it harder to follow along or get help.</p> <p>We strongly recommend against using cshell - tools like uenv are not tested against it.</p> <p></p>"},{"location":"guides/terminal/#managing-x86-and-arm","title":"Managing x86 and ARM","text":"<p>Alps has nodes with different CPU architectures, for example Santis has ARM (Grace <code>aarch64</code>) processors, and Eiger uses x86 (AMD Rome <code>x86_64</code>) processors. Binary applications are generally not portable, for example if you compile or install a tool compiled for <code>x86_64</code> on Eiger, you will get an error when you run it on an <code>aarch64</code> node.</p> cannot execute binary file: Exec format error <p>You will see this error message if you try to execute an executable built for a different architecture.</p> <p>In this case, the <code>rg</code> executable built for <code>aarch64</code> (Grace-Hopper nodes) is run on an <code>x86_64</code> node on Eiger: <pre><code>$ ~/.local/aarch64/bin/rg\n-bash: ./rg: cannot execute binary file: Exec format error\n</code></pre></p> <p>A common pattern for installing local software, for example some useful command line utilities like ripgrep, is to install them in <code>$HOME/.local/bin</code>. This approach won\u2019t work if the same home directory is mounted on two different clusters with different architectures: the version of ripgrep in our example would crash with <code>Exec format error</code> on one of the clusters.</p> <p>Care needs to be taken to store executables, configuration and data for different architectures in separate locations, and automatically configure the login environment to use the correct location when you log into different systems.</p> <p>The following example:</p> <ul> <li>sets architecture-specific <code>bin</code> path for installing programs</li> <li>sets architecture-specific paths for installing application data and configuration</li> <li>selects the correct path by running <code>uname -m</code> when you log in to a cluster</li> </ul> .bashrc<pre><code># Set the \"base\" directory in which all architecture specific will be installed.\n# The $(uname -m) command will generate either x86_64 or aarch64 to match the\n# node type, when run during login.\nxdgbase=$HOME/.local/$(uname -m)\n\n# The XDG variables define where applications look for configurations\nexport XDG_DATA_HOME=$xdgbase/share\nexport XDG_CONFIG_HOME=$xdgbase/config\nexport XDG_STATE_HOME=$xdgbase/state\n\n# set PATH to look for in architecture specific path:\n# - on x86: $HOME/.local/x86_64/bin\n# - on ARM: $HOME/.local/aarch64/bin\nexport PATH=$xdgbase/bin:$PATH\n</code></pre> <p>XDG what?</p> <p>The XDG base directory specification is used by most applications to determine where to look for configurations, and where to store data and temporary files.</p> <p></p>"},{"location":"guides/terminal/#modifying-bashrc","title":"Modifying bashrc","text":"<p>The <code>~/.bashrc</code> in your home directory is executed every time you log in, and there is no way to log in without executing it.</p> <p>It is strongly recommended that customization in <code>~/.bashrc</code> should be kept to the bare minimum:</p> <ol> <li>It sets a fixed set of environment options every time you log in, and all downstream scripts and Slurm batch jobs might assume that these commands have run, so that later modifications to <code>~/.bashrc</code> can break workflows in ways that are difficult to debug.<ul> <li>If a script or batch job requires environment modifications, implement them there.</li> <li>In other words, move the definition of environment used by a workflow to the workflow definition.</li> </ul> </li> <li>It makes it difficult for CSCS to provide support, because it is difficult for support staff to reproduce your environment, and it can take a lot of back and forth before we determine that the root cause of an issue is a command in <code>~/.bashrc</code>.</li> </ol> <p>Do not call <code>module</code> in bashrc</p> <p>Calls to <code>module use</code> and <code>module load</code> in <code>~/.bashrc</code> is possible, however avoid it for the reasons above. If there are module commands in your <code>~/.bashrc</code>, remember to provide a full copy of <code>~/.bashrc</code> with support tickets.</p> <p>Do not call <code>uenv</code> in bashrc</p> <p>The <code>uenv</code> command is designed for creating isolated environments, and calling it in <code>~/.bashrc</code> will not work as expected. See the uenv docs for more information about how to create bespoke uenv environments that can be started with a single command.</p> Help, I broke bashrc! <p>It is possible to add commands to bashrc that will stop you from being able to log in. The author of these docs has done it more than once, after ignoring their own advice.</p> <p>For example, if the command <code>exit</code> is added to <code>~/.bashrc</code> you will be logged out every time you log in.</p> <p>The first thing to try is to execute a command that will back up <code>~/.bashrc</code>, and remove <code>~/.bashrc</code>: <pre><code>ssh eiger.cscs.ch 'bash --norc --noprofile -c \"mv ~/.bashrc ~/.bashrc.back\"'\n</code></pre> If this works, you can then log in normally, and edit the backup and copy it back to <code>~/.bashrc</code>.</p> <p>If there is a critical error, like calling <code>exit</code>, the approach above won\u2019t work. In such cases, the only solution that doesn\u2019t require root permissions is to log in and hit <code>&lt;ctrl-c&gt;</code> during the log in. With luck, this will cancel the login process before <code>~/.bashrc</code> is executed, and you will be able to edit and fix <code>~/.bashrc</code>. Note that you might have to try a few times to get the timing right.</p> <p>If this does not work, create a service desk ticket with the following message:</p> <p>Help request</p> <p>My bashrc has been modified, and I can\u2019t log in any longer to <code>insert-system-name</code>. My username is <code>insert-cscs-username</code>. Can you please make a backup copy of my bashrc, i.e. <code>mv ~/.bashrc ~/.bashrc.back</code>, so that I can log in and fix the issue.</p>"},{"location":"platforms/cwp/","title":"Index","text":""},{"location":"platforms/cwp/#climate-and-weather-platform","title":"Climate and Weather Platform","text":"<p>The Climate and Weather Platform (CWP) provides compute, storage and support to the climate and weather modeling community in Switzerland.</p>"},{"location":"platforms/cwp/#getting-started","title":"Getting Started","text":""},{"location":"platforms/cwp/#getting-access","title":"Getting access","text":"<p>Project administrators (PIs and deputy PIs) of projects on the CWP can to invite users to join their project, before they can use the project\u2019s resources on Alps.</p> <p>This is currently performed using the account and resource management tool.</p> <p>Once invited to a project, you will receive an email, which you can need to create an account and configure multi-factor authentication (MFA).</p>"},{"location":"platforms/cwp/#systems","title":"Systems","text":"<p>Santis is the system deployed on the Alps infrastructure for the climate and weather platform. Its name derives from the highest mountain S\u00e4ntis in the Alpstein massif of North-Eastern Switzerland.</p> <ul> <li> <p> Santis</p> <p>Santis is a large Grace-Hopper cluster.</p> </li> </ul> <p></p>"},{"location":"platforms/cwp/#file-systems-and-storage","title":"File systems and storage","text":"<p>There are three main file systems mounted on the CWP system Santis.</p> type mount filesystem Home /users/$USER VAST Scratch <code>/capstor/scratch/cscs/$USER</code> Capstor Project <code>/capstor/store/cscs/userlab/&lt;project&gt;</code> Capstor"},{"location":"platforms/cwp/#home","title":"Home","text":"<p>Every user has a home path (<code>$HOME</code>) mounted at <code>/users/$USER</code> on the VAST filesystem. The home directory has 50 GB of capacity, and is intended for configuration, small software packages and scripts.</p>"},{"location":"platforms/cwp/#scratch","title":"Scratch","text":"<p>The Scratch filesystem provides temporary storage for high-performance I/O for executing jobs.</p> <p>See the Scratch documentation for more information.</p> <p>The environment variable <code>SCRATCH=/capstor/scratch/cscs/$USER</code> is set automatically when you log into the system, and can be used as a shortcut to access scratch.</p> <p>scratch cleanup policy</p> <p>Files that have not been accessed in 30 days are automatically deleted.</p> <p>Scratch is not intended for permanent storage: transfer files back to the Store after job runs.</p>"},{"location":"platforms/cwp/#project-store","title":"Project Store","text":"<p>Project storage is backed up, with no cleaning policy, as intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters.</p> <p>The environment variable <code>PROJECT</code> is set automatically when you log into the system, and can be used as a shortcut to access the Store path for your primary project.</p> <p>Hard limits on capacity and inodes prevent users from writing to Store if the quota is reached. You can check quota and available space by running the <code>quota</code> command on a login node or ela.</p> <p>Warning</p> <p>It is not recommended to write directly to the <code>$PROJECT</code> path from jobs.</p>"},{"location":"platforms/hpcp/","title":"Index","text":""},{"location":"platforms/hpcp/#hpc-platform","title":"HPC Platform","text":"<p>The HPC Platform (HPCP) provides compute, storage, and related services for the HPC community in Switzerland and abroad. The majority of compute cycles are provided to the User Lab via peer-reviewed allocation schemes.  </p>"},{"location":"platforms/hpcp/#getting-started","title":"Getting Started","text":"<ul> <li> <p> Policies</p> <p>New users are invited to read carefully the CSCS User Policies.</p> </li> </ul>"},{"location":"platforms/hpcp/#getting-access","title":"Getting access","text":"<p>Principal Investigators (PIs) and Deputy PIs can invite users to join their projects using the account and resource management tool.</p> <p>Once invited to a project you will receive an email with information on how to create an account and configure multi-factor authentication (MFA).</p>"},{"location":"platforms/hpcp/#systems","title":"Systems","text":"<ul> <li> <p> Daint</p> <p>Daint is a large Grace-Hopper cluster for GPU-enabled workloads.</p> </li> </ul> <ul> <li> <p> Eiger</p> <p>Eiger is an AMD Epyc cluster for CPU-only workloads.</p> </li> </ul> <p></p>"},{"location":"platforms/hpcp/#file-systems-and-storage","title":"File systems and storage","text":"<p>There are three main file systems mounted on the HPCP clusters.</p> type mount file system Home /users/$USER VAST Scratch <code>/capstor/scratch/cscs/$USER</code> Capstor Store <code>/capstor/store/cscs/&lt;customer&gt;/&lt;project&gt;</code> Capstor"},{"location":"platforms/hpcp/#home","title":"Home","text":"<p>Every user has a home path (<code>$HOME</code>) mounted at <code>/users/$USER</code> on the VAST file system. Home directories have 50 GB of capacity and are intended for keeping configuration files, small software packages, and scripts.</p>"},{"location":"platforms/hpcp/#scratch","title":"Scratch","text":"<p>The Scratch file system is a large, temporary storage system designed for high-performance I/O. It is not backed up. </p> <p>See the Scratch documentation for more information.</p> <p>The environment variable <code>$SCRATCH</code> points to <code>/capstor/scratch/cscs/$USER</code>, and can be used as a shortcut to access your scratch folder.</p> <p>scratch cleanup policy</p> <p>Files that have not been accessed in 30 days are automatically deleted.</p> <p>Scratch is not intended for permanent storage: transfer files back to the Store after batch job completion.</p>"},{"location":"platforms/hpcp/#store","title":"Store","text":"<p>The Store (or Project) file system is provided as a space to store datasets, code, or configuration scripts that can be accessed from different clusters. The file system is backed up and there is no automated deletion policy.</p> <p>The environment variable <code>$STORE</code> can be used as a shortcut to access the Store folder of your primary project.</p> <p>Hard limits on the amount of data and number of files (inodes) will prevent you from writing to Store if your quotas are exceeded. You can check how much data and inodes you are consuming\u2014and their respective quotas\u2014by running the <code>quota</code> command on a login node.</p> <p>Warning</p> <p>It is not recommended to write directly to the <code>$STORE</code> path from batch jobs. </p>"},{"location":"platforms/mlp/","title":"Index","text":""},{"location":"platforms/mlp/#machine-learning-platform","title":"Machine learning platform","text":"<p>The Machine Learning Platform (MLP) provides compute, storage and expertise to the machine learning communities accessing the Alps Research Infrastructure.</p>"},{"location":"platforms/mlp/#getting-started","title":"Getting started","text":"<ul> <li> <p> ML Guides</p> <p>For an overview of how to use common machine learning software, tools and workflows, read our machine learning documentation.</p> <p>Tutorials on how to set up and configure a machine learning environment in order to run LLM workloads such as inference, fine-tuning and multi-node training can be found in the tutorials section.</p> <p>Check out the PyTorch documentation for information about how to run PyTorch.</p> </li> </ul>"},{"location":"platforms/mlp/#getting-access","title":"Getting access","text":"<p>Project administrators (PIs and deputy PIs) of projects on the MLP can to invite users to join their project, before they can use the project\u2019s resources on Alps. This is performed using the project management tool.</p> <p>Once invited to a project, you will receive an email, which you need to create an account and configure multi-factor authentication (MFA).</p>"},{"location":"platforms/mlp/#systems","title":"Systems","text":"<p>The main cluster provided by the MLP is Clariden, a large Grace-Hopper GPU system on Alps.</p> <ul> <li> <p> Clariden</p> <p>Clariden is the main Grace-Hopper cluster.</p> </li> </ul> <ul> <li> <p> Bristen</p> <p>Bristen is a smaller system with A100 GPU nodes for data processing, development, x86 workloads and inference services.</p> </li> </ul> <p></p>"},{"location":"platforms/mlp/#file-systems-and-storage","title":"File Systems and Storage","text":"<p>There are three main file systems mounted on the MLP clusters Clariden and Bristen.</p> type mount filesystem Home <code>/users/$USER</code> VAST Scratch <code>/iopsstor/scratch/cscs/$USER</code> Iopsstor <code>/capstor/scratch/cscs/$USER</code> Capstor Project <code>/capstor/store/cscs/swissai/&lt;project&gt;</code> Capstor"},{"location":"platforms/mlp/#home","title":"Home","text":"<p>Every user has a home path (<code>$HOME</code>) mounted at <code>/users/$USER</code> on the VAST filesystem. The home directory has 50 GB of capacity, and is intended for configuration, small software packages and scripts.</p>"},{"location":"platforms/mlp/#scratch","title":"Scratch","text":"<p>Scratch filesystems provide temporary storage for high-performance I/O for executing jobs. Use scratch to store datasets that will be accessed by jobs, and for job output. Scratch is per user - each user gets separate scratch path and quota.</p> <ul> <li>The environment variable <code>SCRATCH=/iopsstor/scratch/cscs/$USER</code> is set automatically when you log into a system of the ML platform, and can be used as a shortcut to access scratch.</li> <li>There is an additional scratch path mounted on Capstor at <code>/capstor/scratch/cscs/$USER</code>.</li> </ul> <p>scratch cleanup policy</p> <p>Files that have not been accessed in 30 days are automatically deleted.</p> <p>Scratch is not intended for permanent storage: transfer files back to the capstor project storage after job runs.</p> <p></p> <p>file system suitability</p> <p>The Capstor scratch filesystem is based on HDDs and is optimized for large, sequential read and write operations. We recommend using Capstor for storing checkpoint files and other large, contiguous outputs generated by your training runs. In contrast, Iopsstor uses high-performance NVMe drives, which excel at handling IOPS-intensive workloads involving frequent, random access. This makes it a better choice for storing training datasets, especially when accessed randomly during machine learning training. See the Lustre guide for some hints on how to get the best performance out of the filesystem.</p>"},{"location":"platforms/mlp/#scratch-usage-recommendations","title":"Scratch Usage Recommendations","text":"<p>Use Iopsstor scratch (<code>$SCRATCH</code>) for:</p> <ul> <li>Training and validation datasets that are read frequently and non-sequentially.</li> <li>Workloads that perform many small, random I/O operations.</li> </ul> <p>Use Capstor scratch (<code>/capstor/scratch/cscs/$USER</code>) for:</p> <ul> <li>Storing model checkpoints.</li> <li>Outputs from simulations or training jobs that involve large, contiguous I/O.</li> </ul> <p>After your job completes, remember to transfer any important results to your permanent project storage.</p>"},{"location":"platforms/mlp/#project","title":"Project","text":"<p>Project storage is backed up, with no cleaning policy: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters. Project is per project - each project gets a project folder with project-specific quota.</p> <ul> <li>if you need additional storage, ask your PI to contact the CSCS service managers Fawzi or Nicholas.</li> <li>hard limits on capacity and inodes prevent users from writing to project if the quota is reached - you can check quota and available space by running the <code>quota</code> command on a login node or ela </li> <li>it is not recommended to write directly to the project path from jobs.</li> </ul>"},{"location":"policies/","title":"Index","text":""},{"location":"policies/#cscs-user-policies","title":"CSCS User Policies","text":"<p>The CSCS code of conduct outlines the responsibilities and proper practices for the CSCS user community.</p> <p>The User Regulations define the basic guidelines for the usage of CSCS computing resources. The right to access CSCS resources may be revoked to whoever breaches any of the user regulations.</p> <p>The User Support Policies, the Slack Code of Conduct and the Scheduled Maintenance and System Unavailability Policies provide additional information on support services, the regulations of the Users Slack space and the scheduled maintenance events.</p>"},{"location":"policies/#resource-allocation-policies","title":"Resource Allocation Policies","text":"<p>Compute time on Alps systems is measured in node hours. Currently, we only support exclusive node allocations. This means that even if you utilize only a portion of a node\u2019s resources (e.g., a single GPU), your account will still be charged for the entire node.</p> <p>Please note that resources at CSCS are assigned over three-months windows</p> <ul> <li>Quotas are reset on April 1st, July 1st, October 1st and January 1st</li> <li>Please make sure to use thoroughly your quarterly compute budget within the corresponding time frame</li> <li>Resources unused in the three-month periods are not transferred to the next allocation period but are forever lost</li> </ul>"},{"location":"policies/#data-retention-policies","title":"Data Retention Policies","text":"<p>Data belonging to active projects in the filesystems <code>/users</code> and <code>/capstor/store</code> are under backup. There is no backup for data under the scratch filesystem, therefore no data recovery is possible in case of accidental loss or for data deleted due to the cleaning policy implemented on this filesystem.</p> <p>Please note that the long term storage service is granted as long as your project is active, and the data will be removed without further notice 3 months after the expiration of the project: please check the applicable filesystem policies for the grace period granted after the expiration of the project.</p> <p>Furthermore, as soon as your project expires, the backup of the data belonging to the project will be disabled immediately: therefore no data backup will be available after the final data removal.</p> <p></p>"},{"location":"policies/#fair-usage-of-shared-resources","title":"Fair Usage of Shared Resources","text":"<p>The Slurm scheduling system is a shared resource that can handle a limited number of batch jobs and interactive commands simultaneously. Therefore users should not submit hundreds of Slurm jobs and commands at the same time, as doing so would infringe our fair usage policy.</p> <p>Let us also remind you that running compute or memory intensive applications on the login nodes is forbidden. Please submit batch jobs with the Slurm scheduler, in order to allocate and run your processes on compute nodes: compute or memory intensive processes affecting the performance of login nodes will be terminated without warning.</p>"},{"location":"policies/code-of-conduct/","title":"CSCS Code of Conduct","text":"<p>The CSCS code of conduct aims to outline the responsibilities and the proper practices for CSCS user community.</p>"},{"location":"policies/code-of-conduct/#access-to-source-code","title":"Access to Source Code","text":"<p>If you are using your own code for production projects at CSCS, you agree to make this code available to CSCS application analysts for performance analysis and optimization purposes (if necessary). If you are using third-party commercial or community open-source codes, CSCS will contact the developer as needed.</p>"},{"location":"policies/code-of-conduct/#scientific-advisory-committee-sac","title":"Scientific Advisory Committee (SAC)","text":"<p>It is expressly stated that panel committee members must not be contacted under any circumstances on issues regarding proposals. Violation of this rule disqualifies the proposal from scientific review and leads to immediate rejection of proposals.</p>"},{"location":"policies/code-of-conduct/#acknowledgements","title":"Acknowledgements","text":"<p>The User Lab Users must quote and acknowledge the use of CSCS resources in all publications related to their production and development projects as follows:</p> <p>This work was supported by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID ### on Alps</p> <p>The User Lab Users must quote and acknowledge the use of Swiss Share of the LUMI resources in all publications related to their production and development projects as follows:</p> <p>This work was supported by a grant from the Swiss National Supercomputing Centre (CSCS) on the Swiss share of the LUMI system under project ID ###</p> <p>Users with allocations under the  Swiss AI Initiative must quote and acknowledge the use of CSCS resources in all publications related to their projects on Alps as follows:</p> <p>This work was supported as part of the \u201cSwiss AI initiative\u201d by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID ### on Alps</p> <p>User Lab Users should acknowledge the PASC program in their publications as follows:</p> <p>This work was supported by the  Platform for Advanced Scientific Computing (PASC) project \u201cname of the project\u201d.\u201d</p> <p>Users with a share of CSCS infrastructure (Contractual Partners) should acknowledge use of CSCS resources in their publications as follows:</p> <p>We acknowledge access to Piz Daint or Alps at the Swiss National Supercomputing Centre, Switzerland under the [institution]\u2019s share with the project ID ####</p>"},{"location":"policies/maintenance/","title":"Scheduled Maintenance and System Unavailability Policies","text":""},{"location":"policies/maintenance/#scheduled-maintenance-and-system-unavailability-policy","title":"Scheduled Maintenance and System Unavailability Policy","text":"<p>To ensure the reliability and performance of the Alps production vClusters, CSCS continues to implement rolling updates aimed at reducing downtime during routine maintenance. However, regular interventions are still necessary at this stage.</p>"},{"location":"policies/maintenance/#advance-notice","title":"Advance notice","text":"<p>We strive to announce scheduled system unavailability at least one week in advance. In some cases, earlier notice may be possible, although this depends on external factors and internal approval processes.</p>"},{"location":"policies/maintenance/#shared-infrastructure","title":"Shared infrastructure","text":"<p>Alps is a shared research infrastructure supporting a diverse range of research communities, partners, and projects. Occasionally, the system may be temporarily dedicated to specific scientific projects to enable large-scale capability runs.</p> <p></p>"},{"location":"policies/maintenance/#maintenance-and-availability-cadence","title":"Maintenance and availability cadence","text":"<p>To help users plan their activities within each allocation quarter, we provide a tentative schedule of system unavailability. Please note that this schedule is subject to change based on operational requirements:</p>"},{"location":"policies/maintenance/#routine-maintenance","title":"Routine maintenance","text":"<ul> <li>Cadence: Occurs weekly, depending on need</li> <li>Typical duration: Half a day; occasionally up to one full day</li> </ul>"},{"location":"policies/maintenance/#extraordinary-maintenance","title":"Extraordinary maintenance","text":"<ul> <li>Cadence: At least once per quarter</li> <li>Typical duration: Two days; may be extended if necessary</li> </ul>"},{"location":"policies/maintenance/#dedicated-large-scale-capability-runs-of-scientific-projects","title":"Dedicated large-scale capability runs of scientific projects","text":"<ul> <li>Cadence: At most once per quarter</li> <li>Typical duration: One week</li> </ul>"},{"location":"policies/maintenance/#communication-and-feedback","title":"Communication and feedback","text":"<p>CSCS values the constructive feedback provided by users. We will use this input to enhance our communication practices and to develop mitigation strategies for scheduled events that may significantly impact system usability.</p>"},{"location":"policies/regulations/","title":"User Regulations","text":"<p>The User Regulations define the basic guidelines for the usage of CSCS computing resources. The right to access CSCS resources may be revoked to whoever breaches any of the user regulations.</p> <p>These are the Terms &amp; Conditions, which users need to follow in order to access CSCS computing resources.</p> <ul> <li>Access to CSCS facilities is granted on an individual basis. An account is usable by the applicant only and only for the explicit purposes stated in the project application. CSCS does not allow sharing of accounts.</li> <li>The applicant is not permitted to give any other person (project member or otherwise), organization or representative of any organization access to CSCS facilities explicitly or implicitly, through negligence or carelessness. Revealing of passwords or identification protocols through verbal, written or electronic means is strictly prohibited. Any such activity is considered a breach of CSCS security, the contract between the applicant and CSCS at the moment the Account Application Form is submitted and approved, and the established contracts between CSCS and its computer vendors. Should such activity occur, the applicant will be immediately barred from all present and future use of CSCS facilities and is fully liable for all consequences arising from the infraction.</li> <li>Any indication of usage or requests for runs which give rise to serious suspicion will be further investigated and escalated to the appropriate authorities if necessary.</li> <li>Access to and use of data of other accounts on CSCS systems without prior consent from the principal investigator to which project the user account pertains is strictly prohibited. The terms and conditions for use of data from other accounts must be directly agreed to by the data owner.</li> <li>The applicant confirms that all information provided on his/her Account Application Form is true and accurate, and that she/he has not knowingly misrepresented him/herself.</li> <li>The principal investigator should promptly and proactively notify CSCS as soon as the applicants (i.e. the future account owner) should be suspended.</li> </ul> <p>All CSCS account holders are fully bound to obey by the ETH Zurich Acceptable Use Policy for Telematics Resources (\u201cBOT\u201d).</p>"},{"location":"policies/slack/","title":"Slack Code of Conduct","text":"<p>The CSCS Users Slack space is designed to foster a positive and inclusive environment for all members. To ensure a respectful and engaging experience, we kindly ask you to adhere to the following code of conduct:</p> <ol> <li>Respectful Communication<ul> <li>Treat all members with respect and kindness. Avoid offensive, derogatory, or discriminatory language and behaviour.</li> <li>Engage in constructive discussions and debates. Maintain a respectful tone.</li> </ul> </li> <li>Inclusive Atmosphere<ul> <li>Embrace diversity in all forms, including but not limited to, race, gender, sexual orientation, disability, and cultural background.</li> <li>Avoid making assumptions about others based on their background or identity.</li> </ul> </li> <li>Professionalism and Relevance<ul> <li>Keep discussions relevant to the scope of the channel and the scope of CSCS.</li> <li>Refrain from promoting unrelated content, products, or services.</li> </ul> </li> <li>Intellectual Property and Copyright<ul> <li>Respect intellectual property rights. Only share content that you have the right to share.</li> <li>Provide appropriate attribution when sharing information or resources.</li> </ul> </li> <li>Avoid Spam and Self-Promotion<ul> <li>Avoid excessive self-promotion or advertisements: please use the dedicated channel for job postings</li> <li>Share content that adds value to the community, such as relevant articles, resources, and insights.</li> </ul> </li> <li>Privacy and Data Protection<ul> <li>Respect the privacy of others. Do not share personal information without explicit consent.</li> <li>Do not share confidential or sensitive information in the channel.</li> </ul> </li> <li>Helpful and Supportive Environment<ul> <li>Offer help and support to fellow members when possible. Collaboration is key to our community.</li> <li>Ask questions and seek assistance respectfully. We\u2019re here to learn and grow together.</li> </ul> </li> <li>Reporting Violations<ul> <li>If you encounter behaviour that violates this code of conduct, please report it to Admins promptly (for information on how to browse a list of Admins, refer to the Slack help center) .</li> <li>Do not engage in public call-outs or confrontations. Let the Admins handle the situation.</li> </ul> </li> <li>Moderator Authority<ul> <li>Respect the decisions of Admins. They are responsible for maintaining the integrity of the space and ensuring a positive environment. Continuous Improvement This code of conduct is subject to updates and improvements. Your feedback is valuable in creating a better community.</li> </ul> </li> </ol> <p>By participating in the CSCS Users Slack, you agree to abide by this code of conduct. Remember, your contributions can have a positive impact on the community. Let\u2019s work together to create an environment that fosters learning, collaboration, and innovation. Thank you for being a part of the CSCS Users Slack community! \u001b</p>"},{"location":"policies/support/","title":"User Support Policies","text":""},{"location":"policies/support/#user-support-policies","title":"User Support Policies","text":"<p>CSCS operates an advanced research infrastructure dedicated to High-Performance Computing (HPC) and other scientific applications. Our infrastructure encompasses a wide array of resources including compute, network, supporting software and tools, and several software applications used by a broad user base. Our user support policies outline the level of assistance users can expect, the types of support offered, and the guidelines for requesting and receiving assistance.</p>"},{"location":"policies/support/#best-effort-support","title":"Best Effort Support","text":"<p>CSCS is committed to offering best effort support to our users. Our goal is to provide responsive and effective assistance, ensuring the hardware and software infrastructure operates at a high level to satisfy the majority of the scientific community\u2019s needs. However, while we will make a reasonable attempt to assist users with their inquiries, we cannot always guarantee a resolution.</p> <p>Our best effort support includes the following elements:</p> <ul> <li>Timely Responses: Users can expect an initial acknowledgment or response to their inquiry in a timely manner during regular working hours.</li> <li>Direct Assistance: Our support staff is available to provide guidance on technical issues, configuration challenges, troubleshooting, and to offer general advice to address an issue. </li> <li>Escalation Process: In cases where initial support efforts are insufficient to resolve an issue CSCS may, at its discretion, escalate an issue to additional staff or third-party vendors, contingent upon the availability of resources or capacity.</li> <li>Quality Documentation: CSCS provides comprehensive, accurate, and up-to-date documentation. This documentation is designed to help users understand and effectively utilize our infrastructure and services.</li> </ul> <p>CSCS reserves the right to decline support for requests that fall outside the scope of the activities described in the user\u2019s initial project allocation proposal. Support will be focused on ensuring that the resources are used in alignment with the approved objectives and goals. Requests that significantly deviate from the original proposal may not be accommodated.</p> <p></p>"},{"location":"policies/support/#user-applications","title":"User Applications","text":"<p>User applications are those brought to CSCS systems by the users, whether they are developed by the users themselves or another third-party. Packages or applications not provided by CSCS are considered user applications. Users may need to compile or adapt these applications to our system. CSCS will provide guidance on deploying applications on our systems, including configuration and optimizations of the CSCS environment. While we can assist with infrastructure-related issues, we can not configure, optimize, debug, or fix the applications themselves. Users are responsible for resolving application-specific issues themselves or contacting the respective developers.</p> <p></p>"},{"location":"policies/support/#officially-supported-applications","title":"Officially Supported Applications","text":"<p>CSCS offers a range of officially supported applications and their respective versions and configurations, which are packaged and released by CSCS or its supply partners. These packages benefit from our resources, expertise, and comprehensive documentation. They include mission-critical software chosen for their significant impact on the center\u2019s goals, strategic projects, and wide user base. Users can expect timely assistance, troubleshooting, optimization, and integration with CSCS infrastructure for these packages. This support also extends to common tools and libraries provided by CSCS for the development and deployment of scientific applications.</p> <p>While CSCS provides enhanced support for third-party software included in our officially supported applications, our ability to resolve issues is contingent on the extent of our expertise and control. Bugs or other problems that fall outside of our immediate control will be escalated to the relevant third-party vendors, but further resolution will depend on their response and capabilities, limiting our ability to fully address such issues.</p>"},{"location":"policies/support/#prioritisation-criteria","title":"Prioritisation Criteria","text":"<p>Support cases will be prioritised based on factors such as the impact on CSCS\u2019s overall mission and services, potential for knowledge transfer, degree of expertise required, and time and effort required to provide support. Issues directly concerning products and services offered by CSCS will be given higher priority.</p>"},{"location":"policies/support/#collaborative-support","title":"Collaborative Support","text":"<p>The effectiveness and efficiency of our support are greatly enhanced when users work collaboratively with us. By providing thorough information users enable us to deliver more effective and timely assistance. To facilitate effective support, users are expected to:</p> <ul> <li>Consult Documentation: Users are encouraged to review the provided documentation and indicate what they have consulted before seeking support.</li> <li>Provide Detailed Information: Users should offer, to the best of their ability, sufficient documentation and information about their software and the issues they are experiencing.     This includes detailing previous attempts to resolve the issue and any relevant error messages or logs. Clear and precise communication of the problem and steps already taken helps us diagnose and address issues more efficiently.</li> </ul>"},{"location":"policies/support/#closure-of-support-tickets","title":"Closure of Support Tickets","text":"<p>Support tickets related to user applications will be closed if, after providing all feasible guidance and troubleshooting within our support scope and capacity, it is determined that the issue lies beyond the control of CSCS, such as in the user\u2019s application code or third-party dependencies. In such cases, the ticket will be closed after the user has been informed of the situation and provided with any relevant recommendations or resources for further investigation. Users are welcome to reopen the ticket if new, actionable information becomes available.</p>"},{"location":"policies/support/#communication-channels","title":"Communication Channels","text":"<p>Users can request support through the CSCS Service Desk. Updates and communication with support staff will be provided through e-mail or via the Service Desk. Users are also encouraged to communicate with each other via our community channels. CSCS reserves the right to make other forms of communication also available.</p>"},{"location":"policies/support/#continuous-improvement","title":"Continuous Improvement","text":"<p>We are committed to continuously improving our support services. Feedback from users is welcomed and will be used to refine our support policies and procedures to better meet the needs of our community.</p> <p>By adhering to this user support policy, we aim to ensure a consistent and satisfactory support experience for all users at CSCS.</p>"},{"location":"running/","title":"Running Jobs","text":"<p>Slurm is used on CSCS systems to schedule jobs. For scheduling many small jobs (1 core or short time) we recommend HyperQueue. The job report tool can be used in Slurm jobs to collect reports on how well an application uses the system.</p>"},{"location":"running/hyperqueue/","title":"HyperQueue","text":""},{"location":"running/hyperqueue/#hyperqueue","title":"HyperQueue","text":"<p>GREASY</p> <p>GREASY is not supported at CSCS anymore. We recommend using HyperQueue instead.</p> <p>HyperQueue is a meta-scheduler designed for high-throughput computing on high-performance computing (HPC) clusters. It addresses the inefficiency of using traditional schedulers like Slurm for a large number of small, short-lived tasks by allowing you to bundle them into a single, larger Slurm job. This approach minimizes scheduling overhead and improves resource utilization.</p> <p>By using a meta-scheduler like HyperQueue, you get fine-grained control over your tasks within the allocated resources of a single batch job. It\u2019s especially useful for workflows that involve numerous tasks, each requiring minimal resources (e.g., a single CPU core or GPU) or a short runtime.</p> <p></p>"},{"location":"running/hyperqueue/#setup","title":"Setup","text":"<p>Before you can use HyperQueue, you\u2019ll need to download it. No installation is needed as it is a statically linked binary with no external dependencies. You can download the latest version from the official site. Because there are different architectures on Alps (ARM and x86_64), we recommend unpacking the binary in <code>$HOME/.local/&lt;arch&gt;/bin</code>, as described here.</p> <p></p>"},{"location":"running/hyperqueue/#example-workflow","title":"Example workflow","text":"<p>This example demonstrates a basic HyperQueue workflow by running a large number of \u201chello world\u201d tasks, some on a CPU and others on a GPU.</p> <p></p>"},{"location":"running/hyperqueue/#the-task-script","title":"The task script","text":"<p>First, create a simple script that represents the individual tasks you want to run. This script will be executed by HyperQueue workers.</p> task.sh<pre><code>#!/usr/local/bin/bash\n\n# This script is a single task that will be run by HyperQueue.\n# HQ_TASK_ID is an environment variable set by HyperQueue for each task.\n# See HyperQueue documentation for other variables set by HyperQueue\n\necho \"$(date): start task ${HQ_TASK_ID}: $(hostname) CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n\n# Simulate some work\nsleep 30\n\necho \"$(date): end task ${HQ_TASK_ID}: $(hostname) CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES}\"\n</code></pre> <p></p>"},{"location":"running/hyperqueue/#simple-slurm-batch-job-script","title":"Simple Slurm batch job script","text":"<p>Next, create a Slurm batch script that will launch the HyperQueue server and workers, submit your tasks, wait for the tasks to finish, and then shut everything down.</p> job.sh<pre><code>#!/usr/local/bin/bash\n\n#SBATCH --nodes 2\n#SBATCH --ntasks-per-node 1\n#SBATCH --time 00:10:00\n#SBATCH --partition normal\n#SBATCH --account &lt;account&gt;\n\n# Start HyperQueue server and workers\nhq server start &amp;\n\n# Wait for the server to be ready\nhq server wait\n\n# Start HyperQueue workers\nsrun hq worker start &amp;\n\n# Submit tasks (300 CPU tasks and 16 GPU tasks)\nhq submit --resource \"cpus=1\" --array 1-300 ./task.sh;\nhq submit --resource \"gpus/nvidia=1\" --array 1-16 ./task.sh;\n\n# Wait for all jobs to finish\nhq job wait all\n\n# Stop HyperQueue server and workers\nhq server stop\n\necho\necho \"Everything done!\"\n</code></pre> <p>To submit this job, use <code>sbatch</code>: <pre><code>sbatch job.sh\n</code></pre></p> <p></p>"},{"location":"running/hyperqueue/#more-robust-slurm-batch-job-script","title":"More robust Slurm batch job script","text":"<p>A powerful feature of HyperQueue is the ability to resume a job that was interrupted, for example, by reaching a time limit or a node failure. You can achieve this by using a journal file to save the state of your tasks. By adding a journal file, HyperQueue can track which tasks were completed and which are still pending. When you restart the job, it will only run the unfinished tasks.</p> <p>Another useful feature is running multiple servers simultaneously. This can be achieved by starting each server with unique directory set in the variable <code>HQ_SERVER_DIR</code>.</p> <p>Here\u2019s an improved version of the batch script that incorporates these features:</p> job.sh<pre><code>#!/usr/local/bin/bash\n\n#SBATCH --nodes 2\n#SBATCH --ntasks-per-node 1\n#SBATCH --time 00:10:00\n#SBATCH --partition normal\n#SBATCH --account &lt;account&gt;\n\n# Set up the journal file for state tracking\n# If an argument is provided, use it to restore a previous job\n# Otherwise, create a new journal file for the current job\nRESTORE_JOB=$1\nif [ -n \"$RESTORE_JOB\" ]; then\n    export JOURNAL=~/.hq-journal-${RESTORE_JOB}\nelse\n    export JOURNAL=~/.hq-journal-${SLURM_JOBID}\nfi\n\n# Ensure each Slurm job has its own HyperQueue server directory\nexport HQ_SERVER_DIR=~/.hq-server-${SLURM_JOBID}\n\n# Start the HyperQueue server with the journal file\nhq server start --journal=${JOURNAL} &amp;\n\n# Wait for the server to be ready\nhq server wait --timeout=120\nif [ \"$?\" -ne 0 ]; then\n    echo \"Server did not start, exiting ...\"\n    exit 1\nfi\n\n# Start HyperQueue workers\nsrun hq worker start &amp;\n\n# Submit tasks only if we are not restoring a previous job\n# (300 CPU tasks and 16 GPU tasks)\nif [ -z \"$RESTORE_JOB\" ]; then\n    hq submit --resource \"cpus=1\" --array 1-300 ./task.sh;\n    hq submit --resource \"gpus/nvidia=1\" --array 1-16 ./task.sh;\nfi\n\n# Wait for all jobs to finish\nhq job wait all\n\n# Stop HyperQueue server and workers\nhq server stop\n\n# Clean up server directory and journal file\nrm -rf ${HQ_SERVER_DIR}\nrm -rf ${JOURNAL}\n\necho\necho \"Everything done!\"\n</code></pre> <p>To submit a new job, use <code>sbatch</code>: <pre><code>sbatch job.sh\n</code></pre></p> <p>If the job fails for any reason, you can resubmit it and tell HyperQueue to pick up where it left off by passing the original Slurm job ID as an argument:</p> <pre><code>sbatch job.sh &lt;job-id&gt;\n</code></pre> <p>The script will detect the argument, load the journal file from the previous run, and only execute the tasks that haven\u2019t been completed.</p> <p>External references</p> <p>You can find other features and examples in the HyperQueue documentation.</p>"},{"location":"running/jobreport/","title":"Job report","text":""},{"location":"running/jobreport/#job-report","title":"Job report","text":"<p>A batch job summary report is often requested in project proposals at CSCS to demonstrate the effective use of GPUs. jobreport is used in two stages. The first stage monitors an application and records the GPU usage statistics. The monitoring stage must be executed within a <code>slurm</code> environment. The information is recorded as <code>.csv</code> data within a directory  <code>jobreport_${SLURM_JOB_ID}</code> or a directory supplied on the command line. The second stage prints this information in a tabular form that can be inserted into a project proposal.</p>"},{"location":"running/jobreport/#downloading-the-job-summary-report-tool","title":"Downloading the job summary report tool","text":"<p>A precompiled binary for the <code>jobreport</code> utility can be obtained directly from the repository or via the command line:</p> <pre><code>$ wget https://github.com/eth-cscs/alps-jobreport/releases/download/v0.1/jobreport\n$ chmod +x ./jobreport\n</code></pre>"},{"location":"running/jobreport/#command-line-options","title":"Command line options","text":"<p>A full list of command line options with explanations can be obtained by running the command with the <code>--help</code> option:</p> <pre><code>$ ./jobreport --help\nUsage: jobreport [-v -h] [subcommand] -- COMMAND\n\nOptions:\n  -h, --help                        Show this help message\n  -v, --version                     Show version information\n\nSubcommands:\n  monitor                           Monitor the performance metrics for a job. (Default)\n    -h, --help                      Shows help message\n    -o, --output &lt;path&gt;             Specify output directory (default: ./jobreport_&lt;SLURM_JOB_ID&gt;)\n    -u, --sampling_time &lt;seconds&gt;   Set the time between samples (default: automatically determined)\n    -t, --max_time &lt;time&gt;           Set the maximum monitoring time (format: DD-HH:MM:SS, default: 24:00:00)\n  print                             Print a job report\n    -h, --help                      Shows help message\n    -o, --output &lt;path&gt;             Output path for the report file\n  container-hook                    Write enroot hook for jobreport\n    -h, --help                      Shows help message\n    -o, --output &lt;path&gt;             Output path for the enroot hook file\n                                    (default: $HOME/.config/enroot/hooks.d/cscs_jobreport_dcgm_hook.sh)\n\nArguments:\n  COMMAND                           The command to run as the workload\n</code></pre>"},{"location":"running/jobreport/#reported-information","title":"Reported information","text":"<p>The final output from <code>jobreport</code> is a table summarizing the most important details of how your application used the compute resources during its execution. The report is divided into two parts: a general summary and GPU specific values.</p>"},{"location":"running/jobreport/#job-statistics","title":"Job statistics","text":"Field Description Job Id The Slurm job id Step Id The Slurm step id. A job step in Slurm is a subdivision of a job started with srun User The user account that submitted the job Slurm Account The project account that will be billed Start Time, End Time, Elapsed Time The time the job started and ended, and how long it ran Number of Nodes The number of nodes allocated to the job Number of GPUs The number of GPUs allocated to the job Total Energy Consumed The total energy consumed based on the average power usage (below) over the elapsed time Average Power Usage The average power draw over the elapsed time in Watts (W), summed over all GPUs Average SM Utilization The percentage of the process\u2019s lifetime during which Streaming Multiprocessors (SM) were executing a kernel, averaged over all GPUs Average Memory Utilization The percentage of a process\u2019s lifetime during which global (device) memory was being read or written, averaged over all GPUs"},{"location":"running/jobreport/#gpu-specific-values","title":"GPU specific values","text":"Field Description Host The compute node executing a job step GPU The GPU id on a node Elapsed The elapsed time SM Utilization % The percentage of the process\u2019s lifetime during which Streaming Multiprocessors (SM) were executing a kernel Memory Utilization % The percentage of process\u2019s lifetime during which global (device) memory was being read or written"},{"location":"running/jobreport/#example-with-slurm-srun","title":"Example with Slurm: srun","text":"<p>The simplest example to test <code>jobreport</code> is to run it with the sleep command. It is important to separate <code>jobreport</code> (and its options) and your command  with <code>--</code>.</p> <pre><code>$ srun -A my_account -t 5:00 --nodes=1 ./jobreport -- sleep 5\n$ ls\njobreport_16133\n$ ./jobreport print jobreport_16133\nSummary of Job Statistics\n+-----------------------------------------+-----------------------------------------+\n| Job Id                                  | 16133                                   |\n+-----------------------------------------+-----------------------------------------+\n| Step Id                                 | 0                                       |\n+-----------------------------------------+-----------------------------------------+\n| User                                    | jpcoles                                 |\n+-----------------------------------------+-----------------------------------------+\n| Slurm Account                           | unknown_account                         |\n+-----------------------------------------+-----------------------------------------+\n| Start Time                              | 03-07-2024 15:32:24                     |\n+-----------------------------------------+-----------------------------------------+\n| End Time                                | 03-07-2024 15:32:29                     |\n+-----------------------------------------+-----------------------------------------+\n| Elapsed Time                            | 5s                                      |\n+-----------------------------------------+-----------------------------------------+\n| Number of Nodes                         | 1                                       |\n+-----------------------------------------+-----------------------------------------+\n| Number of GPUs                          | 4                                       |\n+-----------------------------------------+-----------------------------------------+\n| Total Energy Consumed                   | 0.5 Wh                                  |\n+-----------------------------------------+-----------------------------------------+\n| Average Power Usage                     | 348.8 W                                 |\n+-----------------------------------------+-----------------------------------------+\n| Average SM Utilization                  | 0%                                      |\n+-----------------------------------------+-----------------------------------------+\n| Average Memory Utilization              | 0%                                      |\n+-----------------------------------------+-----------------------------------------+\n\nGPU Specific Values\n+---------------+------+------------------+------------------+----------------------+\n| Host          | GPU  | Elapsed          | SM Utilization % | Memory Utilization % |\n|               |      |                  | (avg/min/max)    | (avg/min/max)        |\n+---------------+------+------------------+------------------+----------------------+\n| nid006212     | 0    | 5s               |   0 /   0 /   0  |   0 /   0 /   0      |\n| nid006212     | 1    | 5s               |   0 /   0 /   0  |   0 /   0 /   0      |\n| nid006212     | 2    | 5s               |   0 /   0 /   0  |   0 /   0 /   0      |\n| nid006212     | 3    | 5s               |   0 /   0 /   0  |   0 /   0 /   0      |\n+---------------+------+------------------+------------------+----------------------+\n</code></pre> <p><code>jobreport</code> requires successful completion of the application</p> <p>The <code>jobreport</code> tool requires the application to complete successfully. If the application crashes or the job is killed by <code>slurm</code> prematurely, <code>jobreport</code> will not be able to write any output.</p> <p>Too many GPUs reported by <code>jobreport</code></p> <p>If the job reporting utility reports more GPUs than you expect from the number of nodes requested by Slurm, you may be missing options to set the visible devices correctly for your job. See the GH200 Slurm documentation for examples on how to expose GPUs correctly in your job. When oversubscribing ranks to GPUs, the utility will always report too many GPUs. The utility does not combine data for the same GPU from different ranks.</p> <p>workaround known issue on macOS</p> <p>Currently, there is an issue when generating the report file via <code>jobreport print</code> on the macOS terminal:</p> <pre><code>what(): locale::facet::_S_create_c_locale name not valid\n/var/spool/slurmd/job32394/slurm_script: line 21: 199992 Aborted         (core dumped) ./jobreport print report\n</code></pre> <p>To fix this follow these steps:</p> <ol> <li>Open the terminal application</li> <li>In the top-left corner menu select Terminal -&gt; Settings</li> <li>Select your default profile</li> <li>Uncheck \u201cSet locale environment variables on startup\u201d</li> <li>Quit and reopen the terminal and try again. This should fix the issue.</li> </ol>"},{"location":"running/jobreport/#example-with-slurm-batch-script","title":"Example with Slurm: batch script","text":"<p>The <code>jobreport</code> command can be used in a batch script The report printing, too, can be included in the script and does not need the <code>srun</code> command.</p> submit script with jobreport<pre><code>#!/bin/bash\n#SBATCH -t 5:00\n#SBATCH --nodes=2\n\nsrun ./jobreport -o report -- my_command\n./jobreport print report\n</code></pre> <p>When used within an job script, <code>jobreport</code> will work across multiple calls to <code>srun</code>. Each time <code>srun</code> is called, <code>slurm</code> creates a new job step and <code>jobreport</code> records data for each one. Multiple job steps running simultaneously are also allowed. The job report generated contains sections for each <code>slurm</code> job step.</p> submit script with multiple steps<pre><code>#!/bin/bash\n#SBATCH -t 5:00\n#SBATCH --nodes=2\n\nsrun ./jobreport -o report -- my_command_1\nsrun ./jobreport -o report -- my_command_2\n\nsrun --nodes=1 ./jobreport -o report -- my_command_3 &amp;\nsrun --nodes=1 ./jobreport -o report -- my_command_4 &amp;\n\nwait\n</code></pre>"},{"location":"running/jobreport/#example-with-uenv","title":"Example with uenv","text":"<p>The following example runs a program called <code>burn</code> that computes repeated matrix multiplications to stress the GPUs. It was built with, and requires to run the prgenv-gnu.</p> <pre><code>$ srun --uenv=prgenv-gnu/24.2:v1 -t 5:00 --nodes=1 --ntasks-per-node=4 --gpus-per-task=1 ${JOBREPORT} -o report -- ./burn --gpu=gemm -d 30\n\n$ ./jobreport print report\nSummary of Job Statistics\n+-----------------------------------------+-----------------------------------------+\n| Job Id                                  | 15923                                   |\n+-----------------------------------------+-----------------------------------------+\n| Step Id                                 | 0                                       |\n+-----------------------------------------+-----------------------------------------+\n| User                                    | jpcoles                                 |\n+-----------------------------------------+-----------------------------------------+\n| Slurm Account                           | unknown_account                         |\n+-----------------------------------------+-----------------------------------------+\n| Start Time                              | 03-07-2024 14:54:48                     |\n+-----------------------------------------+-----------------------------------------+\n| End Time                                | 03-07-2024 14:55:25                     |\n+-----------------------------------------+-----------------------------------------+\n| Elapsed Time                            | 36s                                     |\n+-----------------------------------------+-----------------------------------------+\n| Number of Nodes                         | 1                                       |\n+-----------------------------------------+-----------------------------------------+\n| Number of GPUs                          | 4                                       |\n+-----------------------------------------+-----------------------------------------+\n| Total Energy Consumed                   | 18.7 Wh                                 |\n+-----------------------------------------+-----------------------------------------+\n| Average Power Usage                     | 1.8 kW                                  |\n+-----------------------------------------+-----------------------------------------+\n| Average SM Utilization                  | 88%                                     |\n+-----------------------------------------+-----------------------------------------+\n| Average Memory Utilization              | 43%                                     |\n+-----------------------------------------+-----------------------------------------+\n\nGPU Specific Values\n+---------------+------+------------------+------------------+----------------------+\n| Host          | GPU  | Elapsed          | SM Utilization % | Memory Utilization % |\n|               |      |                  | (avg/min/max)    | (avg/min/max)        |\n+---------------+------+------------------+------------------+----------------------+\n| nid007044     | 0    | 36s              |  83 /   0 / 100  |  39 /   0 /  50      |\n| nid007044     | 0    | 36s              |  90 /   0 / 100  |  43 /   0 /  50      |\n| nid007044     | 0    | 36s              |  90 /   0 / 100  |  43 /   0 /  48      |\n| nid007044     | 0    | 36s              |  90 /   0 / 100  |  47 /   0 /  54      |\n+---------------+------+------------------+------------------+----------------------+\n</code></pre> <p>Using <code>jobreport</code> with other uenvs</p> <p><code>jobreport</code> works with any uenv, not just <code>prgenv-gnu</code>.</p>"},{"location":"running/jobreport/#example-with-container-engine-ce","title":"Example with container-engine (CE)","text":"<p>Running <code>jobreport</code> with the container-engine (CE) requires a little more setup to allow the CE to mount the required GPU library paths inside the container.</p> <p>A script to set up the mount points needs to be placed in <code>${HOME}/.config/enroot/hooks.d/</code>. This can be generated with the <code>jobreport</code> tool, and by default, the script will be placed in <code>${HOME}/.config/enroot/hooks.d/cscs_jobreport.sh</code>.</p> Generate DCGM hook<pre><code>$ ./jobreport container-hook\nWriting enroot hook to \"/users/myuser/.config/enroot/hooks.d/cscs_jobreport_dcgm_hook.sh\"\nAdd the following to your container .toml file:\n\n[annotations]\ncom.hooks.dcgm.enabled = \"true\"\n</code></pre> <p>As indicated by the output, the hook must be added to the container <code>.toml</code> file.</p> Example .toml file<pre><code>[annotations]\ncom.hooks.dcgm.enabled = \"true\"\n</code></pre> <p>Once the CE is configured, only the EDF file (here <code>my-edf.toml</code>) needs to be specified along with a call to <code>jobreport</code>:</p> Run jobreport in a container<pre><code>$ srun --environment=my-edf.toml ./jobreport -- sleep 5\n</code></pre> <p>Using <code>jobreport</code> with other container images</p> <p><code>jobreport</code> works with any container image, as long as the hook is set up and the EDF file has the correct annotation.</p>"},{"location":"running/known-issues/","title":"Known issues","text":""},{"location":"running/known-issues/#known-issues","title":"Known Issues","text":""},{"location":"running/known-issues/#out-of-memory-on-gh200-nodes","title":"Out of Memory on GH200 nodes","text":"<p>There is a known issue with Nvidia GPU driver version R550.54.15, currently installed on all GH200 nodes, that can reduce the amount of available GPU memory.</p> <p>Under normal conditions, Linux in-memory file caches may migrate from CPU to GPU memory. Allocating GPU memory should trigger the eviction of these caches, freeing up GPU memory. However, due to this bug, eviction does not occur, leading to out-of-memory errors.</p> <p>Applications with heavy I/O workloads are especially affected, as increased I/O generates more cached data. While we currently ensure that at least 90% of GPU memory is available when a node is allocated to a job, available memory may still decrease during the job\u2019s execution.</p> <p>This issue is fixed in driver version R570, which will be deployed during a future system maintenance.</p>"},{"location":"running/slurm/","title":"Slurm","text":""},{"location":"running/slurm/#slurm","title":"Slurm","text":"<p>CSCS uses the Slurm workload manager to efficiently schedule and manage jobs on Alps vClusters. Slurm is an open-source, highly scalable job scheduler that allocates computing resources, queues user jobs, and optimizes workload distribution across the cluster. It supports advanced scheduling policies, job dependencies, resource reservations, and accounting, making it well-suited for high-performance computing environments.</p> <p>Refer to the Quick Start User Guide for commonly used terminology and commands.</p> <ul> <li> <p> Configuring jobs</p> <p>Specific guidance for configuring Slurm jobs on different node types.</p> <p> GH200 nodes (Daint, Clariden, Santis)</p> <p> AMD CPU-only nodes (Eiger)</p> </li> <li> <p> Node sharing</p> <p>Guides on how to effectively use all resources on nodes by running more than one job per node.</p> <p> Node sharing</p> <p> Multiple MPI jobs per node</p> </li> </ul>"},{"location":"running/slurm/#accounts-and-resources","title":"Accounts and resources","text":"<p>Slurm associates each job with a CSCS project in order to perform accounting. The project to use for accounting is specified using the <code>--account/-A</code> flag. If no job is specified, the primary project is used as the default.</p> Which projects am I a member of? <p>Users often are part of multiple projects, and by extension their associated <code>groupd_id</code> groups. You can get a list of your groups using the <code>id</code> command in the terminal: <pre><code>$ id $USER\nuid=12345(bobsmith) gid=32819(g152) groups=32819(g152),33119(g174),32336(vasp6)\n</code></pre> Here the user <code>bobsmith</code> is in three projects (<code>g152</code>, <code>g174</code> and <code>vasp6</code>), with the project <code>g152</code> being their primary project.</p> What is my primary project? <p>In the terminal, use the following command to find your primary group: <pre><code>$ id -gn $USER\ng152\n</code></pre></p> Specifying the account on the command line<pre><code>$ srun -A g123        -n4 -N1 ./run\n$ srun --account=g123 -n4 -N1 ./run\n$ sbatch --account=g123 ./job.sh\n</code></pre> Specifying the account in an sbatch script<pre><code>#!/bin/bash\n\n#SBATCH --account=g123\n#SBATCH --job-name=example-%j\n#SBATCH --time=00:30:00\n#SBATCH --nodes=4\n...\n</code></pre> <p>Note</p> <p>The flags <code>--account</code> and <code>-Cmc</code> that were required on the old Eiger cluster are no longer required.</p>"},{"location":"running/slurm/#prioritisation-and-scheduling","title":"Prioritisation and scheduling","text":"<p>Job priorities are determined based on each project\u2019s resource usage relative to its quarterly allocation, as well as in comparison to other projects. An aging factor is also applied to each job in the queue to ensure fairness over time.</p> <p>Since users from various projects are continuously submitting jobs, the relative priority of jobs is dynamic and may change frequently. As a result, estimated start times are approximate and subject to change based on new job submissions.</p> <p>Additionally, short-duration jobs may be selected for backfilling \u2014 a process where the scheduler fills in available time slots while preparing to run a larger, higher-priority job.</p> <p></p>"},{"location":"running/slurm/#partitions","title":"Partitions","text":"<p>At CSCS, Slurm is configured to accommodate the diverse range of node types available in our HPC clusters. These nodes vary in architecture, including CPU-only nodes and nodes equipped with different types of GPUs. Because of this heterogeneity, Slurm must be tailored to ensure efficient resource allocation, job scheduling, and workload management specific to each node type.</p> <p>Each type of node has different resource constraints and capabilities, which Slurm takes into account when scheduling jobs. For example, CPU-only nodes may have configurations optimized for multi-threaded CPU workloads, while GPU nodes require additional parameters to allocate GPU resources efficiently. Slurm ensures that user jobs request and receive the appropriate resources while preventing conflicts or inefficient utilization.</p> <p></p> <p>How to check the partitions and number of nodes therein?</p> <p>You can check the size of the system by running the following command in the terminal: <pre><code>$ sinfo --format \"| %20R | %10D | %10s | %10l | %10A |\"\n| PARTITION            | NODES      | JOB_SIZE   | TIMELIMIT  | NODES(A/I) |\n| debug                | 32         | 1-2        | 30:00      | 3/29       |\n| normal               | 1266       | 1-infinite | 1-00:00:00 | 812/371    |\n| xfer                 | 2          | 1          | 1-00:00:00 | 1/1        |\n</code></pre> The last column shows the number of nodes that have been allocated in currently running jobs (<code>A</code>) and the number of jobs that are idle (<code>I</code>).</p> <p></p>"},{"location":"running/slurm/#debug-partition","title":"Debug partition","text":"<p>The Slurm <code>debug</code> partition is useful for quick turnaround workflows. The partition has a short maximum time (timelimit can be seen with <code>sinfo -p debug</code>), and a low number of maximum nodes (the <code>MaxNodes</code> can be seen with <code>scontrol show partition=debug</code>).</p> <p></p>"},{"location":"running/slurm/#normal-partition","title":"Normal partition","text":"<p>This is the default partition, and will be used when you do not explicitly set a partition. This is the correct choice for standard jobs. The maximum time is usually set to 24 hours (<code>sinfo -p normal</code> for timelimit), and the maximum nodes can be as much as nodes are available.</p> <p>The following sections will provide detailed guidance on how to use Slurm to request and manage CPU cores, memory, and GPUs in jobs. These instructions will help users optimize their workload execution and ensure efficient use of CSCS computing resources.</p>"},{"location":"running/slurm/#affinity","title":"Affinity","text":"<p>The following sections will document how to use Slurm on different compute nodes available on Alps. To demonstrate the effects different Slurm parameters, we will use a little command line tool affinity that prints the CPU cores and GPUs that are assigned to each MPI rank in a job, and which node they are run on.</p> <p>We strongly recommend using a tool like affinity to understand and test the Slurm configuration for jobs, because the behavior of Slurm is highly dependent on the system configuration. Parameters that worked on a different cluster\u2014or with a different Slurm version or configuration on the same cluster\u2014are not guaranteed to give the same results.</p> <p>It is straightforward to build the affinity tool to experiment with Slurm configurations.</p> Compiling affinity<pre><code>$ uenv start prgenv-gnu/24.11:v2 --view=default     #(1)\n$ git clone https://github.com/bcumming/affinity.git\n$ cd affinity; mkdir build; cd build;\n$ CC=gcc CXX=g++ cmake ..                           #(2)\n$ CC=gcc CXX=g++ cmake .. -DAFFINITY_GPU_BACKEND=cuda       #(3)\n$ CC=gcc CXX=g++ cmake .. -DAFFINITY_GPU_BACKEND=rocm       #(4)\n</code></pre> <ol> <li> <p>Affinity can be built using <code>prgenv-gnu</code> on all clusters.</p> </li> <li> <p>By default affinity will build with MPI support and no GPU support: configure with no additional arguments on a CPU-only system like Eiger.</p> </li> <li> <p>Enable CUDA support on systems that provide NVIDIA GPUs.</p> </li> <li> <p>Enable ROCM support on systems that provide AMD GPUs.</p> </li> </ol> <p>The build generates the following executables:</p> <ul> <li><code>affinity.omp</code>: tests thread affinity with no MPI (always built).</li> <li><code>affinity.mpi</code>: tests thread affinity with MPI (built by default).</li> <li><code>affinity.cuda</code>: tests thread and GPU affinity with MPI (built with <code>-DAFFINITY_GPU_BACKEND=cuda</code>).</li> <li><code>affinity.rocm</code>: tests thread and GPU affinity with MPI (built with <code>-DAFFINITY_GPU_BACKEND=rocm</code>).</li> </ul> Testing CPU affinity <p>Test CPU affinity (this can be used on both CPU and GPU enabled nodes). <pre><code>$ uenv start prgenv-gnu/24.11:v2 --view=default\n$ srun -n8 -N2 -c72 ./affinity.mpi\naffinity test for 8 MPI ranks\nrank   0 @ nid006363: thread 0 -&gt; cores [  0: 71]\nrank   1 @ nid006363: thread 0 -&gt; cores [ 72:143]\nrank   2 @ nid006363: thread 0 -&gt; cores [144:215]\nrank   3 @ nid006363: thread 0 -&gt; cores [216:287]\nrank   4 @ nid006375: thread 0 -&gt; cores [  0: 71]\nrank   5 @ nid006375: thread 0 -&gt; cores [ 72:143]\nrank   6 @ nid006375: thread 0 -&gt; cores [144:215]\nrank   7 @ nid006375: thread 0 -&gt; cores [216:287]\n</code></pre></p> <p>In this example there are 8 MPI ranks:</p> <ul> <li>ranks <code>0:3</code> are on node <code>nid006363</code>;</li> <li>ranks <code>4:7</code> are on node <code>nid006375</code>;</li> <li>each rank has 72 threads numbered <code>0:71</code>;</li> <li>all threads on each rank have affinity with the same 72 cores;</li> <li>each rank gets 72 cores, e.g. rank 1 gets cores <code>72:143</code> on node <code>nid006363</code>.</li> </ul> Testing GPU affinity <p>Use <code>affinity.cuda</code> or <code>affinity.rocm</code> to test on GPU-enabled systems.</p> <pre><code>$ srun -n4 -N1 ./affinity.cuda                      #(1)\nGPU affinity test for 4 MPI ranks\nrank      0 @ nid005555\n cores   : [0:7]\n gpu   0 : GPU-2ae325c4-b542-26c2-d10f-c4d84847f461\n gpu   1 : GPU-5923dec6-288f-4418-f485-666b93f5f244\n gpu   2 : GPU-170b8198-a3e1-de6a-ff82-d440f71c05da\n gpu   3 : GPU-0e184efb-1d1f-f278-b96d-15bc8e5f17be\nrank      1 @ nid005555\n cores   : [72:79]\n gpu   0 : GPU-2ae325c4-b542-26c2-d10f-c4d84847f461\n gpu   1 : GPU-5923dec6-288f-4418-f485-666b93f5f244\n gpu   2 : GPU-170b8198-a3e1-de6a-ff82-d440f71c05da\n gpu   3 : GPU-0e184efb-1d1f-f278-b96d-15bc8e5f17be\nrank      2 @ nid005555\n cores   : [144:151]\n gpu   0 : GPU-2ae325c4-b542-26c2-d10f-c4d84847f461\n gpu   1 : GPU-5923dec6-288f-4418-f485-666b93f5f244\n gpu   2 : GPU-170b8198-a3e1-de6a-ff82-d440f71c05da\n gpu   3 : GPU-0e184efb-1d1f-f278-b96d-15bc8e5f17be\nrank      3 @ nid005555\n cores   : [216:223]\n gpu   0 : GPU-2ae325c4-b542-26c2-d10f-c4d84847f461\n gpu   1 : GPU-5923dec6-288f-4418-f485-666b93f5f244\n gpu   2 : GPU-170b8198-a3e1-de6a-ff82-d440f71c05da\n gpu   3 : GPU-0e184efb-1d1f-f278-b96d-15bc8e5f17be\n$ srun -n4 -N1 --gpus-per-task=1 ./affinity.cuda    #(2)\nGPU affinity test for 4 MPI ranks\nrank      0 @ nid005675\n cores   : [0:7]\n gpu   0 : GPU-a16a8dac-7661-a44b-c6f8-f783f6e812d3\nrank      1 @ nid005675\n cores   : [72:79]\n gpu   0 : GPU-ca5160ac-2c1e-ff6c-9cec-e7ce5c9b2d09\nrank      2 @ nid005675\n cores   : [144:151]\n gpu   0 : GPU-496a2216-8b3c-878e-e317-36e69af11161\nrank      3 @ nid005675\n cores   : [216:223]\n gpu   0 : GPU-766e3b8b-fa19-1480-b02f-0dfd3f2c87ff\n</code></pre> <ol> <li> <p>Test GPU affinity: note how all 4 ranks see the same 4 GPUs.</p> </li> <li> <p>Test GPU affinity: note how the <code>--gpus-per-task=1</code> parameter assigns a unique GPU to each rank.</p> </li> </ol> <p>Quick affinity checks</p> <p>The Slurm flag <code>--cpu-bind=verbose</code> prints information about MPI ranks and their thread affinity.</p> <p>The mask it prints is not very readable, but it can be used with the <code>true</code> command to quickly test Slurm parameters without building the Affinity tool.</p> hello<pre><code>$ srun --cpu-bind=verbose -c32 -n4 -N1 --hint=nomultithread -- true\ncpu-bind=MASK - nid002156, task  0  0 [147694]: mask 0xffffffff set\ncpu-bind=MASK - nid002156, task  1  1 [147695]: mask 0xffffffff0000000000000000 set\ncpu-bind=MASK - nid002156, task  2  2 [147696]: mask 0xffffffff00000000 set\ncpu-bind=MASK - nid002156, task  3  3 [147697]: mask 0xffffffff000000000000000000000000 set\n</code></pre> <p>You can also check GPU affinity by inspecting the value of the <code>CUDA_VISIBLE_DEVICES</code> environment variable.</p> <p></p>"},{"location":"running/slurm/#slurm-features","title":"Slurm features","text":"<p>Slurm allows specifying constraints for jobs, which can be used to change features available on nodes in a job. CSCS implements a few custom features, described below, that can be selected on certain clusters. To check which features are available on a cluster, for example on the <code>normal</code> partition, use <code>sinfo</code>:</p> <pre><code>$ sinfo --partition normal --format %b\nACTIVE_FEATURES\ngh,gpu,thp_never,thp_always,thp_madvise,nvidia_vboost_enabled,nvidia_vboost_disabled\n</code></pre> <p>One or more constraints can be selected using the <code>--constraint</code>/<code>-C</code> flag of <code>sbatch</code> or <code>srun</code>:</p> <pre><code>sbatch --constraint thp_never&amp;nvidia_vboost_enabled batch.sh\n</code></pre> <p></p>"},{"location":"running/slurm/#transparent-hugepages","title":"Transparent hugepages","text":"<p>The THP Slurm feature is only available on GH200 nodes</p> <p>Transparent hugepages (THP) are a Linux kernel feature that allows automatically coalescing pages into huge pages without the user application explicitly asking for hugepages:</p> <p>Performance critical computing applications dealing with large memory working sets are already running on top of libhugetlbfs and in turn hugetlbfs. Transparent HugePage Support (THP) is an alternative mean of using huge pages for the backing of virtual memory with huge pages that supports the automatic promotion and demotion of page sizes and without the shortcomings of hugetlbfs.</p> <p>While this feature generally improves performance, we have observed degrading application performance with the THP feature enabled due to the page coalescing blocking progress on certain operations. An example of this is ICON, a latency-sensitive application where small delays can can cause large performance drops.</p> <p>THP support is enabled by default, and the current setting can be checked with:</p> <pre><code>$ cat /sys/kernel/mm/transparent_hugepage/enabled\n[always] madvise never\n</code></pre> <p>A detailed explanation of how the different options behave can be found in the THP documentation.</p> <p>The available Slurm features to select the THP mode are listed below:</p> Kernel setting Slurm constraint <code>always</code> <code>thp_always</code> (default) <code>madvise</code> <code>thp_madvise</code> <code>never</code> <code>thp_never</code> <p></p>"},{"location":"running/slurm/#nvidia-vboost","title":"NVIDIA vboost","text":"<p>The NVIDIA vboost Slurm feature is only available on GH200 nodes</p> <p>The NVIDIA NeMo documentation describes the vboost feature as:</p> <p>NVIDIA GPUs support a CPU core clock boost mode, which increases the core clock rate by reducing the off-chip memory clock rate. This is particularly beneficial for LLMs, which are typically compute throughput-bound.</p> <p>The vboost slider is at <code>0</code> by default, and the current value can be checked checked with <code>nvidia-smi</code>:</p> <pre><code>$ nvidia-smi boost-slider --list\n+-------------------------------------------------+\n| GPU Boost Slider                                |\n| GPU     Slider       Max Value    Current Value |\n|=================================================|\n|   0     vboost           4              0       |\n+-------------------------------------------------+\n|   1     vboost           4              0       |\n+-------------------------------------------------+\n|   2     vboost           4              0       |\n+-------------------------------------------------+\n|   3     vboost           4              0       |\n+-------------------------------------------------+\n</code></pre> <p>The slider can be set to <code>1</code> using the <code>nvidia_vboost_enable</code> feature:</p> vboost setting Slurm constraint <code>0</code> <code>nvidia_vboost_disable</code> (default) <code>1</code> <code>nvidia_vboost_enable</code> <p></p>"},{"location":"running/slurm/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>The GH200 nodes on Alps have four GPUs per node, and Slurm job submissions must be configured appropriately to best make use of the resources. Applications that can saturate the GPUs with a single process per GPU should generally prefer this mode. Configuring Slurm jobs to use a single GPU per rank is also the most straightforward setup. Some applications perform badly with a single rank per GPU, and require use of NVIDIA\u2019s Multi-Process Service (MPS) to oversubscribe GPUs with multiple ranks per GPU.</p> <p>The best Slurm configuration is application- and workload-specific, so it is worth testing which works best in your particular case. See Scientific Applications for information about recommended application-specific Slurm configurations.</p> <p>Warning</p> <p>The GH200 nodes have their GPUs configured in \u201cdefault\u201d compute mode. The \u201cdefault\u201d mode is used to avoid issues with certain containers. Unlike \u201cexclusive process\u201d mode, \u201cdefault\u201d mode allows multiple processes to submit work to a single GPU simultaneously. This also means that different ranks on the same node can inadvertently use the same GPU leading to suboptimal performance or unused GPUs, rather than job failures.</p> <p>Some applications benefit from using multiple ranks per GPU. However, MPS should be used in these cases.</p> <p>If you are unsure about which GPU is being used for a particular rank, print the <code>CUDA_VISIBLE_DEVICES</code> variable, along with e.g. <code>SLURM_LOCALID</code>, <code>SLURM_PROCID</code>, and <code>SLURM_NODEID</code> variables, in your job script. If the variable is unset or empty all GPUs are visible to the rank and the rank will in most cases only use the first GPU.</p> <p></p>"},{"location":"running/slurm/#one-rank-per-gpu","title":"One rank per GPU","text":"<p>Configuring Slurm to use one GH200 GPU per rank is easiest done using the <code>--ntasks-per-node=4</code> and <code>--gpus-per-task=1</code> Slurm flags. For advanced users, using <code>--gpus-per-task</code> is equivalent to setting <code>CUDA_VISIBLE_DEVICES</code> to <code>SLURM_LOCALID</code>, assuming the job is using four ranks per node. The examples below launch jobs on two nodes with four ranks per node using <code>sbatch</code> and <code>srun</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gh200-single-rank-per-gpu\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus-per-task=1\n\nsrun &lt;application&gt;\n</code></pre> <p>Omitting the <code>--gpus-per-task</code> results in <code>CUDA_VISIBLE_DEVICES</code> being unset, which will lead to most applications using the first GPU on all ranks.</p> <p></p>"},{"location":"running/slurm/#multiple-ranks-per-gpu","title":"Multiple ranks per GPU","text":"<p>Using multiple ranks per GPU can improve performance e.g. of applications that don\u2019t generate enough work for a GPU using a single rank, or ones that scale badly to all 72 cores of the Grace CPU. In these cases Slurm jobs must be configured to assign multiple ranks to a single GPU. This is best done using NVIDIA\u2019s Multi-Process Service (MPS). To use MPS, launch your application using the following wrapper script, which will start MPS on one rank per node and assign GPUs to ranks according to the CPU mask of a rank, ensuring the closest GPU is used:</p> mps-wrapper.sh<pre><code>#!/bin/bash\n# Example mps-wrapper.sh usage:\n# &gt; srun [srun args] mps-wrapper.sh [cmd] [cmd args]\n\n# Only this path is supported by MPS\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log-$(id -un)\n\n# Launch MPS from a single rank per node\nif [[ $SLURM_LOCALID -eq 0 ]]; then\n    CUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\nfi\n\n# Set CUDA device. Disable HWLOC_KEEP_NVIDIA_GPU_NUMA_NODES to avoid GPU NUMA\n# nodes appearing in the list of CUDA devices. They start appearing in hwloc\n# version 2.11.\nnuma_nodes=$(HWLOC_KEEP_NVIDIA_GPU_NUMA_NODES=0 hwloc-calc --physical --intersect NUMAnode $(hwloc-bind --get --taskset))\nexport CUDA_VISIBLE_DEVICES=$numa_nodes\n\n# Wait for MPS to start\nsleep 1\n\n# Run the command\nnumactl --membind=$numa_nodes \"$@\"\nresult=$?\n\n# Quit MPS control daemon before exiting\nif [[ $SLURM_LOCALID -eq 0 ]]; then\n    echo quit | nvidia-cuda-mps-control\nfi\n\nexit $result\n</code></pre> <p>Save the above script as <code>mps-wrapper.sh</code> and make it executable with <code>chmod +x mps-wrapper.sh</code>. If the <code>mps-wrapper.sh</code> script is in the current working directory, you can then launch jobs using MPS for example as follows:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gh200-multiple-ranks-per-gpu\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=32\n#SBATCH --cpus-per-task=8\n\nsrun ./mps-wrapper.sh &lt;application&gt;\n</code></pre> <p>Note that in the example job above:</p> <ul> <li><code>--gpus-per-node</code> is not set at all; the <code>mps-wrapper.sh</code> script ensures that the right GPU is visible for each rank using <code>CUDA_VISIBLE_DEVICES</code></li> <li><code>--ntasks-per-node</code> is set to 32; this results in 8 ranks per GPU</li> <li><code>--cpus-per-task</code> is set to 8; this ensures that threads are not allowed to migrate across the whole GH200 node</li> </ul> <p>The configuration that is optimal for your application may be different.</p> <p></p>"},{"location":"running/slurm/#amd-cpu-nodes","title":"AMD CPU nodes","text":"<p>Alps has nodes with two AMD Epyc Rome CPU sockets per node for CPU-only workloads, most notably in the Eiger cluster provided by the HPC Platform. For a detailed description of the node hardware, see the AMD Rome node hardware documentation.</p> Node description <ul> <li>The node has 2 x 64 core sockets</li> <li>Each socket is divided into 4 NUMA regions<ul> <li>the 16 cores in each NUMA region have faster memory access to their of 32 GB</li> </ul> </li> <li>Each core has two processing units (PUs)</li> </ul> <p></p> <p>Each MPI rank is assigned a set of cores on a node, and Slurm provides flags that can be used directly as flags to <code>srun</code>, or as arguments in an <code>sbatch</code> script. Here are some basic flags that we will use to distribute work.</p> flag meaning <code>-n</code>, <code>--ntasks</code> The total number of MPI ranks <code>-N</code>, <code>--nodes</code> The total number of nodes <code>--ntasks-per-node</code> The total number of nodes <code>-c</code>, <code>--cpus-per-task</code> The number of cores to assign to each rank. <code>--hint=nomultithread</code> Use only one PU per core <p>Slurm is highly configurable</p> <p>These are a subset of the most useful flags. Call <code>srun --help</code> or <code>sbatch --help</code> to get a complete list of all the flags available on your target cluster. Note that the exact set of flags available depends on the Slurm version, how Slurm was configured, and Slurm plugins.</p> <p>The first example assigns 2 MPI ranks per node, with 64 cores per rank, with the two PUs per core: One MPI rank per socket<pre><code># one node\n$ srun -n2 -N1 -c64 ./affinity.mpi\naffinity test for 2 MPI ranks\nrank   0 @ nid002199: thread 0 -&gt; cores [  0: 31,128:159]\nrank   1 @ nid002199: thread 0 -&gt; cores [ 64: 95,192:223]\n\n# two nodes\n$ srun -n4 -N2 -c64 ./affinity.mpi\naffinity test for 4 MPI ranks\nrank   0 @ nid001512: thread 0 -&gt; cores [  0: 31,128:159]\nrank   1 @ nid001512: thread 0 -&gt; cores [ 64: 95,192:223]\nrank   2 @ nid001515: thread 0 -&gt; cores [  0: 31,128:159]\nrank   3 @ nid001515: thread 0 -&gt; cores [ 64: 95,192:223]\n</code></pre></p> <p>Note</p> <p>In the above example we use <code>--ntasks/-n</code> and <code>--nodes/-N</code>. It is possible to achieve the same effect using <code>--nodes</code> and <code>--ntasks-per-node</code>, for example the following both give 8 ranks on 4 nodes:</p> <pre><code>srun --nodes=4 --ntasks=8\nsrun --nodes=4 --ntasks-per-node=2\n</code></pre> <p>It is often more efficient to only run one task per core instead of the default two PU, which can be achieved using the <code>--hint=nomultithread</code> option. One MPI rank per socket with 1 PU per core<pre><code>$ srun -n2 -N1 -c64 --hint=nomultithread ./affinity.mpi\naffinity test for 2 MPI ranks\nrank   0 @ nid002199: thread 0 -&gt; cores [  0: 63]\nrank   1 @ nid002199: thread 0 -&gt; cores [ 64:127]\n</code></pre></p> <p>Always test</p> <p>The best configuration for performance is highly application specific, with no one-size-fits-all configuration. Take the time to experiment with <code>--hint=nomultithread</code>.</p> <p>Memory on the node is divided into NUMA (non-uniform memory access) regions. The 256 GB of a standard-memory node are divided into 8 NUMA nodes of 32 GB, with 16 cores associated with each node:</p> <ul> <li>memory access is optimal when all the cores of a rank are on the same NUMA node;</li> <li>memory access to NUMA regions on the other socket are significantly slower.</li> </ul> How to investigate the NUMA layout of a node <p>Use the command <code>numactl -H</code>.</p> <p><pre><code>$ srun -n1 numactl -H\navailable: 8 nodes (0-7)\nnode 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\nnode 0 size: 63733 MB\nnode 0 free: 62780 MB\nnode 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\nnode 1 size: 64502 MB\nnode 1 free: 61774 MB\nnode 2 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175\nnode 2 size: 64456 MB\nnode 2 free: 63385 MB\nnode 3 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\nnode 3 size: 64490 MB\nnode 3 free: 62613 MB\nnode 4 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207\nnode 4 size: 64502 MB\nnode 4 free: 63897 MB\nnode 5 cpus: 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223\nnode 5 size: 64502 MB\nnode 5 free: 63769 MB\nnode 6 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\nnode 6 size: 64502 MB\nnode 6 free: 63870 MB\nnode 7 cpus: 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\nnode 7 size: 64428 MB\nnode 7 free: 63712 MB\nnode distances:\nnode   0   1   2   3   4   5   6   7\n  0:  10  12  12  12  32  32  32  32\n  1:  12  10  12  12  32  32  32  32\n  2:  12  12  10  12  32  32  32  32\n  3:  12  12  12  10  32  32  32  32\n  4:  32  32  32  32  10  12  12  12\n  5:  32  32  32  32  12  10  12  12\n  6:  32  32  32  32  12  12  10  12\n  7:  32  32  32  32  12  12  12  10\n</code></pre> The <code>node distances</code> table shows that the cores have the fastest memory access to memory in their own region (<code>10</code>), and fast access (<code>12</code>) to NUMA regions on the same socket. The cost of accessing memory of a NUMA node on the other socket is much higher (<code>32</code>).</p> <p>Note that this command was run on a large-memory node that has 8 x 64 GB NUMA regions, for a total of 512 GB.</p> <p>The examples above placed one rank per socket, which is not optimal for NUMA access, because cores assigned to each rank are spread over the 4 NUMA nodes on the socket. To constrain tasks to NUMA nodes, use 16 cores per task:</p> One MPI rank per NUMA region<pre><code>$ srun -n8 -N1 -c16 --hint=nomultithread ./affinity.mpi\naffinity test for 8 MPI ranks\nrank   0 @ nid002199: thread 0 -&gt; cores [  0: 15]\nrank   1 @ nid002199: thread 0 -&gt; cores [ 64: 79]\nrank   2 @ nid002199: thread 0 -&gt; cores [ 16: 31]\nrank   3 @ nid002199: thread 0 -&gt; cores [ 80: 95]\nrank   4 @ nid002199: thread 0 -&gt; cores [ 32: 47]\nrank   5 @ nid002199: thread 0 -&gt; cores [ 96:111]\nrank   6 @ nid002199: thread 0 -&gt; cores [ 48: 63]\nrank   7 @ nid002199: thread 0 -&gt; cores [112:127]\n</code></pre> <p>Always test</p> <p>It might still be optimal for applications that have high threading efficiency and benefit from using fewer MPI ranks to have one rank per socket or even one one rank per node. Always test!</p>"},{"location":"running/slurm/#openmp","title":"OpenMP","text":"<p>In the above examples all threads on each\u2014we are effectively allowing the OS to schedule the threads on the available set of cores as it sees fit. This often gives the best performance, however sometimes it is beneficial to bind threads to explicit cores.</p> <p>The OpenMP threading runtime provides additional options for controlling the pinning of threads to the cores assigned to each MPI rank.</p> <p>Use the <code>--omp</code> flag with <code>affinity.mpi</code> to get more detailed information about OpenMP thread affinity. For example, four MPI ranks on one node with four cores and four OpenMP threads:</p> No OpenMP binding<pre><code>$ export OMP_NUM_THREADS=4\n$ srun -n4 -N1 -c4 --hint=nomultithread ./affinity.mpi --omp\naffinity test for 4 MPI ranks\nrank   0 @ nid001512: threads [0:3] -&gt; cores [  0:  3]\nrank   1 @ nid001512: threads [0:3] -&gt; cores [ 64: 67]\nrank   2 @ nid001512: threads [0:3] -&gt; cores [  4:  7]\nrank   3 @ nid001512: threads [0:3] -&gt; cores [ 68: 71]\n</code></pre> <p>The status <code>threads [0:3] -&gt; cores [  0:  3]</code> is shorthand \u201cthere are 4 OpenMP threads, and the OS can schedule them on cores 0, 1, 2 and 3\u201d.</p> <p>Allowing the OS to schedule threads is usually efficient, however to get the most you can try pinning threads to specific cores. The <code>OMP_PROC_BIND</code> environment variable can be used to tune how OpenMP sets thread affinity. For example, <code>OMO_PROC_BIND=true</code> will give each thread exclusive affinity with a core:</p> OMP_PROC_BIND=true<pre><code>$ export OMP_NUM_THREADS=4\n$ export OMP_PROC_BIND=true\n$ srun -n4 -N1 -c4 --hint=nomultithread ./affinity.mpi --omp\naffinity test for 4 MPI ranks\nrank   0 @ nid001512\n  thread 0 -&gt; core   0\n  thread 1 -&gt; core   1\n  thread 2 -&gt; core   2\n  thread 3 -&gt; core   3\nrank   1 @ nid001512\n  thread 0 -&gt; core  64\n  thread 1 -&gt; core  65\n  thread 2 -&gt; core  66\n  thread 3 -&gt; core  67\nrank   2 @ nid001512\n  thread 0 -&gt; core   4\n  thread 1 -&gt; core   5\n  thread 2 -&gt; core   6\n  thread 3 -&gt; core   7\nrank   3 @ nid001512\n  thread 0 -&gt; core  68\n  thread 1 -&gt; core  69\n  thread 2 -&gt; core  70\n  thread 3 -&gt; core  71\n</code></pre> <p>Note</p> <p>There are many OpenMP variables that can be used to fine tune affinity. See the OpenMP documentation for more information.</p> <p>Warning</p> <p>The <code>OMP_*</code> environment variables only affect thread affinity of applications that use OpenMP for thread-level parallelism. Other threading runtimes will be configured differently, and the <code>affinity.mpi</code> tool will only be able to show the set of cores assigned to the rank.</p> <p></p>"},{"location":"running/slurm/#node-over-subscription","title":"Node over-subscription","text":"<p>The nodes on Alps provide a lot of resources, particularly the GPU nodes that have 4 GPUs. For workflows and use cases with tasks that require only a subset of these resources, for example a simulation that only needs one GPU, allocating a whole node to run one task is a waste of resources.</p> <p>Example</p> <p>A workflow that runs a single GROMACS simulation, that uses one GPU.</p> <ul> <li>The optimal use of resources would allocate one quarter of a node, and allow other jobs to access the other three GPUs.</li> </ul> <p>A workflow that runs 100 independent GROMACS simulations, where each simulation requires two GPUs.</p> <ul> <li>The optimal use of resources would allocate 50 nodes, with two simulations run on each node.</li> </ul> <p></p>"},{"location":"running/slurm/#node-sharing","title":"Node sharing","text":"<p>Under-construction</p> <p>Node sharing, whereby jobs can request part of the resources on a node, and multiple jobs can run on a node (possibly from different users) is not currently available on Alps clusters.</p> <p>CSCS will support this feature on some Alps clusters in the near-medium future.</p> <p></p>"},{"location":"running/slurm/#running-more-than-one-job-step-per-node","title":"Running more than one job step per node","text":"<p>Running multiple job steps in parallel on the same allocated set of nodes can improve resource utilization by taking advantage of all the available CPUs, GPUs, or memory within a single job allocation.</p> <p>The approach is to:</p> <ol> <li>first allocate all the resources on each node to the job;</li> <li>then subdivide those resources at each invocation of srun.</li> </ol> <p>If Slurm believes that a request for resources (cores, GPUs, memory) overlaps with what another step has already allocated, it will defer the execution until the resources are relinquished. This must be avoided.</p> <p>First ensure that all resources are allocated to the whole job with the following preamble:</p> Slurm preamble on a GH200 node<pre><code>#!/usr/bin/env bash\n#SBATCH --exclusive --mem=450G\n</code></pre> <ul> <li><code>--exclusive</code> allocates all the CPUs and GPUs exclusively to this job;</li> <li><code>--mem=450G</code> most of allowable memory (there are 4 Grace CPUs with ~120 GB of memory on the node)</li> </ul> <p>Note</p> <p><code>--mem=0</code> can generally be used to allocate all memory on the node but the Slurm configuration on clariden doesn\u2019t allow this.</p> <p>Next, launch your applications using <code>srun</code>, carefully subdividing resources for each job step. The <code>--exclusive</code> flag must be used again, but note that its meaning differs in the context of <code>srun</code>. Here, <code>--exclusive</code> ensures that only the resources explicitly requested for a given job step are reserved and allocated to it. Without this flag, Slurm reserves all resources for the job step, even if it only allocates a subset\u2014effectively blocking further parallel <code>srun</code> invocations from accessing unrequested but needed resources.</p> <p>Be sure to background each <code>srun</code> command with <code>&amp;</code>, so that subsequent job steps start immediately without waiting for previous ones to finish. A final <code>wait</code> command ensures that your submission script does not exit until all job steps complete.</p> <p>Slurm will automatically set <code>CUDA_VISIBLE_DEVICES</code> for each <code>srun</code> call, restricting GPU access to only the devices assigned to that job step.</p> single nodemulti-node <p>Three jobs on one node</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --exclusive --mem=450G\n#SBATCH -N1\n\nCMD=\"echo \\$(date) \\$(hostname) JobStep:\\${SLURM_STEP_ID} ProcID:\\${SLURM_PROCID} CUDA_VISIBLE_DEVICES=\\${CUDA_VISIBLE_DEVICES}; sleep 5\"\nsrun -N1 --ntasks-per-node=1 --exclusive --gpus-per-task=2 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\nsrun -N1 --ntasks-per-node=1 --exclusive --gpus-per-task=1 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\nsrun -N1 --ntasks-per-node=1 --exclusive --gpus-per-task=1 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\n\nwait\n</code></pre> <p>Output (exact output will vary): <pre><code>$ cat out-537506.*.log\nTue Jul 1 11:40:46 CEST 2025 nid007104 JobStep:0 ProcID:0 CUDA_VISIBLE_DEVICES=0\nTue Jul 1 11:40:46 CEST 2025 nid007104 JobStep:1 ProcID:0 CUDA_VISIBLE_DEVICES=1\nTue Jul 1 11:40:46 CEST 2025 nid007104 JobStep:2 ProcID:0 CUDA_VISIBLE_DEVICES=2,3\n</code></pre></p> <p>Three jobs on two nodes</p> <pre><code>#!/usr/bin/env bash\n#SBATCH --exclusive --mem=450G\n#SBATCH -N2\n\nCMD=\"echo \\$(date) \\$(hostname) JobStep:\\${SLURM_STEP_ID} ProcID:\\${SLURM_PROCID} CUDA_VISIBLE_DEVICES=\\${CUDA_VISIBLE_DEVICES}; sleep 5\"\nsrun -N2 --ntasks-per-node=2 --exclusive --gpus-per-task=1 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\nsrun -N2 --ntasks-per-node=1 --exclusive --gpus-per-task=1 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\nsrun -N2 --ntasks-per-node=1 --exclusive --gpus-per-task=1 --cpus-per-gpu=5 --mem=50G --output \"out-%J.log\"  bash -c \"${CMD}\" &amp;\n\nwait\n</code></pre> <p>Output (exact output will vary): <pre><code>$ cat out-537539.*.log\nTue Jul 1 12:02:01 CEST 2025 nid005085 JobStep:0 ProcID:2 CUDA_VISIBLE_DEVICES=0\nTue Jul 1 12:02:01 CEST 2025 nid005085 JobStep:0 ProcID:3 CUDA_VISIBLE_DEVICES=1\nTue Jul 1 12:02:01 CEST 2025 nid005080 JobStep:0 ProcID:0 CUDA_VISIBLE_DEVICES=0\nTue Jul 1 12:02:01 CEST 2025 nid005080 JobStep:0 ProcID:1 CUDA_VISIBLE_DEVICES=1\nTue Jul 1 12:02:01 CEST 2025 nid005085 JobStep:1 ProcID:1 CUDA_VISIBLE_DEVICES=2\nTue Jul 1 12:02:01 CEST 2025 nid005080 JobStep:1 ProcID:0 CUDA_VISIBLE_DEVICES=2\nTue Jul 1 12:02:01 CEST 2025 nid005085 JobStep:2 ProcID:1 CUDA_VISIBLE_DEVICES=3\nTue Jul 1 12:02:01 CEST 2025 nid005080 JobStep:2 ProcID:0 CUDA_VISIBLE_DEVICES=3\n</code></pre></p>"},{"location":"services/","title":"Services","text":"<ul> <li> <p> CI/CD</p> <p>Configure CI/CD on Alps for your GitHub, GitLab and Bitbucket projects.</p> <p> CI/CD</p> </li> <li> <p> Developer Portal</p> <p>Enables CSCS users to create and manage subscriptions to an API.</p> <p> Developer Portal</p> </li> <li> <p> Kubernetes</p> <p>Kubernetes platform for automating deployment, scaling, and management of containerized applications.</p> <p> Kubernetes</p> </li> </ul>"},{"location":"services/cicd/","title":"CI/CD","text":""},{"location":"services/cicd/#continuous-integration-continuous-deployment-cicd","title":"Continuous Integration / Continuous Deployment (CI/CD)","text":"<p>External references</p> <p>It is helpful to consult the GitLab CI yaml reference documentation and the predefined pipeline variables reference.</p> <p>Webinar video</p> <p>Watch the latest webinar video for an overview of CI/CD and its features</p> <p></p>"},{"location":"services/cicd/#introduction-containerized-cicd","title":"Introduction containerized CI/CD","text":"<p>Containerized CI/CD allows you to build containers and run them at scale on CSCS systems. The basic idea is that you provide a Dockerfile with build instructions and run the newly created container. Most of the boilerplate work is being taken care by the CI implementation such that you can concentrate on providing build instructions and testing. The important information is provided to you from the CI side for the configuration of your repository.</p> <p>We support any git provider that supports webhooks. This includes GitHub, GitLab and Bitbucket. A typical pipeline consists of at least one build job and one test job. The build job makes sure that a new container with your most recent code changes is built. The test step uses the new container as part of an MPI job; e.g., it can run your tests on multiple nodes with GPU support.</p> <p>Building your software inside a container requires a Dockerfile and a name for the container in the registry where the container will be stored. Testing your software then requires the commands that must be executed to run the tests. No explicit container spawning is required (and also not possible). Your test jobs need to specify the number of nodes and tasks required for the test and the test commands.</p> <p></p>"},{"location":"services/cicd/#tutorial-hello-world","title":"Tutorial Hello World","text":"<p>Hello World</p> <p>See the hello world example on GitHub for a project that demonstrates a containerized end to end CI/CD workflow.</p> <p>In this example we are using the containerized hello world repository. This is a sample Hello World CMake project. The application only echos <code>Hello from $HOSTNAME</code>, but this should demonstrate the idea of how to run a program on multiple nodes. The pipeline instructions are inside the file <code>ci/cscs.yml</code>. Let\u2019s walk through the pipeline bit by bit. <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n</code></pre></p> <p>This block includes a yaml file which contains definitions with default values to build and run containers. Have a look inside this file to see available building blocks. <pre><code>stages:\n  - build\n  - test\n</code></pre> Here we define two different stages, named <code>build</code> and <code>test</code>. The names can be chosen freely. <pre><code>variables:\n  PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/helloworld:$CI_COMMIT_SHORT_SHA\n</code></pre></p> <p>This block defines variables that will apply to all jobs. See CI variables.</p> <pre><code>build_job:\n  stage: build\n  extends: .container-builder-cscs-zen2\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.build\n</code></pre> <p>This adds a job named <code>build_job</code> to the stage <code>build</code>. This runner expects a Dockerfile as input, which is specified in the variable <code>DOCKERFILE</code>. The resulting container name is specified with the variable <code>PERSIST_IMAGE_NAME</code>, which has been defined already above, therefore it does not need to be explicitly mentioned in the <code>variables</code> block, again. Further refinements can be found at the reference documentation</p> <pre><code>test_job:\n  stage: test\n  extends: .container-runner-eiger-zen2\n  image: $PERSIST_IMAGE_NAME\n  script:\n    - /opt/helloworld/bin/hello\n  variables:\n    SLURM_JOB_NUM_NODES: 2\n    SLURM_NTASKS: 2\n</code></pre> <p>This block defines a test job. The job will be executed by the .container-runner-eiger-zen2.</p> <p>This runner will pull the image on the cluster Eiger and run the commands as specified in the <code>script</code> tag. In this example we are requesting 2 nodes with 1 task on each node, i.e. 2 tasks total. All Slurm environment variables are supported. The commands will be running inside the container specified by the <code>image</code> tag.</p> <p></p>"},{"location":"services/cicd/#ci-at-cscs","title":"CI at CSCS","text":""},{"location":"services/cicd/#enable-ci-for-your-project","title":"Enable CI for your project","text":"<p>While the procedure to enable CSCS CI for your repository consists of only a few steps outlined below, many of them require features in GitHub, GitLab or Bitbucket. The links in the text contain additional steps which may be needed. Some of those documents are non-trivial, especially if you do not have considerable background in the repository features. Plan sufficient time for the setup and contact a GitHub/GitLab/Bitbucket professional, if needed.</p> <ol> <li>Register your project with CSCS: The first step to use containerized CI/CD is to register your Git repository with CSCS. Please open a Service Desk ticket and include which repository should be registered, and who should own the registration (by default it would be the requester). Once your project has been registered you will be provided with a webhook-secret.</li> </ol> <p>Note</p> <p>CSCS staff members can directly register their project by clicking on register new project at the bottom right of the CI overview page.</p> <ol> <li> <p>Set up CI: Head to the CI overview page, login with your CSCS credentials, and go to the newly registered project.</p> </li> <li> <p>Add FirecREST tokens: Expand the <code>Admin config</code>, and follow the guide (click on the small black triangle next to Firecrest Consumer Key). Enter all fields for FirecREST, i.e.,</p> <ul> <li>Consumer Key</li> <li>Consumer Secret</li> <li>default Slurm account for job submission (what you normally provide in the <code>--account</code>/<code>-A</code> flag to Slurm)</li> </ul> <p>FirecREST credentials</p> <p>If you don\u2019t already know how to obtain FirecREST credentials, you can find more information on CSCS Developer Portal. You must subscribe your application to the FirecREST API of the platform to which you want to submit jobs, e.g. <code>FirecREST-HPC</code> for the HPC platform. If <code>v2</code> is available for your platform, then subscribe to <code>v2</code> of the API, otherwise <code>v1</code> will work too. It is not mandatory to subscribe to the <code>ciext-container-builder</code> API to use pure CI workflows (this API is only used for building manually container images).</p> </li> <li> <p>(Optional) Private project: If your Git repository is a private repository make sure to check the <code>Private repository</code> box and follow the instructions to add an SSH key to your Git repository.</p> </li> <li> <p>Add notification token: On the setup page you will also find the field <code>Notification token</code>. The token is live tested, and you will see a green checkmark when the token is valid and can be used by the CI. It is mandatory to add a token so that your Git repository will be notified about the status of the build jobs. You cannot save anything as long as the notification token is invalid. (Click on the small triangle to get further instructions)</p> </li> <li> <p>Add webhook: On the CI setup page you will find the <code>Setup webhook details</code> button (go to the CI overview, then the project, and there is a blue button with the text <code>Setup webhook details</code>). If you click on it you will see all the entries which have to be added to a new webhook in your Git repository. Follow the link given there to your repository, and add the webhook with the given entries.</p> </li> <li> <p>Default trusted users and default CI-enabled branches: Provide the default list of trusted users and CI-enabled branches. The global configuration will apply to all pipelines that do not overwrite it explicitly.</p> </li> <li> <p>Pipeline default: Your first pipeline has the name <code>default</code>. Click on <code>Pipeline default</code> to see the pipeline setup details. The name can be chosen freely but it cannot contain whitespace (a short descriptive name). Update the entry point, trusted users and CI-enabled branches.</p> </li> <li> <p>Submit your changes</p> </li> <li> <p>(Optional) Add other pipelines: Add other pipelines with a different entry point if you need more pipelines.</p> </li> <li> <p>Add entry point yaml files to Git repository: Commit the yaml entry point files to your repository. You should get notifications about the build status in your repository if everything is correct. See the Hello World Tutorial for a simple yaml-file.</p> </li> </ol>"},{"location":"services/cicd/#clarifications-and-pitfalls-to-the-above-mentioned-steps","title":"Clarifications and pitfalls to the above-mentioned steps","text":"<p>Info</p> <p>This section exemplifies on GitHub, but similar settings are available on GitLab and Bitbucket</p> <p>The <code>notification token</code> setup step is crucial, because this is the number one entrypoint for receiving initial feedback on any errors. You will not be able to save any changes on the CI setup page, as long as the notification token is invalid. The token is checked live, whether it can be used to do notifications.</p> <p>Notification tokens on GitHub can be setup using <code>Classic token</code> or <code>Fine-grained token</code>. We discourage the use of fine-grained tokens. Fine-grained tokens are unsupported, and come with many pitfalls. They can work, but must be enabled at the organization level by an admin, and must be created in the correct organization. You must choose the correct resource owner, i.e., the organization that the project belongs to. If the organization is not listed, then it has disabled fine-grained tokens at the organization level. It can only be enabled globally on an organization by an admin. As for the repository you can restrict it to only the repository that you want to notify with this token or all repositories. Even if you choose \u201cAll repositories\u201d, it is still restricted to the organization, and does not grant the access to any repository outside of the resource owner.</p> <p>Another crucial setup step is the correct webhook setup. The repository provider (GitHub, GitLab, Bitbucket) gives you the ability to see what happened, when the webhook event was sent. If the webhook was not setup correctly, you will receive an HTTP error for the webhook events. The error message can be found in the webhook event response. As an example, here is how you would find it on GitHub: Settings &gt; Webhooks &gt; <code>Edit</code> button of the webhook &gt; <code>Recent Deliveries</code> tab &gt; Choose a webhook event from the list &gt; <code>Response</code> tab &gt; Check for potential error message.</p> <p>A typical error is accepting to defaults of GitHub for new webhooks, where only <code>Push</code> events are being sent. When you forget to select <code>Send me everything</code>, then some events will not trigger pipelines. Double check your webhook settings.</p> <p></p>"},{"location":"services/cicd/#understanding-when-ci-is-triggered","title":"Understanding when CI is triggered","text":""},{"location":"services/cicd/#push-events","title":"Push events","text":"<ul> <li>Every pipeline can define its own list of CI-enabled branches</li> <li>If a pipeline does not define a list of CI-enabled branches, the global list will be used</li> <li>If you push changes to a branch every pipeline that has this branch in its list of CI-enabled branches will be triggered</li> <li>If the global list and all pipelines have an empty list of CI-enabled branches, then CI will never be triggered on push events</li> </ul>"},{"location":"services/cicd/#pull-requests-merge-requests","title":"Pull requests (Merge requests)","text":"<ul> <li>For simplicity we use PR to mean Pull Request, although some providers call it a Merge request. It is the same thing.</li> <li>Every pipeline can define its own list of trusted users.</li> <li>If a pipeline does not define a list of trusted users, the global list will be used.</li> <li>If a PR is opened/edited and targets a CI-enabled branch, and the source branch is not from a fork, then all pipelines will be started that have the target branch in its list of CI-enabled branches.</li> <li>If a PR is opened/edited and targets a CI-enabled branch, but the source branch is from a fork, then a pipeline will be automatically started if and only if the fork is from a user in the pipeline\u2019s trusted user list and the target branch is in the pipeline\u2019s CI-enabled branches.</li> </ul>"},{"location":"services/cicd/#cscs-ci-run-comment","title":"<code>cscs-ci run</code> comment","text":"<ul> <li>You have an open PR</li> <li>You want to trigger a specific pipeline</li> <li>Write a comment inside the PR with the text   <pre><code>cscs-ci run PIPELINE_NAME_1,PIPELINE_NAME_2\n</code></pre></li> <li>Special case: You have only one pipeline, then you can skip the pipeline names and write only the comment <code>cscs-ci run</code></li> <li>The pipeline will only be triggered, if the commenting user is in the pipeline\u2019s trusted users list.</li> <li>Only the first line of the comment will be evaluated, i.e. you can add context from line 2 onwards.</li> <li>The target branch is ignored, i.e. you can test a pipeline even if the target branch is not in the pipeline\u2019s CI-enabled branches.</li> <li>Advanced <code>cscs-ci</code> run command is possible to inject variables into the pipeline (exposed as environment variables)<ul> <li>Triggering a pipeline with additional variables   <pre><code>cscs-ci run PIPELINE_NAME;MY_VARIABLE=some_value;ANOTHER_VAR=other_value\n</code></pre>   This will trigger the pipeline PIPELINE_NAME, and in your jobs there will be the environment variables MY_VARIABLE and ANOTHER_VAR available.</li> <li>Disallowed characters for PIPELINE_NAME, variable name and variable value are the characters <code>,;=</code> (comma, semicolon, equal), because they serve as separators of the different components.</li> </ul> </li> </ul>"},{"location":"services/cicd/#api-call-triggering","title":"API call triggering","text":"<ul> <li>It is possible to trigger a pipeline via an API call</li> <li>Create a file <code>data.yaml</code>, with the content     data.yaml<pre><code>ref: main\npipeline: pipeline_name\nvariables:\n  MY_VARIABLE: some_value\n  ANOTHER_VAR: other_value\n</code></pre></li> <li>Send a POST request to the middleware (replace <code>repository_id</code> and <code>webhook_secret</code>)     <pre><code>$ curl -X POST -u 'repository_id:webhook_secret' --data-binary @data.yaml https://cicd-ext-mw.cscs.ch/ci/pipeline/trigger\n</code></pre></li> <li>To trigger a pull-request use <code>ref: 'pr:&lt;pr-number&gt;'</code></li> <li>To trigger a tag use <code>ref: 'tag:&lt;tag-name&gt;'</code></li> <li>To trigger on a specific commit SHA use <code>ref: 'sha:&lt;commit-sha&gt;'</code></li> </ul>"},{"location":"services/cicd/#understanding-the-underlying-workflow","title":"Understanding the underlying workflow","text":"<p>Typical users do not need to know the underlying workflow behind the scenes, so you can stop reading here. However, it might put the above-mentioned steps into perspective. It also can give you background for inquiring if and when something in the procedure does not go as expected.</p>"},{"location":"services/cicd/#workflow-exemplified-on-a-github-repository","title":"Workflow (exemplified on a GitHub repository)","text":"<ol> <li>(Prerequisite) Repository in GitHub will have a webhook set up, with the setup details from the CI setup page</li> <li>You push some change to the GitHub repository</li> <li>GitHub sends a (push) webhook event to <code>cicd-ext-mw.cscs.ch</code> (CI middleware)</li> <li>CI middleware fetches your repository from GitHub and pushes a \u201cmirror\u201d to GitLab</li> <li>GitLab receives the change in the \u201cmirror\u201d repository and a pipeline is triggered (i.e. it uses the CI yaml as entry point)</li> <li>If the repository uses git submodules, <code>GIT_SUBMODULE_STRATEGY: recursive</code> has to be specified (see GitLab documentation)</li> <li>The container-builder, which has as input a Dockerfile (specified in the variable <code>DOCKERFILE</code>), will take this Dockerfile and execute something similar to <code>docker build -f $DOCKERFILE .</code>, where the build context is the whole (recursively) cloned repository</li> </ol>"},{"location":"services/cicd/#containerized-ci-best-practices","title":"Containerized CI - best practices","text":""},{"location":"services/cicd/#multi-architecture-images","title":"Multi-architecture images","text":"<p>With the introduction of Grace-Hopper nodes, we have now <code>aarch64</code> and <code>x86_64</code> machines. This implies that the container images should be built for the correct architecture. This can be achieved by the following example <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n\nstages:\n  - build\n  - make_multiarch\n  - run\n\n.build:\n  stage: build\n  variables:\n    DOCKERFILE: path/to/my_dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/${ARCH}/my_image_name:${CI_COMMIT_SHORT_SHA}\nbuild aarch64:\n  extends: [.container-builder-cscs-gh200, .build]\nbuild x86_64:\n  extends: [.container-builder-cscs-zen2, .build]\n\nmake multiarch:\n  extends: .make-multiarch-image\n  stage: make_multiarch\n  variables:\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/my_multiarch_image:${CI_COMMIT_SHORT_SHA}\n    PERSIST_IMAGE_NAME_AARCH64: $CSCS_REGISTRY_PATH/aarch64/my_image_name:${CI_COMMIT_SHORT_SHA}\n    PERSIST_IMAGE_NAME_X86_64: $CSCS_REGISTRY_PATH/x86_64/my_image_name:${CI_COMMIT_SHORT_SHA}\n\n.run:\n  stage: run\n  image: $CSCS_REGISTRY_PATH/my_multiarch_image:${CI_COMMIT_SHORT_SHA}\n  script:\n    - uname -a\nrun aarch64:\n  extends: [.container-runner-daint-gh200, .run]\nrun x86_64:\n  extends: [.container-runner-eiger-mc, .run]\n</code></pre></p> <p>We first create two container images which have different names. Then we combine these two names to a single name, with both architectures. Finally in the run step we use the multi-architecture image, where the container runtime will pull the correct architecture.</p> <p>It is not mandatory to combine the container images to a multi-architecture image, i.e. a CI setup which consistently uses the correct architecture specific paths can work. A multi-architecture image is convenient when you plan to distribute it to other users.</p>"},{"location":"services/cicd/#dependency-management","title":"Dependency management","text":""},{"location":"services/cicd/#problem","title":"Problem","text":"<p>A common observation is that your software has many dependencies that are more or less static, i.e. they can change but do so very rarely. A common pattern one can observe to work around rebuilding base images unnecessarily is a multi-stage CI setup</p> <ol> <li>Build (rarely but manually) a base container with all static dependencies and push it to a public container registry</li> <li>Use the base container and build the software container</li> <li>Test the newly created software container</li> <li>Deploy the software container</li> </ol> <p>This works fine but has the drawback that one has to do a manual step whenever the dependencies change, e.g. when one wants to upgrade to new versions of the dependencies. Another drawback of this is that it allows to keep the recipe of the base container outside of the repository, which makes it harder to reproduce results, especially when colleagues want to reproduce a build.</p>"},{"location":"services/cicd/#solution","title":"Solution","text":"<p>A common solution to this problem is that you have a multi stage setup. Your repository should have (at least) two Dockerfiles, let us call them <code>Dockerfile.base</code> and <code>Dockerfile</code>.</p> <ul> <li><code>Dockerfile.base</code>: This dockerfile contains the recipe to build your base-container, it normally derives <code>FROM</code> a very basic container, e.g. <code>docker.io/ubuntu:24.04</code> or CSCS spack base containers. Let us call the container image that is built using this recipe <code>BASE_IMG</code>.</li> <li><code>Dockerfile</code>: This Dockerfile contains the recipe to build your software-container. It must start with <code>FROM $BASE_IMG</code>.</li> </ul> <p>Note</p> <p>Have a look at the Spack based images section, to manage software installed in base containers via Spack</p> <p>The <code>.container-builder-cscs-*</code> blocks can be used to solve this problem. The runner supports the variable <code>CSCS_REBUILD_POLICY</code>, which by default is set to <code>if-not-exists</code>.</p> <p>This means that the runner will check the remote registry if the container image specified in <code>PERSIST_IMAGE_NAME</code> exists. A new container image is built only if it does not exist yet. Note: In case you have one build job, <code>PERSIST_IMAGE_NAME</code> can be specified in the <code>variables:</code> field of this build job or as a global variable, like in the Hello World example. In case you have multiple build jobs and you specify the <code>PERSIST_IMAGE_NAME</code> variable per build job, you need to specify the exact name of the image to be used in the <code>image</code> field of the test job.</p> <p>CI files would look in the simplest case like this:</p> CI YAMLDockerfile base imageDockerfile application ci/cscs.yml<pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n\nstages:\n  - build_base\n  - build\n  - test\n\nbuild base:\n  extends: .container-builder-cscs-zen2\n  stage: build_base\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/base/my_base_container:1.0\n    CSCS_REBUILD_POLICY: if-not-exists # default anyway, only here for verbosity\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$CSCS_REGISTRY_PATH/base/my_base_container:1.0\"]'\n\ntest software single node:\n  extends: .container-runner-daint-gpu\n  image: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n  script:\n    - ./test_suite_1.sh\n    - ./test_suite_2.sh\n  variables:\n    SLURM_JOB_NUM_NODES: 1\n\ntest software multi:\n  extends: .container-runner-daint-gpu\n  image: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n  script:\n    - ./test_suite_1.sh\n    - ./test_suite_2.sh\n  variables:\n    SLURM_JOB_NUM_NODES: 4\n</code></pre> ci/docker/Dockerfile.base<pre><code>FROM docker.io/finkandreas/spack:0.19.2-cuda11.7.1-ubuntu22.04\n\nARG NUM_PROCS\n\nRUN spack-install-helper daint-gpu \\\n    petsc \\\n    trilinos\n</code></pre> ci/docker/Dockerfile<pre><code>ARG BASE_IMG\nFROM $BASE_IMG\n\nARG NUM_PROCS\n\nRUN mkdir /build &amp;&amp; cd /build &amp;&amp; cmake /sourcecode &amp;&amp; make -j$NUM_PROCS\n</code></pre> <p>A setup like this would run the very first time and build the container image <code>$CSCS_REGISTRY_PATH/base/my_base_container:1.0</code>, followed by the job that builds the container image <code>$CSCS_REGISTRY_PATH/software/my_software:1.0</code>. The next time CI is triggered the <code>.container-builder-cscs-zen2</code> would check the remote repository if the target tag (<code>PERSIST_IMAGE_NAME</code>) exists, and only build a new container image if it does not exist yet. Since the tag for the job <code>build base</code> is static, i.e. it is the same for every run of CI, it would build the first time it is running, but not for subsequent runs. In contrast to this is the job <code>build software</code>: Here the tag changes with every CI run, since the variable <code>CI_COMMIT_SHORT_SHA</code> is different for every run.</p>"},{"location":"services/cicd/#manual-dependency-update","title":"Manual dependency update","text":"<p>At some point you realise that you have to update some of the dependencies. You can use a manual update process to update your base-container, where you ensure that you update all necessary image tags. In our example, this means updating in <code>ci/cscs.yml</code> all occurences of <code>$CSCS_REGISTRY_PATH/base/my_base_container:1.0</code> to <code>$CSCS_REGISTRY_PATH/base/my_base_container:2.0</code> (or any other versioning scheme - for all that matters is that the full name must change). Of course something in <code>Dockerfile.base</code> should change too, otherwise you are building the same artifact, with just a different name.</p>"},{"location":"services/cicd/#dynamic-dependency-update","title":"Dynamic dependency update","text":"<p>While manually updating image tags works fine, it has the drawback that it is error-prone. Take for example the situation where you update the tag in <code>build base</code>, but forget to change it in <code>build software</code>. Your pipeline would still run fine, because the dependency of <code>build software</code> exists. Since there is no explicit error for the inconsistencies it is hard to find the error.</p> <p>Therefore, there is also the possibility to have a dynamic way of naming your container images. The idea is the same, i.e. we build first a base-container, and use this base-container to build our software-container.</p> <p>The <code>build base</code> and <code>build software</code> jobs would look similar to this: <pre><code>build base:\n  extends: .container-builder-cscs-zen2\n  stage: build_base\n  before_script:\n    - DOCKER_TAG=`cat ci/docker/Dockerfile.base | sha256sum - | head -c 16`\n    - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_PATH/base/my_base_image:$DOCKER_TAG\n    - echo \"BASE_IMAGE=$PERSIST_IMAGE_NAME\" &gt; build.env\n  artifacts:\n    reports:\n      dotenv: build.env\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base # overwrite with the real path of the Dockerfile\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$BASE_IMAGE\"]'\n</code></pre></p> <p>Let us walk through the changes in the <code>build base</code> job:</p> <ul> <li><code>DOCKER_TAG</code> is computed at runtime by the sha256sum of the <code>Dockerfile.base</code>, i.e. it would change, when you change the content of <code>Dockerfile.base</code> (we keep only the first 16 characters, this is random enough to guarantee that we have a unique name).</li> <li>We export <code>PERSIST_IMAGE_NAME</code> to the dynamic name with <code>DOCKER_TAG</code>.</li> <li>We write the dynamic name to the file <code>build.env</code></li> <li>We tell the CI system to keep the <code>build.env</code> as an artifact (see here the documentation of this)</li> </ul> <p>Note: The dotenv artifacts of a specific job for public projects is available at <code>https://gitlab.com/cscs-ci/ci-testing/webhook-ci/mirrors/&lt;project_id&gt;/&lt;pipeline_id&gt;/-/jobs/&lt;job_id&gt;/artifacts/download?file_type=dotenv</code>.</p> <p>Now let us look at the changes in the <code>build software</code> job:</p> <ul> <li><code>DOCKER_BUILD_ARGS</code> is now using <code>$BASE_IMAGE</code>. This variable exists, because we transferred the information via a <code>dotenv</code> artifact from <code>build base</code> to this job.</li> </ul> <p>In this example the names <code>BASE_IMG</code> and <code>BASE_IMAGE</code> are chosen to be different, for clarification where the different variables are set and used. Feel free to use the same names for consistent naming. The default behaviour is to import all artifacts from all previous jobs. If you want only specific artifacts in your job, you should have a look at dependencies.</p> <p>There is also a building block in the templates, name <code>.dynamic-image-name</code>, which you can use to get rid for most of the boilerplate. It is important to note that this building block will export the dynamic name under the hardcoded name <code>BASE_IMAGE</code> in the <code>dotenv</code> file. The variable <code>DOCKER_TAG</code>, containing the tag of the image, is also exported in the <code>dotenv</code> file. The jobs would look something like this: <pre><code>build base:\n  extends: [.container-builder-cscs-zen2, .dynamic-image-name]\n  stage: build_base\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/base/my_base_image\n    WATCH_FILECHANGES: 'ci/docker/Dockerfile.base'\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$BASE_IMAGE\"]'\n</code></pre></p> <p><code>build base</code> is using additionally the building block <code>.dynamic-image-name</code>, while <code>build software</code> is unchanged. Have a look at the definition of the block <code>.dynamic-image-name</code> in the file .ci-ext.yml for further notes.</p> <p>GT4Py example</p> <p>An example using <code>.dynamic-image-name</code> in action can be found in the gt4py repository.</p>"},{"location":"services/cicd/#image-cleanup","title":"Image cleanup","text":"<p>Images pushed to CSCS_REGISTRY_PATH are cleaned daily according to the following rules:</p> <ul> <li>No deletion if total storage usage &lt; 300GB</li> <li>No deletion of images newer than 30 days</li> <li>First cleanup excluding folders <code>base</code>, <code>baseimg</code>, <code>baseimage</code>, <code>deploy</code>, <code>deployment</code><ul> <li>Delete images that have been unused for more than 30 days (usage means that the image has been downloaded)</li> </ul> </li> <li>Second cleanup if storage usage is still &gt; 300GB<ul> <li>Delete images in the above mentioned excluded folders if the image has been unused for more than 365 days</li> </ul> </li> </ul>"},{"location":"services/cicd/#third-party-registries","title":"Third party registries","text":"<p>While it is recommended to work with the CSCS provided registry at CSCS_REGISTRY_PATH, due to fastest network connection, it is also possible to work with third party registries like dockerhub.com or quay.io. When you work with third party registries, then you have to provide login credentials for the CI jobs.</p> <p>There are two possible ways to push images to third party registries (it is assumed that you have stored the variable <code>DOCKERHUB_ACCESS_TOKEN</code> at the CI setup page):</p> <p>The first approach will push both, CSCS registry and third party registry <code>dockerhub.com</code> <pre><code>my_job:\n  extends: .container-builder-cscs-zen2\n  stage: my_stage\n  variables:\n    DOCKERFILE: path/to/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/my_image:${CI_COMMIT_SHORT_SHA}\n    SECONDARY_IMAGE_NAME: docker.io/your_dockerhub_username/my_image:${CI_COMMIT_SHORT_SHA}\n    SECONDARY_IMAGE_USERNAME: your_dockerhub_username\n    SECONDARY_IMAGE_PASSWORD: $DOCKERHUB_ACCESS_TOKEN\n</code></pre></p> <p>The second approach will only push to the third party registry <code>dockerhub.com</code>: <pre><code>my_job:\n  extends: .container-builder-cscs-zen2\n  stage: my_stage\n  variables:\n    DOCKERFILE: path/to/Dockerfile\n    PERSIST_IMAGE_NAME: docker.io/your_dockerhub_username/my_image:${CI_COMMIT_SHORT_SHA}\n    CUSTOM_REGISTRY_USERNAME: your_dockerhub_username\n    CUSTOM_REGISTRY_PASSWORD: $DOCKERHUB_ACCESS_TOKEN\n</code></pre></p>"},{"location":"services/cicd/#clone-image-from-cscs-registry","title":"Clone image from CSCS registry","text":"<p>CI has access to the CSCS registry while a job is running. However you cannot download images from the CSCS registry directly, because you do not have valid credentials to access the images. If the image has not been pushed to a third party registry like hinted above, and one still wants to inspect the image, one can instruct manually CI to copy the image to a third party registry. Create a yaml file with the content copy_image.yaml<pre><code>username: &lt;repository-id&gt;\npassword: &lt;webhook-secret&gt;\nfrom_image: $CSCS_REGISTRY_PATH/my_cool_image:1.0\nto_image: docker.io/your_dockerhub_username/my_cool_image_cloned:latest\nregistry_credentials:\n  username: your_registry_username\n  password: your_registry_password\n</code></pre></p> <ul> <li><code>from_image</code>: The image that you want to copy out of CSCS registry. It must begin with <code>$CSCS_REGISTRY_PATH</code></li> <li><code>to_image</code>: The path where the image should be copied</li> <li><code>registry_credentials</code>: The credentials which allow to login to the registry specified in <code>to_image</code>. Many registries allow you to create access tokens, which should be preferred to your password.</li> </ul> <p>To trigger the copy POST a request to https://cicd-ext-mw.cscs.ch/ci/image/clone (e.g. with <code>curl</code>) <pre><code>$ curl --data-binary @copy_image.yaml https://cicd-ext-mw.cscs.ch/ci/image/clone\n</code></pre> The call might take some time, depending on image size.</p>"},{"location":"services/cicd/#spack-based-images","title":"Spack based images","text":""},{"location":"services/cicd/#introduction","title":"Introduction","text":"<p>Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. It was designed for administrators in large supercomputing centres, where many users and application teams share common installations of software on clusters. It can also be used by users to install software environments exactly for their needs.</p> <p>We provide Docker images with preinstalled Spack, its configuration for the hardware available at CSCS, and helper scripts that simplify using Spack in a Dockerfile.</p>"},{"location":"services/cicd/#install-helper-script","title":"Install helper script","text":"<p>In the image, we provide a <code>spack-install-helper</code> script that helps build a list of packages for a desired architecture. The script can be used as follows: <pre><code>$ spack-install-helper --target &lt;target-arch&gt; [--add-repo &lt;repo&gt;] [--only-dependencies &lt;spec&gt;] &lt;spec&gt; [&lt;spec&gt;...]\n</code></pre></p> <ul> <li><code>--target</code> (mandatory) specifying the target architecture.   Possible values are <code>alps-zen2</code>, <code>alps-a100</code>, <code>alps-gh200</code>, <code>alps-mi200</code>, <code>alps-mi300a</code></li> <li><code>--add-repo</code> (optional) adds an additional custom Spack repository`</li> <li><code>--only-dependencies</code> (optional) install only dependencies of the spec.   It is useful if you want to install a package manually, e.g. for debugging purposes, or together with <code>--add-repo</code> for developing spack\u2019s <code>package.py</code></li> <li><code>spec</code> are any specs that can be passed to the <code>spack install</code> command</li> </ul>"},{"location":"services/cicd/#building-docker-images","title":"Building Docker images","text":"<p>It is good practice to keep Docker images as small as possible, with only the software needed and nothing more. To support this philosophy, we create our image in a multistage build. In the first stage, Spack is used to install the software stack we need. In the second stage, the software installed in the first stage is copied (without build dependencies and Spack). After that, we can add to the image anything we need. We can, for example, build our software, prepare tests, \u2026</p>"},{"location":"services/cicd/#docker-images-naming-scheme","title":"Docker images naming scheme","text":"<p>We provide two different images that can be used in the first and second stages of the multistage build. The image <code>spack-build</code> is used in the first stage and has Spack installed. The image <code>spack-runtime</code>  is the same, but without spack, only with scripts that make the software installed with spack available.</p> <p>Both images are available with different versions of installed software, this is encoded in the docker tag. The tag of the <code>spack-build</code> image has the following scheme <code>spack&lt;version&gt;-&lt;os&gt;&lt;version&gt;-&lt;arch&gt;[version]</code>, e.g. <code>spack-build:spack0.21.0-ubuntu22.04-cuda12.4.1</code> is an image based on <code>ubuntu-22.04</code> with <code>cuda-12.4.1</code> and <code>spack-0.21.0</code>. The tag of the <code>spack-runtime</code> image has the same scheme, but without <code>spack&lt;version&gt;-</code>, as spack is not installed in this image, e.g. <code>spack-runtime:ubuntu22.04-cuda12.4.1</code>. It is strongly recommended to always use both images with the same OS and arch for the first and the second stage.</p> <p>We provide images based on <code>Ubuntu 22.04 LTS</code>  for three different architectures:</p> <ul> <li>CPU (x86_64)</li> <li>CUDA (<code>x86_64+A100</code> and <code>arm64+GH200</code>)</li> <li>ROCm (<code>x86_64+MI250</code>)</li> </ul>"},{"location":"services/cicd/#docker-registry","title":"Docker registry","text":"<p>We provide these images in two docker registries:</p> <ul> <li>JFrog hosted at CSCS (jfrog.svc.cscs.ch)<ul> <li><code>FROM $CSCS_REGISTRY/docker-ci-ext/base-containers/public/spack-build:&lt;tag&gt;</code></li> <li><code>FROM $CSCS_REGISTRY/docker-ci-ext/base-containers/public/spack-runtime:&lt;tag&gt;</code></li> <li>Available only on the CSCS network</li> <li>Recommended for CI/CD workflows at CSCS</li> </ul> </li> <li>GitHub Container Registry<ul> <li><code>FROM ghcr.io/eth-cscs/docker-ci-ext/base-containers/spack-build:&lt;tag&gt;</code></li> <li><code>FROM ghcr.io/eth-cscs/docker-ci-ext/base-containers/spack-runtime:&lt;tag&gt;</code></li> <li>Available from everywhere</li> <li>Recommended for manual workflows</li> </ul> </li> </ul>"},{"location":"services/cicd/#example-dockerfile","title":"Example Dockerfile","text":"<p>Use this Dockerfile template. Adjust the <code>spack-install-helper</code> command as needed, especially the target architecture and list of packages. Add any commands after <code>fix_spack_install</code> or drop all of them if you need only the software installed by spack.</p> Dockerfile<pre><code># use spack to install the software stack\nFROM $CSCS_REGISTRY/docker-ci-ext/base-containers/public/spack-build:spack0.21.0-ubuntu22.04-cuda12.4.1 as builder\n\n# number or processes used for building the spack software stack\nARG NUM_PROCS\n\nRUN spack-install-helper --target alps-gh200 \\\n    \"git\" \"cmake\" \"valgrind\" \"python@3.11\" \"vim +python +perl +lua\"\n\n# end of builder container, now we are ready to copy necessary files\n\n# copy only relevant parts to the final container\nFROM $CSCS_REGISTRY/docker-ci-ext/base-containers/public/spack-runtime:ubuntu22.04-cuda12.4.1\n\n# it is important to keep the paths, otherwise your installation is broken\n# all these paths are created with the above `spack-install-helper` invocation\nCOPY --from=builder /opt/spack-environment /opt/spack-environment\nCOPY --from=builder /opt/software /opt/software\nCOPY --from=builder /opt/._view /opt/._view\nCOPY --from=builder /etc/profile.d/z10_spack_environment.sh /etc/profile.d/z10_spack_environment.sh\n\n# Some boilerplate to get all paths correctly - fix_spack_install is part of the base image\n# and makes sure that all important things are being correctly setup\nRUN fix_spack_install\n\n# Finally install software that is needed, e.g. compilers\n# It is also possible to build compilers via spack and let all dependencies be handled by spack\nRUN apt-get -yqq update &amp;&amp; apt-get -yqq upgrade \\\n &amp;&amp; apt-get -yqq install build-essential gfortran \\\n &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre>"},{"location":"services/cicd/#manual-container-build","title":"Manual container build","text":"<p>It is possible to use CI\u2019s mechanisms to build a container image manually providing a <code>Dockerfile</code>, or even a full build context with a <code>Dockerfile</code>. For reproducibility one should aim to use a code versioning system like <code>git</code>, but for debugging a manual API call might not pollute the code history so much.</p> <p>To use the API endpoint you need to create API credentials (one time setup) using the developer portal. Please follow this guide. To be able to use manual API container builds you must subscribe your application to the <code>ciext-container-builder</code> API.</p> <p>After your application is created the tab <code>APIs</code> will appear at the top of the developer portal, which allows you to inspect the API for <code>ciext-container-builder</code>. The section <code>Documents</code> is very helpful as it contains the endpoint documentation for <code>/container/build POST</code>. Refer to the documentation there for all parameters.</p> <p>The endpoint can work in two different modes:</p> <ol> <li>Send just a Dockerfile</li> <li>Send a full build context as a tar.gz-archive and tell the API where the Dockerfile inside the tarball is.</li> </ol> <p>Create access token</p> <p>To send any request to the API endpoint, we first need to create an access token, which can be done with the Consumer Key and Consumer secret. <pre><code>$ ACCESS_TOKEN=\"$(curl  -u &lt;your-consumer-key&gt;:&lt;your-consumer-secret&gt; --silent -X POST https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token -d \"grant_type=client_credentials\" | jq -r '.access_token')\"\n</code></pre> The token is stored in the variable ACCESS_TOKEN. This token has only a short validity, so you need to create a fresh access token, whenever the current one becomes invalid (about 5 minutes validity).</p> <p>Build from Dockerfile</p> <p><pre><code>$ curl -H \"Authorization: Bearer $ACCESS_TOKEN\" --data-binary @path/to/Dockerfile \"https://api.cscs.ch/ciext/v1/container/build?arch=x86_64\"\n</code></pre> It is mandatory to specify for which architecture you want to build the container. Valid choices are:</p> <ul> <li><code>x86_64</code> - Correct for all nodes that are not Grace-Hopper</li> <li><code>aarch64</code> - ARM architecture - Correct for Grace-Hopper</li> </ul> <p>The API call above sends the Dockerfile to the server, and the server will reply with a link, where you can see the build log. The final container image will be pushed to JFrog, a CSCS internal container registry. Once the container image is built, it can be pulled from any CSCS machine.</p> <p>If you want to push the image to your Docker Hub account, you need to create a Docker Hub access token with write permissions, and then use the API call (similarly for other OCI registry providers) <pre><code>$ curl -H \"Authorization: Bearer $ACCESS_TOKEN\" -H \"X-Registry-Username &lt;your-dockerhub-username&gt;\" -H \"X-Registry-Password: &lt;your-dockerhub-token&gt;\" --data-binary @path/to/Dockerfile \"https://api.cscs.ch/ciext/v1/container/build?arch=x86_64&amp;image=docker.io/&lt;your-dockerhub-username&gt;/my_image_name:latest\"\n</code></pre></p> <p>Build with code</p> <p>If you are using <code>COPY</code> or <code>ADD</code> statements in your Dockerfile, you will need to send the build context too. To send a full archive the easiest is via the API call <pre><code>$ tar -C path/to/build-context -czf - . | curl -H \"Authorization: Bearer $ACCESS_TOKEN\" --data-binary @- \"https://api.cscs.ch/ciext/v1/container/build?arch=x86_64&amp;dockerfile=relative/path/to/Dockerfile\"\n</code></pre> This is similar to <pre><code>$ docker build -f relative/path/to/Dockerfile .\n</code></pre></p>"},{"location":"services/cicd/#cscs-ci-specifics","title":"CSCS CI specifics","text":""},{"location":"services/cicd/#restart-ci-jobs","title":"Restart CI jobs","text":"<p>This section applies for public projects. Public projects get as notification a link to the pipeline in the Gitlab mirror repository. Since a user does not have any special permissions to the mirror repository, it is not possible to restart jobs through the Gitlab UI. However, it is possible to see an alternative view of your pipeline result in a CSCS-provided pipeline overview. Private projects will always get as notification a link to the CSCS pipeline overview, since the mirror repository is private and the pipeline results are not visible publicly.</p> <p>To view the CSCS pipeline overview for a public project and restart / cancel jobs, follow these steps:</p> <ul> <li>Copy the web link of the CSCS CI status of your project and remove the from the link the <code>type=gitlab</code>.</li> <li>Alternatively, assemble the link yourself, it has the form <code>https://cicd-ext-mw.cscs.ch/ci/pipeline/results/&lt;repository_id&gt;/&lt;project_id&gt;/&lt;pipeline_nb&gt;</code> (the IDs can be found on the Gitlab page of your mirror project).</li> <li>Click on <code>Login to restart jobs</code> at the bottom right and login with your CSCS credentials</li> <li>Click <code>Cancel running</code> or <code>Restart jobs</code> or cancel individual jobs (button next to job\u2019s name)</li> <li>Everybody that has at least Manager access can restart / cancel jobs (access level is managed on the CI setup page in the Admin section)</li> </ul>"},{"location":"services/cicd/#common-pitfalls","title":"Common pitfalls","text":"<p>The CI service uses GitLab to run the pipelines, with the familiar GitLab YAML configuration. However, some features described in the official documentation will not work as expected.</p> <p>Below are known differences:</p> <ul> <li><code>CI_PIPELINE_SOURCE</code> always has the value <code>trigger</code>, i.e. it is never <code>push</code> or <code>merge_request</code>, or any other value mentioned in the GitLab documentation.</li> <li><code>rules: changes</code> will not work, as you never run a branch or a merge request pipeline</li> <li><code>only / except</code> and <code>rules</code> can restrict jobs to specific branches, but remember that a pipeline is only triggered if it matches the rules defined on your repository\u2019s setup page.</li> <li>Trigger jobs for child pipelines must use <code>trigger:forward:pipeline_variables: true</code>, i.e.   <pre><code>my trigger job:\n  trigger:\n    include: path/to/child/pipeline.yml\n    forward:\n      pipeline_variables: true\n</code></pre></li> </ul>"},{"location":"services/cicd/#ci-variables","title":"CI variables","text":"<p>Many variables exist during a pipeline run, they are documented at Gitlab\u2019s predefined variables. Variables are exposed as environment variables during job execution, where the key is the environment variable\u2019s name. Additionally to CI variables available through Gitlab, there are a few CSCS specific variables:</p>"},{"location":"services/cicd/#cscs_registry","title":"<code>CSCS_REGISTRY</code>","text":"<p>value: <code>jfrog.svc.cscs.ch</code></p> <p>CSCS internal registry, preferred registry to store your container images</p>"},{"location":"services/cicd/#cscs_registry_path","title":"<code>CSCS_REGISTRY_PATH</code>","text":"<p>value: <code>jfrog.svc.cscs.ch/docker-ci-ext/&lt;repository-id&gt;</code></p> <p>The prefix path in the CSCS internal container image registry, to which your pipeline has write access. Within this prefix, you can choose any directory structure. Images that are pushed to a path matching <code>**/public/**</code>, can be pulled by anybody within CSCS network</p>"},{"location":"services/cicd/#cscs_ci_mw_url","title":"<code>CSCS_CI_MW_URL</code>","text":"<p>value: <code>https://cicd-ext-mw.cscs.ch/ci</code></p> <p>The URL of the middleware, the orchestrator software</p>"},{"location":"services/cicd/#cscs_ci_default_slurm_account","title":"<code>CSCS_CI_DEFAULT_SLURM_ACCOUNT</code>","text":"<p>value: Configured on the CI setup page in the admin section <code>Firecrest Slurm Account</code></p> <p>The project to which Slurm accounting will go to by default. It can be overwritten via <code>SLURM_ACCOUNT</code> for individual jobs.</p>"},{"location":"services/cicd/#cscs_ci_orig_clone_url","title":"<code>CSCS_CI_ORIG_CLONE_URL</code>","text":"<p>value:</p> <ul> <li>public repositories: HTTPS clone URL, e.g. <code>https://github.com/my-org/my-project</code></li> <li>private repositories: SSH clone URL, e.g. <code>git@github.com:my-org/my-project</code></li> </ul> <p>Clone URL for git. This is needed for some implementation details of the gitlab-runner custom executor. This is the clone URL of the registered project, i.e. this is not the clone URL of the mirror project.</p>"},{"location":"services/cicd/#arch","title":"<code>ARCH</code>","text":"<p>value: <code>x86_64</code> or <code>aarch64</code></p> <p>This is the architecture of the runner. It is either an ARM64 machine, i.e. <code>aarch64</code>, or a traditional <code>x86_64</code> machine.</p>"},{"location":"services/cicd/#runners-reference","title":"Runners reference","text":"<p>Info</p> <p>This section is a reference documentation. It is not meant as a starting point to understand the CI/CD workflow. It documents the interface that a runner provides and the input variables that it accepts. It is a good place for quick lookup, once you have a pipeline already set up.</p> <p>Use CSCS\u2019 runners</p> <p>Each runner and its interface are described below. To use a runner provided by CSCS you must include the runners configuration yaml file in your pipeline like this: <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n</code></pre></p> <p>The runners <code>container-runner</code>, <code>uenv-builder</code>, <code>uenv-runner</code>, and <code>baremetal-runner</code> will submit a Slurm job to a cluster. To configure your Slurm job, you must use the <code>variables</code> section in your job\u2019s yaml config. Accepted variables are documented at Slurm\u2019s srun man page.</p> <p>Parametrizing Slurm</p> <pre><code>my job:\n    extends: .container-runner-daint-gh200\n    image: $CSCS_REGISTRY_PATH/my_image:$CI_COMMIT_SHORT_SHA\n    script:\n        - echo \"I am running as rank $SLURM_PROCID on $(hostname)\"\n    variables:\n        SLURM_JOB_NUM_NODES: 3\n        SLURM_NTASKS: 12\n        SLURM_TIMELIMIT: '00:10:00'\n</code></pre> <p>SLURM_TIMELIMIT</p> <p>Special attention should go the variable <code>SLURM_TIMELIMIT</code>, which sets the maximum time of your Slurm job. You will be billed the node hours that your CI jobs are spending on the cluster, i.e. you want to set the <code>SLURM_TIMELIMIT</code> to the maximum time that you expect the job to run. You should also pay attention to wrap the value in quotes, because the gitlab-runner interprets the time differently than Slurm, when it is not wrapped in quotes, i.e. This is correct: <pre><code>SLURM_TIMELIMIT: \"00:30:00\"\n</code></pre> Without quotes the value will be converted to the integer 1800 internally, before being passed to the runner (i.e. number of seconds), however for Slurm the convention is that an integer value is in the unit minutes.</p>"},{"location":"services/cicd/#container-builder","title":"container-builder","text":"<p>We provide the <code>container-builder</code> runner for every available CPU architecture at CSCS. This runner takes as input a Dockerfile, builds a container image based on the recipe in the Dockerfile and publishes the image to an OCI registry.</p> <p>The naming for the runner is <code>.container-builder-cscs-&lt;MICROARCHITECTURE&gt;</code>.</p> <p>The following runners are available:</p> <ul> <li><code>.container-builder-cscs-zen2</code></li> <li><code>.container-builder-cscs-gh200</code></li> </ul>"},{"location":"services/cicd/#variables","title":"Variables","text":""},{"location":"services/cicd/#dockerfile","title":"<code>DOCKERFILE</code>","text":"<p>Mandatory variable, example value: <code>ci/docker/Dockerfile</code></p> <p>Relative path in your repository to the Dockerfile recipe.</p>"},{"location":"services/cicd/#persist_image_name","title":"<code>PERSIST_IMAGE_NAME</code>","text":"<p>Mandatory variable, example value: <code>$CSCS_REGISTRY_PATH/subdirectory/my_image:$CI_COMMIT_SHORT_SHA</code></p> <p>The path where to store the container image. CSCS provides a registry through the variable <code>CSCS_REGISTRY_PATH</code>. Images stored in the CSCS provided registry can only be accessed from the CSCS network. A pipeline has read and write access to any path inside <code>$CSCS_REGISTRY_PATH</code>.</p> <p>See also dependency management for common naming and third party registry usage.</p>"},{"location":"services/cicd/#cscs_build_in_memory","title":"<code>CSCS_BUILD_IN_MEMORY</code>","text":"<p>Optional variable, default: <code>TRUE</code></p> <p>Instruct the runner that the whole build process will build in memory. The default value is <code>TRUE</code>, and you should only set it to <code>FALSE</code> if you see your job failing due to out-of-memory errors.</p>"},{"location":"services/cicd/#docker_build_args","title":"<code>DOCKER_BUILD_ARGS</code>","text":"<p>Optional variable, example value: <code>[\"ARG1=val1\", \"ARG2=val2\"]</code></p> <p>This allows the usage of the keyword ARG  in your Dockerfile. The value must be a valid JSON array, where each entry is a string.</p> <p>It is almost always correct to wrap the full value in single-quotes.</p> <p>It is also possible to define the argument\u2019s values as an entry in <code>variables</code>, and then reference in <code>DOCKER_BUILD_ARGS</code> only the variables that you want to expose to the build process, i.e. something like this: <pre><code>my job:\n  extends: .container-builder-cscs-gh200\n  variables:\n    VAR1: some value\n    VAR2: another variable\n    DOCKER_BUILD_ARGS: '[\"VAR1\", \"VAR2\"]'\n</code></pre></p>"},{"location":"services/cicd/#cscs_rebuild_policy","title":"<code>CSCS_REBUILD_POLICY</code>","text":"<p>Optional variable, default: <code>if-not-exists</code></p> <p>This variable can be:</p> <ul> <li><code>always</code><ul> <li>A new container image will always we built.</li> </ul> </li> <li><code>if-not-exists</code><ul> <li>The runner will first check on the registry if the image <code>$PERSIST_IMAGE_NAME</code>  exists already.   If it exists, then the runner will not rebuild the image.   This is useful to disable rebuilding of base containers.   See section dependency management.</li> </ul> </li> </ul>"},{"location":"services/cicd/#secondary_registry","title":"<code>SECONDARY_REGISTRY</code>","text":"<p>Optional variable, example value: <code>docker.io/username/my_image:1.0</code></p> <p>Allows pushing also to <code>$SECONDARY_REGISTRY</code>, additionally to <code>$PERSIST_IMAGE_NAME</code>. The result image will pushed to both registries.</p>"},{"location":"services/cicd/#secondary_registry_username","title":"<code>SECONDARY_REGISTRY_USERNAME</code>","text":"<p>Optional variable</p> <p>The username to push to <code>$SECONDARY_REGISTRY</code>. Mandatory when using <code>SECONDARY_REGISTRY</code>.</p>"},{"location":"services/cicd/#secondary_registry_password","title":"<code>SECONDARY_REGISTRY_PASSWORD</code>","text":"<p>Optional variable</p> <p>The password/token to push to <code>$SECONDARY_REGISTRY</code>. Mandatory when using <code>SECONDARY_REGISTRY</code> For security you should store a secret variable on the CI setup page, and forward it in the job yaml. If possible do not use your password, but create an access token.</p>"},{"location":"services/cicd/#custom_registry_username","title":"<code>CUSTOM_REGISTRY_USERNAME</code>","text":"<p>Optional variable</p> <p>If <code>$PERSIST_IMAGE_NAME</code> is not inside the CSCS default registry, then you have to provide the credentials for pushing to the registry.</p>"},{"location":"services/cicd/#custom_registry_password","title":"<code>CUSTOM_REGISTRY_PASSWORD</code>","text":"<p>Optional variable</p> <p>If <code>$PERSIST_IMAGE_NAME</code> is not inside the CSCS default registry, then you have to provide the credentials for pushing to the registry. For security you should store a secret variable on the CI setup page, and forward it in the job yaml. If possible do not use your password, but create an access token.</p>"},{"location":"services/cicd/#build-arguments","title":"Build arguments","text":"<p>Build arguments are configured with the variable <code>DOCKER_BUILD_ARGS</code>. Additionally these build arguments are injected</p>"},{"location":"services/cicd/#cscs_registry_path_1","title":"<code>CSCS_REGISTRY_PATH</code>","text":"<p>This allows to do in your <code>Dockerfile</code> this: <pre><code>ARG CSCS_REGISTRY_PATH\nFROM $CSCS_REGISTRY_PATH/some_subdir/my_base_image:1.0\n</code></pre></p>"},{"location":"services/cicd/#num_procs","title":"<code>NUM_PROCS</code>","text":"<p>This is an integer value with the numbers of CPU cores assigned to your build process. This is useful as number of jobs for <code>make</code>. <pre><code>ARG NUM_PROCS\nRUN cd build &amp;&amp; make -j$NUM_PROCS\n</code></pre></p>"},{"location":"services/cicd/#build-context","title":"Build context","text":"<p>The build context during the build process is the cloned repository, which allows you to copy the sources inside the image via <pre><code>COPY . /tmp/cloned_repository\n</code></pre></p> <p>Additionally the cloned repository is bind-mounted inside the build process under the path <code>/sourcecode</code>, which allows you to read files from there, without the need to copy them inside the container image.. <pre><code>RUN cp -a /sourcecode /tmp/cloned_repository\n</code></pre></p>"},{"location":"services/cicd/#example-jobs","title":"Example jobs","text":"<pre><code>job1:\n  extends: .container-builder-cscs-zen2\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/x86_64/my_image:$CI_COMMIT_SHORT_SHA\n\njob2:\n  extends: .container-builder-cscs-gh200\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/aarch64/my_image:$CI_COMMIT_SHORT_SHA\n</code></pre>"},{"location":"services/cicd/#container-runner","title":"container-runner","text":"<p>This runner submits SLURM jobs through FirecREST. See the comments below and the FirecREST documentation for additional FirecREST information.</p> <p>The naming for the runner is <code>.container-runner-&lt;CLUSTERNAME&gt;-&lt;MICROARCHITECTURE&gt;</code></p> <p>The following runners are available:</p> <ul> <li><code>.container-runner-eiger-zen2</code></li> <li><code>.container-runner-daint-gh200</code></li> <li><code>.container-runner-santis-gh200</code></li> <li><code>.container-runner-clariden-gh200</code></li> </ul> <p>The container image is specified in the tag <code>image</code> in the job yaml. This tag is mandatory.</p>"},{"location":"services/cicd/#variables_1","title":"Variables","text":""},{"location":"services/cicd/#git_strategy","title":"<code>GIT_STRATEGY</code>","text":"<p>Optional variable, default is <code>none</code></p> <p>This is a default Gitlab variable, but mentioned here explicitly, because very often you do not need to clone the repository source code when you run your containerized application.</p> <p>The default is <code>none</code>, and you must explicitly set it to <code>fetch</code>  or <code>clone</code>  to fetch the source code by the runner.</p>"},{"location":"services/cicd/#cscs_cuda_mps","title":"<code>CSCS_CUDA_MPS</code>","text":"<p>Optional variable, default is <code>NO</code></p> <p>Enable running with <code>nvidia-mps-server</code>, which allows multiple ranks sharing the same GPU.</p>"},{"location":"services/cicd/#use_mpi","title":"<code>USE_MPI</code>","text":"<p>Optional variable, default is <code>AUTO</code></p> <p>Enable running with MPI hooks enabled. This allows to inject the host MPI library inside the container runtime for native MPI speed.</p> <p>This variable is optional and the default value is <code>AUTO</code> , where it is set to <code>YES</code>, if you run with more than 1 rank, otherwise <code>NO</code>.</p>"},{"location":"services/cicd/#use_nccl","title":"<code>USE_NCCL</code>","text":"<p>Optional variable, default is empty</p> <p>Set to the NCCL variant that you would like to use (e.g. <code>cuda12</code>) This adds the annotations <code>aws_ofi_nccl.variant=&lt;value&gt;</code> and <code>aws_ofi_nccl.enabled=true</code>.</p>"},{"location":"services/cicd/#edf_append","title":"<code>EDF_APPEND</code>","text":"<p>Optional variable, default is empty</p> <p>This allows to append any user-defined additional EDF keys that are not yet controlled by explicit variables.</p> <p>In general you should prefer using the variables to enable/disable specific annotations.</p>"},{"location":"services/cicd/#cscs_additional_mounts","title":"<code>CSCS_ADDITIONAL_MOUNTS</code>","text":"<p>Optional variable, default is empty</p> <p>This allows mounting user defined host directories inside the container. The value must be a valid JSON array of strings, where each entry is of the form <code>&lt;host-path&gt;:&lt;container-path&gt;</code>. Example: <pre><code>CSCS_ADDITIONAL_MOUNTS: '[\"/scratch/capstor/cscs:/scratch\", \"/users/&lt;my-username&gt;:/home/cscs_home\"]'\n</code></pre></p>"},{"location":"services/cicd/#example-jobs_1","title":"Example jobs","text":"<pre><code>job1:\n  extends: .container-runner-daint-gh200\n  image: $CSCS_REGISTRY_PATH/aarch64/my_image:$CI_COMMIT_SHORT_SHA\n  script:\n    - /usr/bin/my_application /data/some_input.xml\n  variables:\n    CSCS_ADDITIONAL_MOUNTS: '[\"/capstor/scratch/cscs/&lt;my_username&gt;/data:/data\"]'\n\njob2:\n  extends: .container-runner-eiger-zen2\n  image: $CSCS_REGISTRY_PATH/x86_64/my_image:$CI_COMMIT_SHORT_SHA\n  script:\n    - /usr/bin/my_application ./data_in_repository.txt\n  variables:\n    GIT_STRATEGY: fetch\n</code></pre>"},{"location":"services/cicd/#container-runner-lightweight","title":"container-runner-lightweight","text":"<p>This runner allows lightweight jobs that do not need many resources. The advantage is that the job is not running via Slurm and can therefore start faster. The maximum timeout for this runner is 60 minutes and you can request at most 4 CPUs and 4GB of memory. If your job does not fit these requirements, then you must use the default container-runner.</p> <p>Typical examples of when this runner is the right choice (not limited to these use cases though):</p> <ul> <li>Upload code coverage artifacts</li> <li>Create a dynamic pipeline yaml file</li> </ul> <p>The naming for the runner is <code>.container-runner-lightweight-&lt;MICROARCHITECTURE&gt;</code></p> <p>The following runners are available:</p> <ul> <li><code>.container-runner-lightweight-zen2</code></li> <li><code>.container-runner-lightweight-gh200</code></li> </ul> <p>This runner is restricted to public images. It is not possible to run an image that cannot be pulled anonymously. If you have built a container image in a previous stage and stored it in <code>$CSCS_REGISTRY_PATH</code>, then you must ensure that it is in a subdirectory with the name <code>public</code>, i.e., the image path must match the wildcard <code>$CSCS_REGISTRY_PATH/**/public/**</code>.</p> <p>You can set the CPU and memory requests/limits with variables. A request specifies the minimum amount of resources that your job requires. Your job will not be scheduled until the requested resources are available. A limit is the maximum that your job might be able to use if available, but the job is not guaranteed to be allocated that limit.</p>"},{"location":"services/cicd/#variables_2","title":"Variables","text":""},{"location":"services/cicd/#kubernetes_cpu_request","title":"<code>KUBERNETES_CPU_REQUEST</code>","text":"<p>Optional variable, default is 1</p> <p>Number of CPUs minimally needed to schedule this job.</p>"},{"location":"services/cicd/#kubernetes_cpu_limit","title":"<code>KUBERNETES_CPU_LIMIT</code>","text":"<p>Optional variable, default is 1 Limit the job to use at most that many CPUs.</p>"},{"location":"services/cicd/#kubernetes_memory_request","title":"<code>KUBERNETES_MEMORY_REQUEST</code>","text":"<p>Optional variable, default is <code>1Gi</code></p> <p>The amount of memory minimally needed to schedule the job.</p>"},{"location":"services/cicd/#kubernetes_memory_limit","title":"<code>KUBERNETES_MEMORY_LIMIT</code>","text":"<p>Optional variable, default is <code>1Gi</code></p> <p>Limit the job to use at most this much memory. You will get an OOM (out-of-memory) error, if you exceed the limit.</p>"},{"location":"services/cicd/#example-jobs_2","title":"Example jobs","text":"<pre><code>job1:\n  extends: .container-runner-lightweight-zen2\n  image: docker.io/python:3.11\n  script:\n    - ci/pipeline/generate_pipeline.py &gt; dynamic_pipeline.yaml\n  artifacts:\n    paths:\n      - dynamic_pipeline.yaml\n\njob2:\n  extends: .container-runner-lightweight-aarch64\n  image: docker.io/python:3.11\n  script:\n    - ci/upload_code_coverage.sh\n</code></pre>"},{"location":"services/cicd/#uenv-builder","title":"uenv-builder","text":"<p>This runner submits SLURM jobs through FirecREST. See the comments below and the FirecREST documentation for additional FirecREST information.</p> <p>The naming for the runner is <code>.uenv-builder-&lt;CLUSTERNAME&gt;-&lt;MICROARCHITECTURE&gt;</code></p> <p>The following runners are available:</p> <ul> <li><code>.uenv-builder-eiger-zen2</code></li> <li><code>.uenv-builder-daint-gh200</code></li> <li><code>.uenv-builder-santis-gh200</code></li> <li><code>.uenv-builder-clariden-gh200</code></li> </ul> <p><code>uenv-builder</code> is very similar to container-builder, the main difference is that you are building a uenv based on a recipe directory instead of Dockerfile.</p> <p>The uenv will be registered under the name <code>$UENV_NAME/$UENV_VERSION:$UENV_TAG</code>.</p> <p>A uenv will only be rebuilt, if there is no uenv already registered under that name.</p> <p>The tag\u2019s default value is calculated as a hash from the contents of your uenv recipe yaml files, which ensures that a uenv is rebuilt every time the content of the recipe\u2019s yaml files changes. Additionally to the computed hash value, the uenv image will also be registered under the name <code>$UENV_NAME/$UENV_VERSION:$CI_PIPELINE_ID</code>, which allows to refer to the image in subsequent uenv-runner jobs.</p>"},{"location":"services/cicd/#variables_3","title":"Variables","text":""},{"location":"services/cicd/#uenv_name","title":"<code>UENV_NAME</code>","text":"<p>Mandatory variable, default is empty</p> <p>The name of the uenv. Use alpha-numeric characters, dash (<code>-</code>), underscore (<code>_</code>), and dot (<code>.</code>).</p>"},{"location":"services/cicd/#uenv_version","title":"<code>UENV_VERSION</code>","text":"<p>Mandatory variable, default is empty</p> <p>The version of the uenv. Use alpha-numeric characters, dash (<code>-</code>), underscore (<code>_</code>), and dot (<code>.</code>).</p>"},{"location":"services/cicd/#uenv_recipe","title":"<code>UENV_RECIPE</code>","text":"<p>Mandatory variable, default is empty</p> <p>The relative path to the directory containing the recipe yaml files.</p>"},{"location":"services/cicd/#uenv_tag","title":"<code>UENV_TAG</code>","text":"<p>Optional variable, default is a computed hash</p> <p>Set to an explicit tag, if you want to opt-out of the feature that a uenv is automatically rebuilt, when the contents of the recipe yaml files changes. Please keep in mind that a uenv is only rebuilt, when the full uenv name changes.</p>"},{"location":"services/cicd/#example-jobs_3","title":"Example jobs","text":"<pre><code>job1:\n  extends: .uenv-builder-eiger-zen2\n  variables:\n    UENV_NAME: prgenv-gnu\n    UENV_VERSION: 24.10\n    UENV_RECIPE: ci/uenv-recipes/prgenv-gnu/eiger-zen2\n\njob2:\n  extends: .uenv-builder-daint-gh200\n  variables:\n    UENV_NAME: prgenv-gnu\n    UENV_VERSION: 24.10\n    UENV_RECIPE: ci/uenv-recipes/prgenv-gnu/daint-gh200\n</code></pre>"},{"location":"services/cicd/#uenv-runner","title":"uenv-runner","text":"<p>This runner submits SLURM jobs through FirecREST. See the comments below and the FirecREST documentation for additional FirecREST information.</p> <p>The naming for the runner is <code>.uenv-runner-&lt;CLUSTERNAME&gt;-&lt;MICROARCHITECTURE&gt;</code></p> <p>The following runners are available:</p> <ul> <li><code>.uenv-runner-eiger-zen2</code></li> <li><code>.uenv-runner-daint-gh200</code></li> <li><code>.uenv-runner-santis-gh200</code></li> <li><code>.uenv-runner-clariden-gh200</code></li> </ul> <p><code>uenv-runner</code> is very similar to container-runner, the main difference is that you are running with a uenv image mounted instead of inside a container.</p> <p>The uenv image is specified in the tag <code>image</code> in the job yaml. This tag is mandatory.</p>"},{"location":"services/cicd/#variables_4","title":"Variables","text":""},{"location":"services/cicd/#with_uenv_view","title":"<code>WITH_UENV_VIEW</code>","text":"<p>Optional variable, default is empty</p> <p>Loads the view of a uenv.</p>"},{"location":"services/cicd/#cscs_cuda_mps_1","title":"<code>CSCS_CUDA_MPS</code>","text":"<p>Optional variable, default is <code>NO</code></p> <p>Enable running with <code>nvidia-mps-server</code>, which allows multiple ranks sharing the same GPU.</p>"},{"location":"services/cicd/#example-jobs_4","title":"Example jobs","text":"<pre><code>job1:\n  extends: .uenv-runner-eiger-zen2\n  image: prgenv-gnu/24.7:v3\n  script:\n    - gcc --version\n  variables:\n    WITH_UENV_VIEW: 'default'\n\njob2:\n  extends: .uenv-runner-daint-gh200\n  image:  gromacs/2024:v1\n  script:\n    - gmx_mpi --version\n  variables:\n    WITH_UENV_VIEW: 'gromacs'\n    SLURM_JOB_NUM_NODES: 1\n    SLURM_NTASKS: 4\n</code></pre>"},{"location":"services/cicd/#baremetal-runner","title":"baremetal-runner","text":"<p>This runner submits SLURM jobs through FirecREST. See the comments below and the FirecREST documentation for additional FirecREST information.</p> <p>The naming for the runner is <code>baremetal-runner-&lt;CLUSTERNAME&gt;-&lt;MICROARCHITECTURE&gt;</code></p> <p>The following runners are available:</p> <ul> <li><code>.baremetal-runner-eiger-zen2</code></li> <li><code>.baremetal-runner-daint-gh200</code></li> <li><code>.baremetal-runner-santis-gh200</code></li> <li><code>.baremetal-runner-clariden-gh200</code></li> </ul> <p>This runner mode is almost equivalent to writing a Slurm sbatch script. Instead of <code>#SBATCH</code> instructions, you need to use the <code>SLURM_*</code>  variables to specify your Slurm requirements. Otherwise all commands in <code>script</code> are executed only on the first allocated node. To run with multiple ranks, the command in <code>script</code> must switch context via <code>srun</code>.</p> <p>This runner has no additional variables.</p>"},{"location":"services/cicd/#example-job","title":"Example job","text":"<pre><code>job:\n  extends: .baremetal-runner-daint-gh200\n  script:\n    - hostname\n    - srun -n 4 --uenv prgenv-gnu/24.7:v3 --view=default ./my_application\n  variables:\n    SLURM_JOB_NUM_NODES: 1\n</code></pre>"},{"location":"services/cicd/#f7t-controller","title":"f7t-controller","text":"<p>This runner allows submitting jobs to Slurm clusters using FirecREST.</p> <p>The following runners are available:</p> <ul> <li><code>.f7t-controller</code></li> </ul> <p>With this runner, all the dependencies for submitting jobs with FirecREST are already available in the environment. You can either use the client tool <code>firecrest</code>, or a python script that uses the pyfirecrest library. When the job starts, the runner will expose four environment variables, which are needed to allow submitting jobs through FirecREST, without further configuration.</p> <ul> <li><code>AUTH_TOKEN_URL</code>: This is the same value as the variable F7T_TOKEN_URL in the job description</li> <li><code>FIRECREST_URL</code>: This is the same value as the variable F7T_URL  in the job description</li> <li><code>FIRECREST_CLIENT_ID</code>: The value that is set the CI setup page in the admin section</li> <li><code>FIRECREST_CLIENT_SECRET</code>: The value that is set in the CI setup page in the admin section</li> </ul> <p>A job can be submitted with the client, e.g. via <pre><code>$ firecrest submit --system eiger --account $CSCS_CI_DEFAULT_SLURM_ACCOUNT my_script.sh\n</code></pre></p>"},{"location":"services/cicd/#example-job_1","title":"Example job","text":"<pre><code>job:\n  extends: .f7t-controller\n  script:\n    - CLUSTER=eiger\n    - SUBMISSION=\"$(firecrest submit --system eiger --working-dir=/capstor/scratch/cscs/jenkssl/firecrest/$CI_JOB_ID script.sh)\"\n    - echo \"$SUBMISSION\"\n    - JOBID=$(echo \"$SUBMISSION\" | jq -r '.jobId')\n    - |\n      while true ; do\n        JOB_INFO=$(firecrest job-info --system eiger)\n        if echo $JOB_INFO | jq -r \".[] | select(.jobId == $JOBID) | .status.state\" &gt;/dev/null ; then\n          JOB_STATE=$(echo $JOB_INFO | jq -r \".[] | select(.jobId == $JOBID) | .status.state\")\n          echo JOB_STATE=$JOB_STATE\n          if [[ -n \"$JOB_STATE\" &amp;&amp; \"$JOB_STATE\" != \"RUNNING\" &amp;&amp; \"$JOB_STATE\" != \"PENDING\" ]] ; then\n              echo \"Job finished\"\n              break\n          else\n              echo \"job is still in queue/running\"\n          fi\n        else\n          echo \"Failed parsing JOB_INFO response as json, retrying to fetch job info\"\n        fi\n        sleep 30\n      done\n  variables:\n    F7T_URL: 'https://api.cscs.ch/hpc/firecrest/v2'\n</code></pre>"},{"location":"services/cicd/#reframe-runner","title":"reframe-runner","text":"<p>This runner will run ReFrame.</p> <p>The following runners are available:</p> <ul> <li><code>.reframe-runner</code></li> </ul> <p>ReFrame jobs are submitted with FirecREST. This runner is a thin wrapper over the f7t-controller. The machine where ReFrame is running does not have to be a powerful machine, hence it does not make sense to start the main ReFrame process from a compute node. It makes more sense to start the ReFrame process on a cloud machine and submit the compute jobs through FirecREST to the actual cluster.</p> <p>The easiest way to use the FirecREST scheduler of ReFrame is to use the configuration files that are provided in the main branch of the CSCS Reframe tests repository. In case you want to run ReFrame for a system that is not already available in this directory, please open a ticket to the Service Desk and we will add it or help you update one of the existing ones.</p> <p>Something you should be aware of when running with this scheduler is that ReFrame will not have direct access to the filesystem of the cluster so the stage directory will need to be kept in sync through FirecREST. It is recommended to try to clean the stage directory whenever possible with the <code>postrun_cmds</code> and <code>postbuild_cmds</code> and to avoid autodetection of the processor in each run. Normally ReFrame stores these files in <code>~/.reframe/topology/{system}-{part}/processor.json</code>, but you get a \u201cclean\u201d runner every time. You could either add them in the configuration files or store the files in the first run and copy them to the right directory before ReFrame runs.</p> <p>Finally, you can find some more information in the repository.</p> <p>The default command that is executed is <pre><code>$ reframe -C $RFM_CONFIG -c $RFM_CHECKPATH -Sbuild_locally=0 --report-junit=report.xml -r\n</code></pre> This default can be overwritten, by providing a user-defined <code>script</code> tag in the job.</p>"},{"location":"services/cicd/#variables_5","title":"Variables","text":""},{"location":"services/cicd/#rfm_version","title":"<code>RFM_VERSION</code>","text":"<p>Optional variable, default is a recent version of ReFrame</p> <p>This reframe version will be installed and is available to the job.</p>"},{"location":"services/cicd/#rfm_config","title":"<code>RFM_CONFIG</code>","text":"<p>Mandatory variable, default is empty</p> <p>The path to the config that is passed to <code>reframe</code> via <code>-C</code>.</p>"},{"location":"services/cicd/#rfm_checkpath","title":"<code>RFM_CHECKPATH</code>","text":"<p>Mandatory variable, default is empty</p> <p>The path to the checks that is passed to <code>reframe</code> through <code>-c</code>.</p>"},{"location":"services/cicd/#example-job_2","title":"Example job","text":"<pre><code>job:\n  before_script:\n    - git clone https://github.com/eth-cscs/cscs-reframe-tests\n    - pip install -r cscs-reframe-tests/config/utilities/requirements.txt\n    - sed -i -e \"s/account=csstaff/account=$CSCS_CI_DEFAULT_SLURM_ACCOUNT/\" cscs-reframe-tests/config/systems-firecrest/eiger.py\n  variables:\n    FIRECREST_SYSTEM: 'eiger'\n    FIRECREST_BASEDIR: /capstor/scratch/cscs/jenkssl/reframe-runner\n    RFM_FIRECREST: '1'\n    RFM_CONFIG: cscs-reframe-tests/config/cscs.py\n    RFM_CHECKPATH: cscs-reframe-tests/checks/microbenchmarks/mpi/halo_exchange\n</code></pre>"},{"location":"services/cicd/#firecrest","title":"FirecREST","text":"<p>This is not a runner per se, but since most runners are built on top of FirecREST some relevant notes how CI is interacting with FirecREST.</p> <p>The terms <code>Client ID</code> and <code>Consumer Key</code> are meaning the same thing. In the developer portal they are called <code>Consumer Key</code> and <code>Consumer Secret</code>, while the standard naming would be <code>Client ID</code> and <code>Client Secret</code>.</p> <p>CI will submit jobs with the FirecREST client id/secret that have been stored at the project\u2019s CI setup page (Admin section). Storing the client id/secret is mandatory, because most runners will not work without these credentials.</p> <p>The credentials are tied to a CSCS username, hence the pipeline will run within the context of this user. It is possible and encouraged to request with a Service Desk ticket a CI service account. Then the FirecREST credentials can be tied to the CI service account.</p> <p>You will always need 4 pieces of information to interact with FirecREST:</p> <ul> <li>Token dispenser URL</li> <li>API endpoint URL</li> <li>Client ID</li> <li>Client Secret</li> </ul> <p>In the CI context the token dispenser URL is passed with the variable <code>F7T_TOKEN_URL</code>, the API endpoint is passed with the variable <code>F7T_URL</code>. The client ID/Secret are stored in the CI setup page. The client ID/Secret can be overridden on a per-job basis. The variables for overriding are <code>F7T_CLIENT_ID</code>  and <code>F7T_CLIENT_SECRET</code>.</p> <p>In a nutshell, the client ID and client secret are used to request from the token dispenser URL an access token. The token dispenser will reply with an access token, if and only if the client ID/secret pair is valid. This access token is then used to authorize the API requests that are being sent to the FirecREST API endpoint.</p> <p>The documented runners above, set the correct <code>F7T_TOKEN_URL</code>  and <code>F7T_URL</code> for the clusters. When you are running on the f7t-controller runner, then you might have to modify the default variables, because this runner is not targeting a specific cluster, but it can target different clusters in the same job. Targeting different clusters in the same job can require to provide different <code>F7T_URL</code>. The <code>F7T_TOKEN_URL</code> is currently the same for all clusters.</p>"},{"location":"services/cicd/#example-projects","title":"Example projects","text":"<p>A couple of projects which use this CI setup. Please have a look there for more advanced usage:</p> <ul> <li>dcomex-framework: entry point is <code>ci/prototype.yml</code></li> <li>mars: two pipelines, with entry points <code>ci/gitlab/cscs/gpu/gitlab-daint.yml</code> and <code>ci/gitlab/cscs/mc/gitlab-daint.yml</code></li> <li>sparse_accumulation: entry point is <code>ci/pipeline.yml</code></li> <li>gt4py: entry point is <code>ci/cscs-ci.yml</code></li> <li>SIRIUS: entry point is <code>ci/cscs-daint.yml</code></li> <li>sphericart: entry point is <code>ci/pipeline.yml</code></li> </ul>"},{"location":"services/devportal/","title":"Developer Portal","text":""},{"location":"services/devportal/#developer-portal","title":"Developer Portal","text":"<p>The Developer Portal facilitates CSCS users to manage client applications\u2019 subscriptions to an API at CSCS, such as FirecREST or CI/CD.</p>"},{"location":"services/devportal/#terminology","title":"Terminology","text":""},{"location":"services/devportal/#api","title":"API","text":"<p>An API is an interface exposed by CSCS to their users to allow them accessing protected resources (computing, storage, accounting data, etc.) in an automated fashion.</p> <p>Examples of automated access to HPC resources are:</p> <ul> <li>A cronjob that triggers a CI/CD pipeline to test software periodically</li> <li>A workflow manager that executes workflows asynchronously</li> <li>A web application that performs unattended data transfer after a job finishes</li> </ul> <p></p>"},{"location":"services/devportal/#application","title":"Application","text":"<p>An application (or \u201cclient application\u201d or \u201cclient\u201d) is a software created by users that consumes the resources exposed by an API.</p> <p>At CSCS, it requires OIDC/OAuth2 access tokens to authenticate against an Identity Provider (IdP) in order to access the protected resources.</p> <p>An application consumes API resources by subscribing to the API. At CSCS, applications can be subscribed to multiple APIs.</p>"},{"location":"services/devportal/#identity-provider-idp","title":"Identity Provider (IdP)","text":"<p>An identity provider is an entity that authenticates an application and allows accessing protected resources. In this case, CSCS provides an IdP for all its APIs.</p>"},{"location":"services/devportal/#production-keys","title":"Production Keys","text":"<p>Applications must identify against the CSCS IdP. To do this, users must create the production keys (Consumer ID - or Client ID -, and Consumer Secret - or Client Secret) that are going to be used to authenticate.</p> <p>Warning</p> <p>The pair Consumer Key and Secret represents a personal credential to access CSCS resources, therefore can\u2019t be shared and must be stored securely (i.e.: not exposed in public repositories, forums, documentation, etc)</p>"},{"location":"services/devportal/#getting-started","title":"Getting started","text":"<p>Start by browsing to developer.cscs.ch, then sign in by clicking the <code>SIGN-IN</code> button on the top right hand corner of the page.</p> <p>Once logged in, you will see a list of APIs that are available to your user.</p> <p>Warning</p> <p>You might not see version 1 or version 2 of some APIs. You will be able to see all the versions when you subscribe your Application to the API.</p>"},{"location":"services/devportal/#creating-an-application","title":"Creating an Application","text":"<p>Click on the <code>Applications</code> option at the top of the screen to manage your applications.</p> <p></p> <p>To create a new application, click on the <code>ADD NEW APPLICATION</code> button at the top of the Applications view</p> <p></p> <p>Complete the mandatory fields (marked with <code>*</code>) and make sure to give the application a unique name and select the number of requests per minute.</p> <p>When finished, click on the <code>SAVE</code> button.</p> <p>Note</p> <p>On your first login you will find the <code>DefaultApplication</code> which is created by default</p> <p>Note</p> <p>The quota of requests per minute will be shared by all subscribers to the Application over all APIs</p>"},{"location":"services/devportal/#configuring-production-keys","title":"Configuring Production Keys","text":"<p>Once the Application is created, create the Production Keys by clicking on <code>Production Keys</code> in the left panel:</p> <p></p> <p>Click on the <code>Generate Keys</code> button at the bottom of the page to generate the production keys of your application:</p> <p></p> <p>Once the keys are generated, you will see the pair <code>Consumer Key</code> (also known as \u201cclient id\u201d) and <code>Consumer Secret</code> (also known as \u201cclient secret\u201d).</p> <p></p> <p>Note</p> <p>At the moment, only FIRECREST keys are available for the users</p>"},{"location":"services/devportal/#subscribe-to-an-api","title":"Subscribe to an API","text":"<p>Once you have set up your Application, is time to subscribe it to an API.</p> <p>To do so:</p> <ul> <li>(8.a) click on the  <code>Subscriptions</code> option on the left panel</li> <li>(8.b) click the  <code>SUBSCRIBE APIS</code> button</li> <li>(8.c) choose the business plan and the version of the API you want to subscribe to by clicking the <code>SUBSCRIBE</code> button on the right-side of the requested API.</li> </ul> <p></p>"},{"location":"services/devportal/#manage-your-applications","title":"Manage your applications","text":"<p>Back on the Subscription Management page, you can review your active subscriptions and APIs that your Application has access to.</p> <p></p>"},{"location":"services/devportal/#regenerate-production-keys","title":"Regenerate production keys","text":"<p>It\u2019s a good practice to rotate or regenerate the production keys of your application to increase security.</p> <p>To do this, go to the <code>Production Keys</code> view of the desired application and click on the button <code>REMOVE KEYS</code>:</p> <p></p> <p>To regenerate the keys, refer back to the Configuring Production Keys section.</p> <p>Info</p> <p>After resetting the credentials, you will need to reconfigure your software(s) to use the new keys</p>"},{"location":"services/devportal/#remove-your-application","title":"Remove your application","text":"<p>To delete your application (which means removing all subscriptions and production keys), refer to the Application view and click on the <code>DELETE</code> button in the top-right corner.</p> <p></p>"},{"location":"services/devportal/#view-information-of-apis","title":"View information of APIs","text":"<p>The Developer Portal includes information of the APIs exposed by CSCS on the API view.</p> <p>Click on <code>APIs</code> button on the top of the Developer Portal, and select on the left menu the option <code>Overview</code></p> <p></p> <p>The information exposed in the overview view is:</p> <ul> <li>Amount of subscriptions to the API (among all clients)</li> <li>Documentation of the API (OpenAPI reference, SDK tools, etc)</li> <li>Community information (Public repository, slack channel)</li> <li>Contact information (developers and business)</li> <li>Business plans (amount of requests per minute that are allowed in the API subscription)</li> <li>Applications owned by the user that are subscribed to the API</li> </ul>"},{"location":"services/devportal/#api-documents","title":"API Documents","text":"<p>Clicking in the left option <code>Documents</code> the user will be able to see the API reference with examples:</p> <p></p>"},{"location":"services/devportal/#further-information","title":"Further Information","text":"<ul> <li>WSO2 Developer Portal</li> <li>Open ID Connect protocol (OIDC)</li> <li>JSON Web Tokens</li> <li>OpenAPI initiative</li> </ul>"},{"location":"services/kubernetes/","title":"Index","text":""},{"location":"services/kubernetes/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes is only available for specific partners. </p> <p>Note</p> <p>Kubernetes is not available for normal users on Alps.</p> <p>This documentation is designed to help partners who have been granted access to a Kubernetes cluster. </p> <p>It explains how clusters are provisioned, maintained, and the policies in place for upgrades and updates.</p> <ul> <li> <p> Cluster Architecture</p> <p>CSCS Kubernetes cluster overview. What are the main components and how to interact with it. </p> <p> Clusters</p> </li> <li> <p> Kubernetes Upgrades</p> <p>Kubernetes Cluster upgrade policy (Kubernetes version upgrades)</p> <p> Kubernetes Upgrades</p> </li> <li> <p> Node Updates</p> <p>Cluster Nodes OS update policy (Regular Node Security Updates)</p> <p> Node OS Updates</p> </li> </ul>"},{"location":"services/kubernetes/clusters/","title":"Clusters","text":""},{"location":"services/kubernetes/clusters/#cscs-kubernetes-clusters","title":"CSCS Kubernetes clusters","text":"<p>This document provides an overview of the Kubernetes clusters maintained by CSCS and offers step-by-step instructions for accessing and interacting with them.</p>"},{"location":"services/kubernetes/clusters/#architecture","title":"Architecture","text":"<p>All Kubernetes clusters at CSCS are:</p> <ul> <li>Managed using Rancher</li> <li>Running RKE2 (Rancher Kubernetes Engine 2)</li> </ul> <p>CSCS offers two types of Kubernetes clusters for partners:</p> <ul> <li>Harvester-only clusters: These clusters run exclusively on virtual machines provisioned by Harvester (SUSE Virtualization), providing a flexible and isolated environment suitable for most workloads.</li> <li>Alpernetes clusters: These clusters combine Harvester VMs with compute nodes from the Alps supercomputer. This hybrid setup, called Alpernetes, enables workloads to leverage both virtualized infrastructure and high-performance computing resources within the same Kubernetes environment.</li> </ul>"},{"location":"services/kubernetes/clusters/#cluster-environments","title":"Cluster Environments","text":"<p>Clusters are grouped into two main environments:</p> <ul> <li>TDS \u2013 Test and Development Systems  </li> <li>PROD \u2013 Production</li> </ul> <p>See Kubernetes upgrades for detailed upgrade policy.</p>"},{"location":"services/kubernetes/clusters/#kubernetes-api-access","title":"Kubernetes API Access","text":"<p>You can access the Kubernetes API in two main ways:</p>"},{"location":"services/kubernetes/clusters/#direct-internet-access","title":"Direct Internet Access","text":"<ul> <li>A Virtual IP is exposed for the API server.  </li> <li>Access is restricted by source IP addresses of the partner.</li> </ul>"},{"location":"services/kubernetes/clusters/#access-via-cscs-jump-host","title":"Access via CSCS Jump Host","text":"<ul> <li>Connect through a jump host (e.g., <code>ela.cscs.ch</code>).</li> <li>API calls are securely proxied through Rancher.</li> </ul> <p>To check which method you are using, examine the <code>current-context</code> in your <code>kubeconfig</code> file.</p>"},{"location":"services/kubernetes/clusters/#cluster-access","title":"Cluster Access","text":"<p>To interact with the cluster, you need the <code>kubectl</code> CLI: \ud83d\udd17 Install kubectl </p> <code>kubectl</code> is pre-installed on the CSCS jump host."},{"location":"services/kubernetes/clusters/#retrieve-your-kubeconfig-file","title":"Retrieve your kubeconfig file","text":""},{"location":"services/kubernetes/clusters/#internal-cscs-users","title":"Internal CSCS Users","text":"<p>Access Rancher and download the kubeconfig for your cluster. </p>"},{"location":"services/kubernetes/clusters/#external-users","title":"External Users","text":"<p>A specific Rancher user and password should have been provided to the partner.</p> <p>Use the <code>kcscs</code> tool installed on <code>ela.cscs.ch</code> to obtain the kubeconfig by following the next steps.</p> <p>Download your SSH keys from SSH Service (and add them to the SSH agent).</p> <p>SSH to the jump host using the downloaded SSH keys <pre><code>ssh ela.cscs.ch\n</code></pre></p> <p>Login with <code>kcscs</code> with the provided Rancher credentials <pre><code>kcscs login\n</code></pre></p> <p>List the accessible clusters <pre><code>kcscs list\n</code></pre></p> <p>Retrieve the kubeconfig file for a specific cluster <pre><code>kcscs get\n</code></pre></p>"},{"location":"services/kubernetes/clusters/#store-the-kubeconfig-file","title":"Store the kubeconfig file","text":"<p><pre><code>mv mykubeconfig.yaml ~/.kube/config\n</code></pre> or <pre><code>export KUBECONFIG=/home/user/kubeconfig.yaml\n</code></pre></p>"},{"location":"services/kubernetes/clusters/#test-connectivity","title":"Test connectivity","text":"<pre><code>kubectl get nodes\n</code></pre> <p>Warning</p> <p>The kubeconfig file contains credentials. Keep it secure.</p>"},{"location":"services/kubernetes/clusters/#pre-installed-applications","title":"Pre-installed Applications","text":"<p>All CSCS-provided clusters include a set of pre-installed tools and components, described below:</p>"},{"location":"services/kubernetes/clusters/#ceph-csi","title":"<code>ceph-csi</code>","text":"<p>Provides dynamic persistent volume provisioning via the Ceph Container Storage Interface (CEPH CSI).</p>"},{"location":"services/kubernetes/clusters/#storage-classes","title":"Storage Classes","text":"<ul> <li><code>cephfs</code> \u2013 ReadWriteMany (RWX), backed by HDD (large data volumes)</li> <li><code>rbd-hdd</code> \u2013 ReadWriteOnce (RWO), backed by HDD</li> <li><code>rbd-nvme</code> \u2013 RWO, backed by NVMe (high-performance workloads like databases)</li> <li><code>*-retain</code> \u2013 Same classes, but retain the volume after PVC deletion</li> </ul>"},{"location":"services/kubernetes/clusters/#external-dns","title":"<code>external-dns</code>","text":"<p>Automatically manages DNS entries for:</p> <ul> <li>Ingress resources</li> <li>Services of type <code>LoadBalancer</code> (when annotated)</li> </ul>"},{"location":"services/kubernetes/clusters/#example","title":"Example","text":"<pre><code>kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.mycluster.tds.cscs.ch.\"\n</code></pre> <p>Use a valid name under the configured subdomain</p> <p>\ud83d\udd17 external-dns documentation</p>"},{"location":"services/kubernetes/clusters/#cert-manager","title":"<code>cert-manager</code>","text":"<p>Handles automatic issuance of TLS certificates from Let\u2019s Encrypt.</p>"},{"location":"services/kubernetes/clusters/#example_1","title":"Example","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: echo\nspec:\n  secretName: echo\n  commonName: echo.mycluster.tds.cscs.ch\n  dnsNames:\n    - echo.mycluster.tds.cscs.ch\n  issuerRef:\n    kind: ClusterIssuer\n    name: letsencrypt\n</code></pre> <p>You can also issue certificates automatically via Ingress annotations (see <code>ingress-nginx</code> section).</p> <p>\ud83d\udd17 cert-manager documentation</p>"},{"location":"services/kubernetes/clusters/#metallb","title":"<code>metallb</code>","text":"<p>Enables <code>LoadBalancer</code> service types by assigning public IPs.</p> <p>The public IP pool is limited. Prefer using <code>Ingress</code> unless you specifically need a <code>LoadBalancer</code> Service for TCP traffic.</p> <p>\ud83d\udd17 MetalLB documentation</p>"},{"location":"services/kubernetes/clusters/#ingress-nginx","title":"<code>ingress-nginx</code>","text":"<p>Default Ingress controller with class <code>nginx</code>. Supports automatic TLS via cert-manager annotations.</p> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myIngress\n  namespace: myIngress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt\nspec:\n  rules:\n    - host: example.tds.cscs.ch\n      http:\n        paths:\n          - pathType: Prefix\n            path: /\n            backend:\n              service:\n                name: myservice\n                port:\n                  number: 80\n  tls:\n    - hosts:\n        - example.tds.cscs.ch\n      secretName: myingress-cert\n</code></pre> <p>\ud83d\udd17 NGINX Ingress Docs \ud83d\udd17 cert-manager Ingress Usage</p>"},{"location":"services/kubernetes/clusters/#external-secrets","title":"<code>external-secrets</code>","text":"<p>Integrates with secret management tools like HashiCorp Vault.</p> <p>Enables the usage of <code>ExternalSecret</code> resources to fetch secrets from <code>SecreStore</code> or <code>ClusterSecretStore</code> resources to fetch secrets and store them into <code>Secrets</code> inside the cluster.</p> <p>It helps to avoid storing secrets in the deployment manifests, especially in GitOps environments.</p> <p>\ud83d\udd17 external-secrets documentation</p>"},{"location":"services/kubernetes/clusters/#kured","title":"<code>kured</code>","text":"<p>Responsible for automatic node reboots (e.g., after kernel updates).</p> <p>\ud83d\udd17 kured documentation</p>"},{"location":"services/kubernetes/clusters/#observability","title":"Observability","text":"<p>Includes:</p> <ul> <li>Beats agents \u2013 Export logs and metrics to CSCS\u2019s central log system</li> <li>Prometheus \u2013 Scrapes metrics and exports them to CSCS\u2019s central monitoring cluster</li> </ul>"},{"location":"services/kubernetes/kubernetes-upgrades/","title":"Kubernetes Upgrades","text":""},{"location":"services/kubernetes/kubernetes-upgrades/#kubernetes-cluster-upgrade-policy","title":"Kubernetes Cluster Upgrade Policy","text":"<p>To maintain a secure, stable, and supported platform, we regularly upgrade our Kubernetes clusters. We use RKE2 as our Kubernetes distribution.</p>"},{"location":"services/kubernetes/kubernetes-upgrades/#upgrade-flow","title":"Upgrade Flow","text":"<p>Phased Rollout</p> <ul> <li>Upgrades are first applied to TDS clusters (Test and Development Systems).</li> <li>After a minimum of 2 weeks, if no critical issues are observed, the same upgrade will be applied to PROD clusters.</li> </ul> <p>No Fixed Schedule</p> <ul> <li>Upgrades are not done on a strict calendar basis.</li> <li>Timing may depend on compatibility with other infrastructure components (e.g., storage, CNI plugins, monitoring tools).</li> <li>However, all clusters will be upgraded before the current Kubernetes version reaches End of Life (EOL).</li> </ul>"},{"location":"services/kubernetes/kubernetes-upgrades/#upgrade-impact","title":"Upgrade Impact","text":"<p>The impact of a Kubernetes upgrade can vary, depending on the nature of the changes involved:</p> <p>Minimal Impact</p> <ul> <li>For example, upgrades that affect only the <code>kubelet</code> may be transparent to workloads.</li> <li>Rolling restarts may occur, but no downtime is expected for well-configured applications.</li> </ul> <p>Potentially Disruptive</p> <ul> <li>Upgrades involving components such as the CNI (Container Network Interface) may cause temporary network interruptions.</li> <li>Other control plane or critical component updates might cause short-lived disruption to scheduling or connectivity.</li> </ul> Applications that follow cloud-native best practices (e.g., readiness probes, multiple replicas, graceful shutdown handling) are less likely to be impacted by upgrades."},{"location":"services/kubernetes/kubernetes-upgrades/#what-you-can-expect","title":"What You Can Expect","text":"<ul> <li>Upgrades are performed using safe, tested procedures with minimal risk to production workloads.</li> <li>TDS clusters serve as a canary environment, allowing us to identify issues early.</li> <li>All clusters are kept aligned with supported Kubernetes versions.</li> </ul>"},{"location":"services/kubernetes/node-updates/","title":"Node OS Updates","text":""},{"location":"services/kubernetes/node-updates/#kubernetes-nodes-os-update-policy","title":"Kubernetes Nodes OS Update Policy","text":"<p>To ensure the security and stability of our infrastructure, CSCS will perform monthly OS updates on all nodes of our Kubernetes clusters.</p>"},{"location":"services/kubernetes/node-updates/#maintenance-schedule","title":"Maintenance Schedule","text":"<ul> <li>Frequency: Every first week of the month </li> <li>Reboot Window: Monday to Friday, between 09:00 and 15:00 </li> <li>Time Zone: Europe/Zurich</li> </ul> <p>These updates include important security patches and system updates for the operating systems of cluster nodes.</p> Nodes will be rebooted only if required by the updates."},{"location":"services/kubernetes/node-updates/#urgent-security-patches","title":"Urgent Security Patches","text":"<p>In the event of a critical zero-day vulnerability, we will apply patches and perform reboots (if required) as soon as possible, outside of the regular update schedule if needed.  </p> <ul> <li>Affected nodes will be updated immediately to protect the platform.</li> <li>Users will be notified ahead of time when possible.</li> <li>Standard safety and rolling reboot practices will still be followed.</li> </ul>"},{"location":"services/kubernetes/node-updates/#reboot-management-with-kured","title":"Reboot Management with Kured","text":"<p>We use Kured (KUbernetes REboot Daemon) to safely automate the reboot process. Kured ensures that:</p> <ul> <li>Reboots are triggered only when necessary (e.g., after kernel updates).</li> <li>Nodes are rebooted one at a time to avoid service disruption.</li> <li>Reboots occur only during the defined window </li> <li>Nodes are cordoned, drained, and gracefully reintegrated after reboot.</li> </ul>"},{"location":"services/kubernetes/node-updates/#application-requirements","title":"Application Requirements","text":"<p>To avoid service disruption during node maintenance, applications must be designed for high availability. Specifically:</p> <ul> <li>Use multiple replicas spread across nodes.</li> <li>Follow cloud-native best practices, including:</li> <li>Proper readiness and liveness probes</li> <li>Graceful shutdown support</li> <li>Stateless design or resilient handling of state</li> <li>Appropriate resource requests and limits</li> </ul> <p>Warning</p> <p>Applications that do not meet these requirements may experience temporary disruption during node reboots.</p>"},{"location":"software/","title":"Index","text":""},{"location":"software/#applications-and-frameworks","title":"Applications and frameworks","text":"<p>This section documents the most widely-used applications and frameworks on Alps.</p> <p></p>"},{"location":"software/#supported-software","title":"Supported software","text":"<p>CSCS provides a catalogue of supported software: applications, frameworks and tools that CSCS provide  and support on Alps. All other software can be installed by users as user software.</p> <p>These pages provided documentation for all supported software, and installation guides for selected applications that are frequently requested, particularly if they are challenging to install.</p> <ul> <li> <p> Scientific Applications</p> <p> CP2K</p> <p> GROMACS</p> <p> LAMMPS</p> <p> NAMD</p> <p> Quantum ESPRESSO</p> <p> VASP</p> </li> <li> <p> Communication Libraries</p> <p> Cray MPICH</p> <p> MPICH</p> <p> OpenMPI</p> <p> NCCL</p> <p> RCCL</p> <p> libfabric</p> </li> <li> <p> Machine Learning</p> <p> PyTorch</p> <p>To learn more about how to use PyTorch on Alps:</p> <p> ML tutorials</p> </li> <li> <p> Climate and Weather</p> <p> WRF and CRYOWRF</p> </li> <li> <p> Scientific Visualisation</p> <p> Paraview</p> </li> <li> <p> Commercial Software</p> <p> Matlab</p> </li> </ul> <p>Prohibited Software</p> <p>There is a small list of software that is not allowed on CSCS\u2019 systems, or that CSCS can\u2019t provide support for.</p> <p> Prohibited software</p>"},{"location":"software/unsupported/","title":"Prohibited Software","text":""},{"location":"software/unsupported/#prohibited-software","title":"Prohibited software","text":"<p>The use of the following applications prohibited from use on CSCS systems.</p> Software Use Reason Prohibited Gaussian Electronic structure simulation License terms prohibit installation on CSCS systems"},{"location":"software/commercial/","title":"Index","text":""},{"location":"software/commercial/#commercial-software","title":"Commercial software","text":"<p>This is a collection of installation guides for commonly requested commercial software.</p> <ul> <li>Matlab</li> </ul> <p>You are responsible for respecting license terms</p> <p>When installing software that require a license, it is the responsibility of users to ensure that the software is only available to them personally, or to members of their group who are also permitted by the license to access the software.</p>"},{"location":"software/commercial/matlab/","title":"Matlab","text":""},{"location":"software/commercial/matlab/#matlab","title":"Matlab","text":"<p>CSCS does not have a license to provide Matlab to users on Alps. Users or groups with valid licenses can install Matlab themselves as user software.</p>"},{"location":"software/communication/","title":"Index","text":""},{"location":"software/communication/#communication-libraries","title":"Communication Libraries","text":"<p>CSCS provides common communication libraries optimized for the Slingshot 11 network on Alps.</p> <p>For most scientific applications relying on MPI, Cray MPICH is recommended. MPICH and OpenMPI may also be used, with limitations. Cray MPICH, MPICH, and OpenMPI make use of libfabric to interact with the underlying network.</p> <p>Most machine learning applications rely on NCCL or RCCL for high-performance implementations of collectives. NCCL and RCCL have to be configured with a plugin using libfabric to make full use of the Slingshot network.</p> <p>See the individual pages for each library for information on how to use and best configure the libraries.</p> <ul> <li>Cray MPICH</li> <li>MPICH</li> <li>OpenMPI</li> <li>NCCL</li> <li>RCCL</li> <li>libfabric</li> </ul>"},{"location":"software/communication/cray-mpich/","title":"Cray MPICH","text":""},{"location":"software/communication/cray-mpich/#cray-mpich","title":"Cray MPICH","text":"<p>Cray MPICH is the recommended MPI implementation on Alps. It is available through uenvs like prgenv-gnu and the application-specific uenvs.</p> <p>The Cray MPICH documentation contains detailed information about Cray MPICH. On this page we outline the most common workflows and issues that you may encounter on Alps.</p> <p></p>"},{"location":"software/communication/cray-mpich/#gpu-aware-mpi","title":"GPU-aware MPI","text":"<p>We recommend using GPU-aware MPI whenever possible, as it almost always provides a significant performance improvement compared to communication through CPU memory. To use GPU-aware MPI with Cray MPICH</p> <ol> <li>the application must be linked to the GTL library, and</li> <li>the <code>MPICH_GPU_SUPPORT_ENABLED=1</code> environment variable must be set.</li> </ol> <p>If either of these are missing, the application will fail to communicate GPU buffers.</p> <p>In supported uenvs, Cray MPICH is built with GPU support (on clusters that have GPUs). This means that Cray MPICH will automatically be linked to the GTL library, which implements the the GPU support for Cray MPICH.</p> Checking that the application links to the GTL library <p>To check if your application is linked against the required GTL library, running <code>ldd</code> on your executable <code>myexecutable</code> should print something similar to: <pre><code>$ ldd myexecutable | grep gtl\n        libmpi_gtl_cuda.so =&gt; /user-environment/linux-sles15-neoverse_v2/gcc-13.2.0/cray-gtl-8.1.30-fptqzc5u6t4nals5mivl75nws2fb5vcq/lib/libmpi_gtl_cuda.so (0x0000ffff82aa0000)\n</code></pre></p> <p>The path may be different, but the <code>libmpi_gtl_cuda.so</code> library should be printed when using CUDA. In ROCm environments the <code>libmpi_gtl_hsa.so</code> library should be linked. If the GTL library is not linked, nothing will be printed.</p> <p>In addition to linking to the GTL library, Cray MPICH must be configured to be GPU-aware at runtime by setting the <code>MPICH_GPU_SUPPORT_ENABLED=1</code> environment variable. On some CSCS systems this option is set by default. See this page for more information on configuring Slurm to use GPUs.</p> <p>Segmentation faults when trying to communicate GPU buffers without <code>MPICH_GPU_SUPPORT_ENABLED=1</code></p> <p>If you attempt to communicate GPU buffers through MPI without setting <code>MPICH_GPU_SUPPORT_ENABLED=1</code>, it will lead to segmentation faults, usually without any specific indication that it is the communication that fails. Make sure that the option is set if you are communicating GPU buffers through MPI.</p> <p>Error: \u201c<code>GPU_SUPPORT_ENABLED</code> is requested, but GTL library is not linked\u201d</p> <p>If <code>MPICH_GPU_SUPPORT_ENABLED</code> is set to <code>1</code> and your application does not link against one of the GTL libraries you will get an error similar to the following during MPI initialization: <pre><code>MPICH ERROR [Rank 0] [job id 410301.1] [Thu Feb 13 12:42:18 2025] [nid005414] - Abort(-1) (rank 0 in comm 0): MPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n (Other MPI error)\n\naborting job:\nMPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n</code></pre></p> <p>This means that the required GTL library was not linked to the application. In supported uenvs, GPU support is enabled by default. If you believe a uenv should have GPU support but you are getting the above error, feel free to get in touch with us to understand whether there is an issue with the uenv or something else in your environment.  If you are using Cray modules you must load the corresponding accelerator module, e.g. <code>craype-accel-nvidia90</code>, before compiling your application.</p> <p>Alternatively, if you wish to not use GPU-aware MPI, either unset <code>MPICH_GPU_SUPPORT_ENABLED</code> or explicitly set it to <code>0</code> in your launch scripts.</p> <p></p>"},{"location":"software/communication/cray-mpich/#known-issues","title":"Known issues","text":"<p>This section documents known issues related to Cray MPICH on Alps. Resolved issues are also listed for reference.</p>"},{"location":"software/communication/cray-mpich/#existing-issues","title":"Existing Issues","text":""},{"location":"software/communication/cray-mpich/#cray-mpich-hangs","title":"Cray MPICH hangs","text":"<p>Cray MPICH may sometimes hang on larger runs.</p> <p>Workaround</p> <p>There are many possible reasons why an application would hang, many unrelated to Cray MPICH. However, if you are experiencing hangs the issue may be worked around by setting: <pre><code>export FI_MR_CACHE_MONITOR=disabled\n</code></pre></p> <p>The option <pre><code>export FI_MR_CACHE_MONITOR=userfaultfd\n</code></pre> may also avoid hangs, and typically performs better than completely disabling the cache monitor.</p> <p>Performance may be negatively affected by this option.</p>"},{"location":"software/communication/cray-mpich/#cxil_map-write-error-when-doing-inter-node-gpu-aware-mpi-communication","title":"<code>\"cxil_map: write error\"</code> when doing inter-node GPU-aware MPI communication","text":"<p>This error message is sometimes triggered by applications that use GPU Direct MPI calls when they trigger a bug in gdrcopy (a low-level library used to copy buffers between GPUs). Setting the following option will completely disable gdrcopy. Note that this has a performance impact for small message sizes, so it should only be enabled on a case-by-case basis. <pre><code>export FI_CXI_SAFE_DEVMEM_COPY_THRESHOLD=0\n</code></pre></p> <p></p>"},{"location":"software/communication/cray-mpich/#slow-intra-node-host-communication-with-cray-mpich","title":"Slow intra-node host communication with Cray MPICH","text":"<p>Cray MPICH can perform badly when doing intra-node CPU-CPU memory communication.</p> <p>Workaround</p> <p>In some situations Cray MPICH can perform better when communication is done over the NICs, even within a node. To force Cray MPICH to use NICs for all communication, set:</p> <pre><code>export MPIR_CVAR_NO_LOCAL=1\n</code></pre> <p>Whenever possible, prefer using GPU-GPU communication instead of CPU-CPU communication. It can even be beneficial to transfer data to the GPU only for the communication even if the buffer originally is in CPU memory.</p>"},{"location":"software/communication/cray-mpich/#resolved-issues","title":"Resolved issues","text":""},{"location":"software/communication/cray-mpich/#cxil_map-write-error-when-doing-inter-node-gpu-aware-mpi-communication_1","title":"<code>\"cxil_map: write error\"</code> when doing inter-node GPU-aware MPI communication","text":"The issue has been resolved on the 7th of October 2024 with a system update <p>The issue was caused by a system misconfiguration.</p> <p>When doing inter-node GPU-aware communication with Cray MPICH after the update on the 30th of September 2024 on Alps, applications will fail with: <pre><code>cxil_map: write error\n</code></pre></p> Workaround <p>The only workaround is to not use inter-node GPU-aware MPI.</p> Workaround for CP2K <p>For users of CP2K encountering this issue, one can disable the use of COSMA, which uses GPU-aware MPI, by placing the following in the <code>&amp;GLOBAL</code> section of your input file:  <pre><code>&amp;FM\nTYPE_OF_MATRIX_MULTIPLICATION SCALAPACK\n&amp;END FM\n</code></pre></p> <p>Unless you run RPA calculations, this should have limited impact on performance.</p>"},{"location":"software/communication/cray-mpich/#mpi_thread_multiple-does-not-work","title":"<code>MPI_THREAD_MULTIPLE</code> does not work","text":"<p>The issue has been resolved in Cray MPICH version 8.1.30</p> <p>When using <code>MPI_THREAD_MULTIPLE</code> on GH200 systems Cray MPICH may fail with an assertion that looks similar to: <pre><code>Assertion failed [...]: (&amp;MPIR_THREAD_GLOBAL_ALLFUNC_MUTEX)-&gt;count == 0\n</code></pre></p> <p>or</p> <pre><code>Assertion failed [...]: MPIR_THREAD_GLOBAL_ALLFUNC_MUTEX.count == 0\n</code></pre> Workaround <p>The issue can be worked around by falling back to a less optimized implementation of <code>MPICH_THREAD_MULTIPLE</code> by setting <code>MPICH_OPT_THREAD_SYNC=0</code>.</p>"},{"location":"software/communication/libfabric/","title":"libfabric","text":""},{"location":"software/communication/libfabric/#libfabric","title":"Libfabric","text":"<p>Libfabric, or Open Fabrics Interfaces (OFI), is a low level networking library that abstracts away various networking backends. It is used by Cray MPICH, and can be used together with OpenMPI, NCCL, and RCCL to make use of the Slingshot network on Alps.</p>"},{"location":"software/communication/libfabric/#using-libfabric","title":"Using libfabric","text":"<p>If you are using a uenv provided by CSCS, such as prgenv-gnu, Cray MPICH is linked to libfabric and the high speed network will be used. No changes are required in applications.</p> <p>If you are using containers, the system libfabric can be loaded into your container using the CXI hook provided by the container engine. Using the hook is essential to make full use of the Alps network.</p>"},{"location":"software/communication/libfabric/#tuning-libfabric","title":"Tuning libfabric","text":"<p>Tuning libfabric (particularly together with Cray MPICH, OpenMPI, NCCL, and RCCL) depends on many factors, including the application, workload, and system. For a comprehensive overview libfabric options for the CXI provider (the provider for the Slingshot network), see the <code>fi_cxi</code> man pages. Note that the exact version deployed on Alps may differ, and not all options may be applicable on Alps.</p> <p>See the Cray MPICH known issues page for issues when using Cray MPICH together with libfabric.</p> <p>Todo</p> <p>More options?</p>"},{"location":"software/communication/mpich/","title":"MPICH","text":""},{"location":"software/communication/mpich/#mpich","title":"MPICH","text":"<p>MPICH is an open-source MPI implementation actively developed in this github repository. It can be installed inside containers directly from the source code manually, or using Spack or similar package managers.</p>"},{"location":"software/communication/mpich/#mpich-inside-containers","title":"MPICH inside containers","text":"<p>MPICH can be built inside containers, however for native Slingshot performance special care has to be taken to ensure that communication is optimal for all cases:</p> <ul> <li>Intra-node communication (this is via shared memory, especially <code>xpmem</code>)</li> <li>Inter-node communication (this should go through the OpenFabrics Interface - OFI)</li> <li>Host-to-Host memory communication</li> <li>Device-to-Device memory communication</li> </ul> <p>To achieve native performance MPICH must be built with both <code>libfabric</code> and <code>xpmem</code> support. Additionally, when building for GH200 nodes, one needs to ensure to build <code>libfabric</code> and <code>mpich</code> with CUDA support.</p> <p>At runtime, the container engine CXI hook will replace the libraries <code>xpmem</code> and <code>libfabric</code> inside the container, with the libraries on the host system. This will ensure native performance when doing MPI communication.</p> <p>These are example Dockerfiles that can be used on Eiger and Daint to build a container image with MPICH and best communication performance.</p> <p>They are explicit and building manually the necessary packages, however for production one can fall back to Spack to do the building.</p> Dockerfile.cpuDockerfile.gpu <pre><code>FROM docker.io/ubuntu:24.04\n\nARG libfabric_version=1.22.0\nARG mpi_version=4.3.1\nARG osu_version=7.5.1\n\nRUN apt-get update \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends build-essential ca-certificates automake autoconf libtool make gdb strace wget python3 git gfortran \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/hpc/xpmem \\\n    &amp;&amp; cd xpmem/lib \\\n    &amp;&amp; gcc -I../include -shared -o libxpmem.so.1 libxpmem.c \\\n    &amp;&amp; ln -s libxpmem.so.1 libxpmem.so \\\n    &amp;&amp; mv libxpmem.so* /usr/lib64 \\\n    &amp;&amp; cp ../include/xpmem.h /usr/include/ \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd ../../ \\\n    &amp;&amp; rm -Rf xpmem\n\nRUN wget -q https://github.com/ofiwg/libfabric/archive/v${libfabric_version}.tar.gz \\\n    &amp;&amp; tar xf v${libfabric_version}.tar.gz \\\n    &amp;&amp; cd libfabric-${libfabric_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf v${libfabric_version}.tar.gz libfabric-${libfabric_version}\n\nRUN wget -q https://www.mpich.org/static/downloads/${mpi_version}/mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; tar xf mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; cd mpich-${mpi_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --enable-fast=O3,ndebug --enable-fortran --enable-cxx --with-device=ch4:ofi --with-libfabric=/usr --with-xpmem=/usr \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf mpich-${mpi_version}.tar.gz mpich-${mpi_version}\n\nRUN wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; tar xf osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; cd osu-micro-benchmarks-v${osu_version} \\\n    &amp;&amp; ./configure --prefix=/usr/local CC=$(which mpicc) CFLAGS=-O3 \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf osu-micro-benchmarks-v${osu_version} osu-micro-benchmarks-v${osu_version}.tar.gz\n</code></pre> <pre><code>FROM docker.io/nvidia/cuda:12.8.1-devel-ubuntu24.04\n\nARG libfabric_version=1.22.0\nARG mpi_version=4.3.1\nARG osu_version=7.5.1\n\nRUN apt-get update \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends build-essential ca-certificates automake autoconf libtool make gdb strace wget python3 git gfortran \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# When building on a machine without a GPU,\n# during the build process on Daint the GPU driver and libraries are not imported into the build process\nRUN echo '/usr/local/cuda/lib64/stubs' &gt; /etc/ld.so.conf.d/cuda_stubs.conf &amp;&amp; ldconfig\n\nRUN git clone https://github.com/hpc/xpmem \\\n    &amp;&amp; cd xpmem/lib \\\n    &amp;&amp; gcc -I../include -shared -o libxpmem.so.1 libxpmem.c \\\n    &amp;&amp; ln -s libxpmem.so.1 libxpmem.so \\\n    &amp;&amp; mv libxpmem.so* /usr/lib \\\n    &amp;&amp; cp ../include/xpmem.h /usr/include/ \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd ../../ \\\n    &amp;&amp; rm -Rf xpmem\n\nRUN wget -q https://github.com/ofiwg/libfabric/archive/v${libfabric_version}.tar.gz \\\n    &amp;&amp; tar xf v${libfabric_version}.tar.gz \\\n    &amp;&amp; cd libfabric-${libfabric_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --with-cuda=/usr/local/cuda \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf v${libfabric_version}.tar.gz libfabric-${libfabric_version}\n\nRUN wget -q https://www.mpich.org/static/downloads/${mpi_version}/mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; tar xf mpich-${mpi_version}.tar.gz \\\n    &amp;&amp; cd mpich-${mpi_version} \\\n    &amp;&amp; ./autogen.sh \\\n    &amp;&amp; ./configure --prefix=/usr --enable-fast=O3,ndebug --enable-fortran --enable-cxx --with-device=ch4:ofi --with-libfabric=/usr --with-xpmem=/usr --with-cuda=/usr/local/cuda \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; ldconfig \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf mpich-${mpi_version}.tar.gz mpich-${mpi_version}\n\nRUN wget -q http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; tar xf osu-micro-benchmarks-v${osu_version}.tar.gz \\\n    &amp;&amp; cd osu-micro-benchmarks-v${osu_version} \\\n    &amp;&amp; ./configure --prefix=/usr/local --with-cuda=/usr/local/cuda CC=$(which mpicc) CFLAGS=-O3 \\\n    &amp;&amp; make -j$(nproc) \\\n    &amp;&amp; make install \\\n    &amp;&amp; cd .. \\\n    &amp;&amp; rm -rf osu-micro-benchmarks-v${osu_version} osu-micro-benchmarks-v${osu_version}.tar.gz\n\n# Get rid of the stubs libraries, because at runtime the CUDA driver and libraries will be available\nRUN rm /etc/ld.so.conf.d/cuda_stubs.conf &amp;&amp; ldconfig\n</code></pre> <p>GPU-to-GPU inter-node communication</p> <p>To make sure that GPU-to-GPU performance is good for inter-node communication one must set the variable <pre><code>$ export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n</code></pre></p> <p>Once the container is built and pushed to a registry, one can create a container environment. To verify performance, one can run the <code>osu_bw</code> benchmark, which is doing a bandwidth benchmark for different message sizes between two ranks. For reference this is the expected performance for different memory residency, with inter-node and intra-node communication:</p> CPU-to-CPU memory intra-nodeCPU-to-CPU memory inter-nodeGPU-to-GPU memory intra-nodeGPU-to-GPU memory inter-node <pre><code>$ export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n$ srun --mpi=pmi2 -t00:05:00 --environment=$PWD/osu_gpu.toml -n2 -N1 /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw H H\n# OSU MPI Bandwidth Test v7.5\n# Datatype: MPI_CHAR.\n# Size      Bandwidth (MB/s)\n1                       1.19\n2                       2.37\n4                       4.78\n8                       9.61\n16                      8.71\n32                     38.38\n64                     76.89\n128                   152.89\n256                   303.63\n512                   586.09\n1024                 1147.26\n2048                 2218.82\n4096                 4303.92\n8192                 8165.95\n16384                7178.94\n32768                9574.09\n65536               43786.86\n131072              53202.36\n262144              64046.90\n524288              60504.75\n1048576             36400.29\n2097152             28694.38\n4194304             23906.16\n</code></pre> <pre><code>$ export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n$ srun --mpi=pmi2 -t00:05:00 --environment=$PWD/osu_gpu.toml -n2 -N2 /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw H H\n# OSU MPI Bandwidth Test v7.5\n# Datatype: MPI_CHAR.\n# Size      Bandwidth (MB/s)\n1                       0.97\n2                       1.95\n4                       3.91\n8                       7.80\n16                     15.67\n32                     31.24\n64                     62.58\n128                   124.99\n256                   249.13\n512                   499.63\n1024                 1009.57\n2048                 1989.46\n4096                 3996.43\n8192                 7139.42\n16384               14178.70\n32768               18920.35\n65536               22169.18\n131072              23226.08\n262144              23627.48\n524288              23838.28\n1048576             23951.16\n2097152             24007.73\n4194304             24037.14\n</code></pre> <pre><code>$ export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n$ srun --mpi=pmi2 -t00:05:00 --environment=$PWD/osu_gpu.toml -n2 -N1 /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw D D\n# OSU MPI-CUDA Bandwidth Test v7.5\n# Datatype: MPI_CHAR.\n# Size      Bandwidth (MB/s)\n1                       0.14\n2                       0.29\n4                       0.58\n8                       1.16\n16                      2.37\n32                      4.77\n64                      9.87\n128                    19.77\n256                    39.52\n512                    78.29\n1024                  158.19\n2048                  315.93\n4096                  633.14\n8192                 1264.69\n16384                2543.21\n32768                5051.02\n65536               10069.17\n131072              20178.56\n262144              38102.36\n524288              64397.91\n1048576             84937.73\n2097152            104723.15\n4194304            115214.94\n</code></pre> <pre><code>$ export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n$ srun --mpi=pmi2 -t00:05:00 --environment=$PWD/osu_gpu.toml -n2 -N2 /usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt/osu_bw D D\n# OSU MPI-CUDA Bandwidth Test v7.5\n# Datatype: MPI_CHAR.\n# Size      Bandwidth (MB/s)\n1                       0.09\n2                       0.18\n4                       0.37\n8                       0.74\n16                      1.48\n32                      2.96\n64                      5.91\n128                    11.80\n256                   227.08\n512                   463.72\n1024                  923.58\n2048                 1740.73\n4096                 3505.87\n8192                 6351.56\n16384               13377.55\n32768               17226.43\n65536               21416.23\n131072              22733.04\n262144              23335.00\n524288              23624.70\n1048576             23821.72\n2097152             23928.62\n4194304             23974.34\n</code></pre>"},{"location":"software/communication/nccl/","title":"NCCL","text":""},{"location":"software/communication/nccl/#nccl","title":"NCCL","text":"<p>NCCL is an optimized inter-GPU communication library for NVIDIA GPUs. It is commonly used in machine learning frameworks, but traditional scientific applications can also benefit from NCCL.</p>"},{"location":"software/communication/nccl/#using-nccl","title":"Using NCCL","text":"<p>To use the Slingshot network on Alps, the <code>aws-ofi-nccl</code> plugin must be used. With the container engine, the AWS OFI NCCL hook can be used to load the plugin into the container and configure NCCL to use it.</p> <p>Most uenvs, like <code>prgenv-gnu</code>, also contain the NCCL plugin. When using e.g. the <code>default</code> view of <code>prgenv-gnu</code> the <code>aws-ofi-nccl</code> plugin will be available in the environment. Alternatively, loading the <code>aws-ofi-nccl</code> module with the <code>modules</code> view also makes the plugin available in the environment. The environment variables described below must be set to ensure that NCCL uses the plugin.</p> <p>While the container engine sets these automatically when using the NCCL hook, the following environment variables should always be set for correctness and optimal performance when using NCCL with uenv:</p> <pre><code># This forces NCCL to use the libfabric plugin, enabling full use of the\n# Slingshot network. If the plugin can not be found, applications will fail to\n# start. With the default value, applications would instead fall back to e.g.\n# TCP, which would be significantly slower than with the plugin. More information\n# about `NCCL_NET` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net\nexport NCCL_NET=\"AWS Libfabric\"\n# Use GPU Direct RDMA when GPU and NIC are on the same NUMA node. More\n# information about `NCCL_NET_GDR_LEVEL` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net-gdr-level-formerly-nccl-ib-gdr-level\nexport NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\n# Starting with nccl 2.27 a new protocol (LL128) was enabled by default, which\n# typically performs worse on Slingshot. The following disables that protocol.\nexport NCCL_PROTO=^LL128\n# These `FI` (libfabric) environment variables have been found to give the best\n# performance on the Alps network across a wide range of applications. Specific\n# applications may perform better with other values.\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_DEFAULT_TX_SIZE=32768\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_CXI_RX_MATCH_MODE=software\nexport FI_MR_CACHE_MONITOR=userfaultfd\n</code></pre> <p>Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms contains detailed information about NCCL algorithms and protocols, which can be helpful for deciding if your application could benefit from an alternative configuration.</p> <p>NCCL watchdog timeout or hanging process</p> <p>In some cases, still under investigation, NCCL may hang resulting in a stuck process or a watchdog timeout error. In this scenario, we recommend disabling Slingshot eager messages with the following workaround: <pre><code># Disable eager messages to avoid NCCL timeouts\nexport FI_CXI_RDZV_GET_MIN=0\nexport FI_CXI_RDZV_THRESHOLD=0\nexport FI_CXI_RDZV_EAGER_SIZE=0\n</code></pre></p> <p>Using NCCL with uenvs</p> <p>The environment variables listed above are not set automatically when using uenvs.</p> <p>GPU-aware MPI with NCCL</p> <p>Using GPU-aware MPI together with NCCL can easily lead to deadlocks. Unless care is taken to ensure that the two methods of communication are not used concurrently, we recommend not using GPU-aware MPI with NCCL. To explicitly disable GPU-aware MPI with Cray MPICH, explicitly set <code>MPICH_GPU_SUPPORT_ENABLED=0</code>. Note that this option may be set to <code>1</code> by default on some Alps clusters. See the Cray MPICH documentation for more details on GPU-aware MPI with Cray MPICH.</p> <p><code>invalid usage</code> error with <code>NCCL_NET=\"AWS Libfabric\"</code></p> <p>If you are getting error messages such as: <pre><code>nid006352: Test NCCL failure common.cu:958 'invalid usage (run with NCCL_DEBUG=WARN for details)\n</code></pre> this may be due to the plugin not being found by NCCL. If this is the case, running the application with the recommended <code>NCCL_DEBUG=WARN</code> should print something similar to the following: <pre><code>nid006352:34157:34217 [1] net.cc:626 NCCL WARN Error: network AWS Libfabric not found.\n</code></pre> When using uenvs like <code>prgenv-gnu</code>, make sure you are either using the <code>default</code> view which loads <code>aws-ofi-nccl</code> automatically, or, if using the <code>modules</code> view, load the <code>aws-ofi-nccl</code> module with <code>module load aws-ofi-nccl</code>. If the plugin is found correctly, running the application with <code>NCCL_DEBUG=INFO</code> should print: <pre><code>nid006352:34610:34631 [0] NCCL INFO Using network AWS Libfabric\n</code></pre></p> <p>Do not use <code>NCCL_NET_PLUGIN=\"ofi\"</code> with uenvs</p> <p>NCCL has an alternative way of specifying what plugin to use: <code>NCCL_NET_PLUGIN</code>. When using uenvs, do not set <code>NCCL_NET_PLUGIN=\"ofi\"</code> instead of, or in addition to, <code>NCCL_NET=\"AWS Libfabric\"</code>. If you do, your application will fail to start since NCCL will:</p> <ol> <li>fail to find the plugin because of the name of the shared library in the uenv, and</li> <li>prefer <code>NCCL_NET_PLUGIN</code> over <code>NCCL_NET</code>, so it will fail to find the plugin even if <code>NCCL_NET=\"AWS Libfabric\"</code> is correctly set.</li> </ol> <p>When both environment variables are set the error message, with <code>NCCL_DEBUG=WARN</code>, will look similar to when the plugin isn\u2019t available: <pre><code>nid006365:179857:179897 [1] net.cc:626 NCCL WARN Error: network AWS Libfabric not found.\n</code></pre></p> <p>With <code>NCCL_DEBUG=INFO</code>, NCCL will print: <pre><code>nid006365:180142:180163 [0] NCCL INFO NET/Plugin: Could not find: ofi libnccl-net-ofi.so. Using internal network plugin.\n...\nnid006365:180142:180163 [0] net.cc:626 NCCL WARN Error: network AWS Libfabric not found.\n</code></pre></p> <p>If you only set <code>NCCL_NET=\"ofi\"</code>, NCCL may silently fail to load the plugin but fall back to the default implementation.</p>"},{"location":"software/communication/openmpi/","title":"OpenMPI","text":""},{"location":"software/communication/openmpi/#openmpi","title":"OpenMPI","text":"<p>Cray MPICH is the recommended MPI implementation on Alps. However, OpenMPI can be used as an alternative in some cases, with limited support from CSCS.</p> <p>To use OpenMPI on Alps, it must be built against libfabric with support for the Slingshot 11 network.</p>"},{"location":"software/communication/openmpi/#using-openmpi","title":"Using OpenMPI","text":"<p>Warning</p> <p>Building and using OpenMPI on Alps is still work in progress. The instructions found on this page may be inaccurate, but are a good starting point to using OpenMPI on Alps.</p> <p>Todo</p> <p>Deploy experimental uenv.</p> <p>Todo</p> <p>Document OpenMPI uenv next to prgenv-gnu, prgenv-nvfortran, and linalg?</p> <p>OpenMPI is provided through a uenv similar to <code>prgenv-gnu</code>. Once the uenv is loaded, compiling and linking with OpenMPI and libfabric is transparent. At runtime, some additional options must be set to correctly use the Slingshot network.</p> <p>First, when launching applications through Slurm, PMIx must be used for application launching. This is done with the <code>--mpi</code> flag of <code>srun</code>: <pre><code>srun --mpi=pmix ...\n</code></pre></p> <p>Additionally, the following environment variables should be set: <pre><code>export PMIX_MCA_psec=\"native\" # (1)!\nexport FI_PROVIDER=\"cxi\" # (2)!\nexport OMPI_MCA_pml=\"^ucx\" # (3)!\nexport OMPI_MCA_mtl=\"ofi\" # (4)!\n</code></pre></p> <ol> <li>Ensures PMIx uses the same security domain as Slurm. Otherwise PMIx will print warnings at startup.</li> <li>Use the CXI (Slingshot) provider.</li> <li>Use anything except UCX for point-to-point communication. The <code>^</code> signals that OpenMPI should exclude all listed components.</li> <li>Use libfabric for the Matching Transport Layer.</li> </ol> <p>CXI provider does all communication through the network interface cards (NICs)</p> <p>When using the libfabric CXI provider, all communication goes through NICs, including intra-node communication. This means that intra-node communication can not make use of shared memory optimizations and the maximum bandwidth will not be severely limited.</p> <p>Libfabric has a new LINKx provider, which allows using different libfabric providers for inter- and intra-node communication. This provider is not as well tested, but can in theory perform better for intra-node communication, because it can use shared memory. To use the LINKx provider, set the following, instead of <code>FI_PROVIDER=cxi</code>:</p> <pre><code>export FI_PROVIDER=\"lnx\" # (1)!\nexport FI_LNX_PROV_LINKS=\"shm+cxi\" # (2)!\n</code></pre> <ol> <li>Use the libfabric LINKx provider, to allow using different libfabric providers for inter- and intra-node communication.</li> <li>Use the shared memory provider for intra-node communication and the CXI (Slingshot) provider for inter-node communication.</li> </ol>"},{"location":"software/communication/rccl/","title":"RCCL","text":""},{"location":"software/communication/rccl/#rccl","title":"RCCL","text":"<p>RCCL is an optimized inter-GPU communication library for AMD GPUs. It provides equivalent functionality to NCCL for AMD GPUs.</p> <p>Todo</p> <ul> <li>high level description</li> <li>libfabric/aws-ofi-rccl plugin</li> <li>configuration options</li> </ul> <p>Info</p> <p>RCCL uses many of the same configuration options as NCCL, with the <code>NCCL</code> prefix, not <code>RCCL</code>. Refer to NCCL documentation to tune RCCL.</p>"},{"location":"software/container-engine/","title":"Index","text":""},{"location":"software/container-engine/#container-engine","title":"Container Engine","text":"<p>The Container Engine (CE) toolset is designed to enable computing jobs to seamlessly run inside Linux application containers, thus providing support for containerized user environments.</p>"},{"location":"software/container-engine/#concept","title":"Concept","text":"<p>Containers effectively encapsulate a software stack; however, to be useful in HPC computing environments, they often require the customization of bind mounts, environment variables, working directories, hooks, plugins, etc.  To simplify this process, the Container Engine (CE) toolset supports the specification of user environments through Environment Definition Files.</p> <p>An Environment Definition File (EDF) is a text file in the TOML format that declaratively and prescriptively represents the creation of a computing environment based on a container image. Users can create their own custom environments and share, edit, or build upon already existing environments.</p> <p>The Container Engine (CE) toolset leverages its tight integration with the Slurm workload manager to parse Fs directly from the command line or batch script and instantiate containerized user environments seamlessly and transparently.</p> <p>Through the EDF, container use cases can be abstracted to the point where end users perform their workflows as if they were operating natively on the computing system.</p>"},{"location":"software/container-engine/#benefits","title":"Benefits","text":"<ul> <li>Freedom: Container gives users full control of the user space. The user can decide what to install without involving a sysadmin.</li> <li>Reproducibility: Workloads consistently run in the same environment, ensuring uniformity across job experimental runs.</li> <li>Portability: The self-contained nature of containers simplifies the deployment across architecture-compatible HPC systems.</li> <li>Seamless Access to HPC Resources: CE facilitates native access to specialized HPC resources like GPUs, interconnects, and other system-specific tools crucial for performance.</li> </ul>"},{"location":"software/container-engine/#quick-start","title":"Quick Start","text":"<p>Let\u2019s set up a containerized Ubuntu 24.04 environment on the scratch folder (<code>${SCRATCH}</code>).</p>"},{"location":"software/container-engine/#step-1-create-an-environment","title":"Step 1. Create an environment","text":"<p>Save this file below as\u00a0<code>ubuntu.toml</code> in <code>${HOME}/.edf</code> directory (the default location of EDF files). Create <code>${HOME}/.edf</code> if the folder doesn\u2019t exist. A more detailed explanation of each entry for the EDF can be seen in the\u00a0EDF reference.</p> <pre><code>image = \"library/ubuntu:24.04\"\nmounts = [\"${SCRATCH}:${SCRATCH}\"]\nworkdir = \"${SCRATCH}\"\n</code></pre>"},{"location":"software/container-engine/#step-2-launch-a-program","title":"Step 2. Launch a program","text":"<p>Use Slurm on the login node to launch a program inside the environment. Notice that the environment (EDF) is specified with the <code>--environment</code> option.  CE pulls the image automatically when the container starts.</p> <pre><code>$ srun --environment=ubuntu echo \"Hello\" \nHello\n</code></pre> <p>Or, use <code>--pty</code> to directly enter the environment.</p> <pre><code>$ srun --environment=ubuntu --pty bash\n[compute-node]$ \n</code></pre> <p>Entering the environment on Daint</p> <pre><code>[daint-ln002]$ srun --environment=ubuntu --pty bash   # (1)\n\n[nid005333]$ pwd                                      # (2)\n/capstor/scratch/cscs/&lt;username&gt;\n\n[nid005333]$ cat /etc/os-release                      # (3)\nPRETTY_NAME=\"Ubuntu 24.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\n...\n\n[nid005333]$ exit                                     # (4)\n[daint-ln002]$\n</code></pre> <ol> <li>Starting an interactive shell session within the Ubuntu 24.04 container deployed on a compute node using <code>srun --environment=ubuntu --pty bash</code>.</li> <li>Check the current folder (dubbed the working directory) is set to the user\u2019s scratch folder, as per EDF.</li> <li>Show the OS version of your container\u00a0(using <code>cat /etc/os-release</code>) based on Ubuntu 24.04 LTS.</li> <li>Exiting the container (<code>exit</code>), returning to the login node.</li> </ol>"},{"location":"software/container-engine/edf/","title":"EDF reference","text":""},{"location":"software/container-engine/edf/#edf-reference","title":"EDF reference","text":"<p>EDF files use the TOML format. For details about the data types used by the different parameters, please refer to the TOML spec webpage.</p>"},{"location":"software/container-engine/edf/#edf-entries","title":"EDF entries","text":""},{"location":"software/container-engine/edf/#base_environment","title":"<code>base_environment</code>","text":"Type array or string Default <code>\"\"</code> <p>Ordered list of EDFs that this file inherits from. Parameters from listed environments are evaluated sequentially. Supports up to 10 levels of recursion.</p> <p>Example</p> <ul> <li> <p>Single environment inheritance:     <pre><code>base_environment = \"common_env\"\n</code></pre></p> </li> <li> <p>Multiple environment inheritance:     <pre><code>base_environment = [\"common_env\", \"ml_pytorch_env1\"]\n</code></pre></p> </li> </ul> <p>Note</p> <ul> <li>Parameters from the listed environments are evaluated sequentially, adding new entries or overwriting previous ones, before evaluating the parameters from the current EDF. In other words, the current EDF inherits the parameters from the EDFs listed in <code>base_environment</code>. When evaluating <code>mounts</code> or <code>env</code> parameters, values from downstream EDFs are appended to inherited values.</li> <li>The individual EDF entries in the array follow the same search rules as the arguments of the\u00a0<code>--environment</code> CLI option for Slurm; they can be either file paths or filenames without extension if the file is located in the EDF search path.</li> <li>This parameter can be a string if there is only one base environment.</li> </ul>"},{"location":"software/container-engine/edf/#image","title":"<code>image</code>","text":"Type string Default <code>\"\"</code> <p>The container image to use. If empty, CE doesn\u2019t enter a container. Can reference a remote Docker/OCI registry or a local Squashfs file as a filesystem path.</p> <p>Example</p> <ul> <li> <p>Reference of Ubuntu image in the Docker Hub registry (default registry)     <pre><code>image = \"library/ubuntu:24.04\"\n</code></pre></p> </li> <li> <p>Explicit reference of Ubuntu image in the Docker Hub registry     <pre><code>image = \"docker.io#library/ubuntu:24.04\"\n</code></pre></p> </li> <li> <p>Reference to PyTorch image from NVIDIA Container Registry (nvcr.io)     <pre><code>image = \"nvcr.io#nvidia/pytorch:22.12-py3\"\n</code></pre></p> </li> <li> <p>Image from third-party quay.io registry     <pre><code>image = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04-arm64\"\n</code></pre></p> </li> <li> <p>Reference to a manually pulled image stored in parallel FS     <pre><code>image = \"/path/to/image.squashfs\"\n</code></pre></p> </li> </ul> <p>Note</p> <ul> <li>The full format for remote references is <code>[USER@][REGISTRY#]IMAGE[:TAG]</code>.<ul> <li><code>[REGISTRY#]</code>: (optional) registry URL, followed by\u00a0#. Default: Docker Hub.</li> <li><code>IMAGE</code>: image name.</li> <li><code>[:TAG]</code>: (optional) image tag name, preceded by\u00a0:.</li> </ul> </li> <li>The registry user can also be specified in the\u00a0<code>$HOME/.config/enroot/.credentials</code> file.</li> </ul>"},{"location":"software/container-engine/edf/#workdir","title":"<code>workdir</code>","text":"Type string Default (inherited from image) <p>Initial working directory when the container starts.</p> <p>Example</p> <ul> <li>Working directory pointing to a user defined project path\u00a0     <pre><code>workdir = \"/home/user/projects\"\n</code></pre></li> <li>Working directory pointing to the <code>/tmp</code> directory     <pre><code>workdir = \"/tmp\"\n</code></pre></li> </ul>"},{"location":"software/container-engine/edf/#entrypoint","title":"<code>entrypoint</code>","text":"Type bool Default <code>false</code> <p>If true, run the entrypoint from the container image.</p> <p>Example</p> <pre><code>entrypoint = true\n</code></pre>"},{"location":"software/container-engine/edf/#writable","title":"<code>writable</code>","text":"Type bool Default <code>true</code> <p>If false, the container filesystem is read-only.</p> <p>Example</p> <pre><code>writable = true\n</code></pre> <p></p>"},{"location":"software/container-engine/edf/#mounts","title":"<code>mounts</code>","text":"Type array Default <code>[]</code> <p>List of mounts in the format <code>SOURCE:DESTINATION[:FLAGS]</code>. By default, it performs bind mount unless the only flag is <code>sqsh</code>, in which it performs a SquashFS mount. When bind mounting, the flags are forwarded to the system mount operation (e.g., <code>ro</code> or <code>private</code>).</p> <p>Example</p> <ul> <li> <p>Literal fixed mount map     <pre><code>mounts = [\"/capstor/scratch/cscs/amadonna:/capstor/scratch/cscs/amadonna\"]\n</code></pre></p> </li> <li> <p>Mapping path with <code>env</code> variable expansion     <pre><code>mounts = [\"/capstor/scratch/cscs/${USER}:/capstor/scratch/cscs/${USER}\"]\n</code></pre></p> </li> <li> <p>Mounting the scratch filesystem using a host environment variable     <pre><code>mounts = [\"${SCRATCH}:${SCRATCH}\"]\n</code></pre></p> </li> <li> <p>Mounting a SquashFS image <code>${SCRATCH}/data.sqsh</code> to <code>/data</code> <pre><code>mounts = [\"${SCRATCH}/data.sqsh:/data:sqsh\"]\n</code></pre></p> </li> <li> <p>Mounting multiple entities (the scratch filesystem and a SquashFS image)     <pre><code>mounts = [\"${SCRATCH}:${SCRATCH}\", \"${SCRATCH}/data.sqsh:/data:sqsh\"]\n</code></pre></p> </li> </ul> <p>Note</p> <ul> <li>Mount flags are separated with a plus symbol, for example:\u00a0<code>ro+private</code>.</li> </ul>"},{"location":"software/container-engine/edf/#edf-tables","title":"EDF tables","text":""},{"location":"software/container-engine/edf/#env","title":"<code>env</code>","text":"<p>Environment variables to set in the container. Empty string values will unset the variable. Inherited from the host and the image by default.</p> <p>Example</p> <ul> <li> <p>Basic <code>env</code> block     <pre><code>[env]\nMY_RUN = \"production\"\nDEBUG = \"false\"\n</code></pre></p> </li> <li> <p>Use of environment variable expansion     <pre><code>[env]\nMY_NODE = \"${VAR_FROM_HOST}\"\nPATH = \"${PATH}:/custom/bin\"\nDEBUG = \"true\"\n</code></pre></p> </li> </ul> <p>Note</p> <ul> <li>By default, containers inherit environment variables from the container image and the host environment, with variables from the image taking precedence.</li> <li>The\u00a0env\u00a0table can be used to further customize the container environment by setting, modifying, or unsetting variables.</li> <li>Values of the table entries must be strings. If an entry has a null value, the variable corresponding to the entry key is unset in the container.</li> </ul>"},{"location":"software/container-engine/edf/#annotations","title":"<code>annotations</code>","text":"<p>OCI-like annotations for the container. For more details, refer to the Annotations section.</p> <p>Example</p> <ul> <li> <p>Disabling the CXI hook     <pre><code>[annotations]\ncom.hooks.cxi.enabled = \"false\"\n</code></pre></p> </li> <li> <p>Control of SSH hook parameters via annotation and variable expansion     <pre><code>[annotations.com.hooks.ssh]\nauthorize_ssh_key = \"/capstor/scratch/cscs/${USER}/tests/edf/authorized_keys\"\nenabled = \"true\"\n</code></pre></p> </li> <li> <p>Alternative example for usage of annotation with fixed path     <pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"/path/to/authorized_keys\"\ncom.hooks.ssh.enabled = \"true\"\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/edf/#environment-variable-expansion","title":"Environment variable expansion","text":"<p>Environment variable expansion allows for dynamic substitution of environment variable values within the EDF (Environment Definition File). This capability applies across all configuration parameters in the EDF, providing flexibility in defining container environments.</p> <ul> <li>Syntax. Use <code>${VAR}</code> to reference an environment variable <code>VAR</code>. The variable\u2019s value is resolved from the combined environment, which includes variables defined in the host and the container image, the later taking precedence.</li> <li>Scope. Variable expansion is supported across all EDF parameters. This includes EDF\u2019s parameters like <code>mounts</code>, <code>workdir</code>, <code>image</code>, etc. For example, <code>${SCRATCH}</code> can be used in <code>mounts</code> to reference a directory path.</li> <li>Undefined Variables. Referencing an undefined variable results in an error. To safely handle undefined variables, you can use the syntax <code>${VAR:-}</code>, which evaluates to an empty string if VAR is undefined.</li> <li>Preventing Expansion. To prevent expansion, use double dollar signs $$. For example,\u00a0<code>$${VAR}</code> will render as the literal string <code>${VAR}</code>.</li> <li>Limitations<ul> <li>Variables defined within the <code>[env]</code> EDF table cannot reference other entries from <code>[env]</code> tables in the same or other EDF files (e.g. the ones entered as base environments). Therefore, only environment variables from the host can be referenced.  </li> </ul> </li> <li>Environment Variable Resolution Order. The environment variables in containers are set based on the following order: <ul> <li>EDF env: Variable values as defined in EDF\u2019s <code>[env]</code> table.</li> <li>Container Image: Variables defined in the container image\u2019s environment take precedence.</li> <li>Host Environment: Environment variables defined in the host system.</li> </ul> </li> </ul>"},{"location":"software/container-engine/edf/#relative-paths-expansion","title":"Relative paths expansion","text":"<p>Relative filesystem paths can be used within EDF parameters, and will be expanded by the CE at runtime.  The paths are interpreted as relative to the working directory of the process calling the CE, not to the location of the EDF file. Relative paths should be prepended by <code>./</code>.</p>"},{"location":"software/container-engine/known-issue/","title":"Known issues","text":""},{"location":"software/container-engine/known-issue/#buffer-overflow-errors-with-long-command-strings","title":"Buffer overflow errors with long command strings","text":"<p>We are aware of an issue, as of the system update on 10th September 2025, which is causing a buffer overflow error and abrupt termination of jobs using the CE when entering very long strings as the command to execute in the Slurm job step.</p> <p>The issue presents itself with a error message similar to the following:</p> <pre><code>srun: error: nid001309: task 0: Aborted\n*** buffer overflow detected ***: terminated\n</code></pre> <p>We have identified the nature of the problem and are working towards deploying a fix.</p>"},{"location":"software/container-engine/known-issue/#compatibility-with-alpine-linux","title":"Compatibility with Alpine Linux","text":"<p>Alpine Linux is incompatible with some hooks, causing errors when used with Slurm. For example,</p> EDF: alpine.toml<pre><code>image = \"alpine:3.19\"\n</code></pre> Command-line<pre><code>$ srun -lN1 --environment=alpine echo \"abc\"\n0: slurmstepd: error: pyxis: container start failed with error code: 1\n0: slurmstepd: error: pyxis: printing enroot log file:\n0: slurmstepd: error: pyxis:     [ERROR] Failed to refresh the dynamic linker cache\n0: slurmstepd: error: pyxis:     [ERROR] /etc/enroot/hooks.d/87-slurm.sh exited with return code 1\n0: slurmstepd: error: pyxis: couldn't start container\n0: slurmstepd: error: spank: required plugin spank_pyxis.so: task_init() failed with rc=-1\n0: slurmstepd: error: Failed to invoke spank plugin stack\n</code></pre> <p>This is because some hooks (e.g., Slurm and CXI hooks) leverage <code>ldconfig</code> (from Glibc) when they bind-mount host libraries inside containers; since Alpine Linux provides an alternative\u00a0<code>ldconfig</code> (from Musl Libc), it does not work as intended by hooks. As a workaround, users may disable problematic hooks. For example,</p> EDF: alpine_workaround.toml<pre><code>image = \"alpine:3.19\"\n\n[annotations]\ncom.hooks.cxi.enabled = \"false\"\n\n[env]\nENROOT_SLURM_HOOK = \"0\"\n</code></pre> Command-line<pre><code>$ srun -lN1 --environment=alpine_workaround echo \"abc\"\nabc\n</code></pre> <p>Notice the section <code>[annotations]</code> disabling Slurm and CXI hooks.</p>"},{"location":"software/container-engine/known-issue/#using-nccl-from-remote-ssh-terminals","title":"Using NCCL from remote SSH terminals","text":"<p>We are aware of an issue when enabling both the AWS OFI NCCL hook and the SSH hook, and launching programs using NCCL from Bash sessions connected via SSH. The issue manifests with messages reporting <code>Error: network 'AWS Libfabric' not found</code>.</p> <p>In addition to setting up a server for remote connections, the SSH hook also performs actions intended to improve the user experience. One of these is creating a script to be loaded by Bash in order to propagate the container job environment variables when connecting through SSH. The script is translating the value of the <code>NCCL_NET</code> variable as <code>\"'AWS Libfabric'\"</code>, that is with additional quotes compared to the original value set by the AWS OFI NCCL hook. The quoted string induces NCCL to look for a network which is not defined, resulting in the unrecoverable error mentioned earlier.</p> <p>As a workaround, resetting the NCCL_NET variable to the correct value is effective in allowing NCCL to use the AWS OFI plugin and access the Slingshot network, e.g. <code>export NCCL_NET=\"AWS Libfabric\"</code>.</p>"},{"location":"software/container-engine/known-issue/#mounting-home-directories-when-using-the-ssh-hook","title":"Mounting home directories when using the SSH hook","text":"<p>Mounting individual home directories (usually located on the <code>/users</code> filesystem) overrides the files created by the SSH hook in <code>${HOME}/.ssh</code>, including the one which includes the authorized key entered in the EDF through the corresponding annotation. In other words, when using the SSH hook and bind mounting the user\u2019s own home folder or the whole <code>/users</code>, it is necessary to authorize manually the desired key.</p> <p>It is generally NOT recommended to mount home folders inside containers, due to the risk of exposing personal data to programs inside the container. Defining a mount related to <code>/users</code> in the EDF should only be done when there is a specific reason to do so, and the container image being deployed is trusted.</p> <p></p>"},{"location":"software/container-engine/known-issue/#why-environment-as-sbatch-is-discouraged","title":"Why <code>--environment</code> as <code>#SBATCH</code> is discouraged","text":"<p>The use of <code>--environment</code> as <code>#SBATCH</code> is known to cause unexpected behaviors and is exclusively reserved for highly customized workflows. This is because <code>--environment</code> as <code>#SBATCH</code> puts the entire SBATCH script in a container from the EDF file. The following are a few known associated issues.</p> <ul> <li> <p>Slurm availability in a container: Either Slurm components are not completely injected inside a container, or injected Slurm components do not function properly.</p> </li> <li> <p>Non-host execution context: Since the SBATCH script runs inside a container, most host resources are inaccessible by default unless EDF explicitly exposes them. Affected resources include: filesystems, devices, system resources, container hooks, etc.</p> </li> <li> <p>Nested use of <code>--environment</code>: running <code>srun --environment</code> in <code>#SBATCH --environment</code> results in double-entering EDF containers, causing unexpected errors in the underlying container runtime.</p> </li> </ul> <p>To avoid any unexpected confusion, users are advised not to use <code>--environment</code> as <code>#SBATCH</code>. If users encounter a problem while using this, it\u2019s recommended to move <code>--environment</code> from <code>#SBATCH</code> to each <code>srun</code> and see if the problem disappears.</p> <p></p>"},{"location":"software/container-engine/known-issue/#container-start-fails-with-id-cannot-find-name-for-user-id","title":"Container start fails with <code>id: cannot find name for user ID</code>","text":"<p>If your slurm job using a container fails to start with an error message similar to: <pre><code>slurmstepd: error: pyxis: container start failed with error code: 1\nslurmstepd: error: pyxis: container exited too soon\nslurmstepd: error: pyxis: printing engine log file:\nslurmstepd: error: pyxis:     id: cannot find name for user ID 42\nslurmstepd: error: pyxis:     id: cannot find name for user ID 42\nslurmstepd: error: pyxis:     id: cannot find name for user ID 42\nslurmstepd: error: pyxis:     mkdir: cannot create directory \u2018/iopsstor/scratch/cscs/42\u2019: Permission denied\nslurmstepd: error: pyxis: couldn't start container\nslurmstepd: error: spank: required plugin spank_pyxis.so: task_init() failed with rc=-1\nslurmstepd: error: Failed to invoke spank plugin stack\nsrun: error: nid001234: task 0: Exited with exit code 1\nsrun: Terminating StepId=12345.0\n</code></pre> it does not indicate an issue with your container, but instead means that one or more of the compute nodes have user databases that are not fully synchronized. If the problematic node is not automatically drained, please let us know so that we can ensure the node is in a good state. You can check the state of a node using <code>sinfo --nodes=&lt;node&gt;</code>, e.g.: <pre><code>$ sinfo --nodes=nid006886\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ndebug        up    1:30:00      0    n/a\nnormal*      up   12:00:00      1 drain$ nid006886\nxfer         up 1-00:00:00      0    n/a\n</code></pre></p>"},{"location":"software/container-engine/resource-hook/","title":"Hooks and native resources","text":""},{"location":"software/container-engine/resource-hook/#annotations","title":"Annotations","text":"<p>Annotations define arbitrary metadata for containers in the form of key-value pairs. Within the EDF, annotations are designed to be similar in appearance and behavior to those defined by the OCI Runtime Specification. Annotation keys usually express a hierarchical namespace structure, with domains separated by \u201c.\u201d\u00a0(full stop) characters.</p> <p>As annotations are often used to control hooks, they have a deep nesting level. For example, to execute the SSH hook described below, the annotation\u00a0<code>com.hooks.ssh.enabled</code> must be set to the string <code>true</code>.</p> <p>EDF files support setting annotations through the <code>annotations</code> table. This can be done in multiple ways in TOML: for example, both of the following usages are equivalent:</p> <p>TOML nest levels</p> <ul> <li> <p>In the TOML key <pre><code>[annotations]\ncom.hooks.ssh.enabled = \"true\"\n</code></pre></p> </li> <li> <p>In the TOML table name <pre><code>[annotations.com.hooks.ssh]\nenabled = \"true\"\n</code></pre></p> </li> </ul> Relevant details of the TOML format <ul> <li> <p>All property assignments belong to the section immediately preceding them (the statement in square brackets), which defines the table they refer to.</p> </li> <li> <p>Tables, on the other hand, do not automatically belong to the tables declared before them; to nest tables, their name has to list their parents using the dot notations (so the previous example defines the table <code>ssh</code> inside <code>hooks</code>, which in turn is inside <code>com</code>, which is inside <code>annotations</code>).</p> </li> <li> <p>An assignment can implicitly define subtables if the key you assign is a dotted list. As a reference, see the examples made earlier in this section, where assigning a string to the <code>com.hooks.ssh.enabled</code> attribute within the <code>[annotations]</code> table is exactly equivalent to assigning to the <code>enabled</code> attribute within the <code>[annotations.com.hooks.ssh]</code> subtable.</p> </li> <li> <p>Attributes can be added to a table only in one place in the TOML file. In other words, each table must be defined in a single square bracket section. For example, in the invalid example below, the <code>ssh</code> table was doubly defined both in the <code>[annotations]</code>\u00a0and in the <code>[annotations.com.hooks.ssh]</code> sections. See the TOML format spec for more details.</p> Valid<pre><code>[annotations.com.hooks.ssh]\nauthorize_ssh_key = \"${SCRATCH}/tests/edf/authorized_keys\"\nenabled = \"true\"\n</code></pre> Valid<pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"${SCRATCH}/tests/edf/authorized_keys\"\ncom.hooks.ssh.enabled = \"true\"\n</code></pre> Invalid<pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"${SCRATCH}/tests/edf/authorized_keys\"\n\n[annotations.com.hooks.ssh]\nenabled = \"true\"\n</code></pre> </li> </ul> <p></p>"},{"location":"software/container-engine/resource-hook/#container-hooks","title":"Container Hooks","text":"<p>Container hooks let you customize container behavior to fit system-specific needs, making them especially valuable for High-Performance Computing.</p> <ul> <li>What they do: Hooks extend container runtime functionality by enabling custom actions during a container\u2019s life cycle.</li> <li>Use for HPC: HPC systems rely on specialized hardware and fine-tuned software, unlike generic containers. Hooks bridge this gap by allowing containers to access these system-specific resources or enable custom features.</li> </ul> <p>Info</p> <p>This section outlines all hooks supported in production by the Container Engine. However, specific Alps vClusters may support only a subset or use custom configurations. For details about available features in individual vClusters, consult platform documentation or contact CSCS support.</p> <p>Note</p> <p>In the examples below, EDF files are assumed to be at <code>${EDF_PATH}</code>.</p> <p></p>"},{"location":"software/container-engine/resource-hook/#hpe-slingshot-interconnect","title":"HPE Slingshot interconnect","text":"<pre><code>com.hooks.cxi.enabled = \"true\"\n</code></pre> <p>The Container Engine provides a hook to allow containers relying on\u00a0libfabric to leverage the HPE Slingshot 11 high-speed interconnect. This component is commonly referred to as the \u201cCXI hook\u201d, taking its name from the CXI libfabric provider required to interface with Slingshot 11. The hook leverages bind-mounting the custom host libfabric library into the container (in addition to all the required dependency libraries and devices as well).</p> <p>If a libfabric library is already present in the container filesystem (for example, it\u2019s provided by the image), it is replaced with its host counterpart, otherwise the host libfabric is just added to the container.</p> <p>The hook is activated by setting the\u00a0<code>com.hooks.cxi.enabled</code> annotation, which can be defined in the EDF.</p> <p>Tip</p> <p>On most vClusters, the CXI hook for Slingshot connectivity is enabled implicitly by default or by other hooks. Therefore, entering the enabling annotation in the EDF is unnecessary in many cases.</p> <p>Note</p> <ul> <li>Due to the nature of Slingshot and the mechanism implemented by the CXI hook, container applications need to use a communication library which supports libfabric in order to benefit from usage of the hook.</li> <li>Libfabric support might have to be defined at compilation time (as is the case for some MPI implementations, like MPICH and OpenMPI) or could be dynamically available at runtime (as is the case with NCCL - see also this section for more details).</li> </ul> Comparison between with and without the CXI hook <ul> <li>Without the CXI hook</li> </ul> EDF: osu-mb-wo-cxi.toml<pre><code>image = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04-arm64\"\n\n[annotations]\ncom.hooks.cxi.enabled = \"false\"\n</code></pre> Command-line<pre><code>$ srun -N2 --mpi=pmi2 --environment=osu-mb-wo-cxi ./osu_bw\n# OSU MPI Bandwidth Test v6.2\n# Size      Bandwidth (MB/s)\n1                       0.22\n2                       0.40\n4                       0.90\n8                       1.82\n16                      3.41\n32                      6.81\n64                     13.18\n128                    26.74\n256                    11.95\n512                    38.06\n1024                   39.65\n2048                   83.22\n4096                  156.14\n8192                  143.08\n16384                  53.78\n32768                 106.77\n65536                  49.88\n131072                871.86\n262144                780.97\n524288                694.58\n1048576               831.02\n2097152              1363.30\n4194304              1279.54\n</code></pre> <ul> <li>With the CXI hook enabling access to the Slingshot high-speed network</li> </ul> EDF: osu-mb-cxi.toml<pre><code>image = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04\"\n\n[annotations]\ncom.hooks.cxi.enabled = \"true\"\n</code></pre> Command-line<pre><code>$ srun -N2 --mpi=pmi2 --environment=osu-mb-cxi ./osu_bw\n# OSU MPI Bandwidth Test v6.2\n# Size      Bandwidth (MB/s)\n1                       1.21\n2                       2.32\n4                       4.85\n8                       8.38\n16                     19.36\n32                     38.47\n64                     76.28\n128                   151.76\n256                   301.25\n512                   604.17\n1024                 1145.03\n2048                 2367.25\n4096                 4817.16\n8192                 8633.36\n16384               16971.18\n32768               18740.55\n65536               21978.65\n131072              22962.31\n262144              23436.78\n524288              23672.92\n1048576             23827.78\n2097152             23890.95\n4194304             23925.61\n</code></pre> <p>How to check the CXI provider works inside a container</p> <p>You might want to check if the CXI provider works inside a container, i.e. that the CXI hook has been correctly applied.</p> <p>You can check if the CXI provider is working using <code>fi_info</code>, usually available alongside your <code>libfabric</code> installation, within the container:</p> <pre><code>fi_info -p cxi\n</code></pre> CXI provider is working <pre><code>$ fi_info -p cxi\nprovider: cxi\n    fabric: cxi\n    domain: cxi0\n    version: 0.1\n    type: FI_EP_RDM\n    protocol: FI_PROTO_CXI\n</code></pre> CXI provider not working <pre><code>$ fi_info -p cxi\nfi_getinfo: -61 (No data available)\n</code></pre> <p></p>"},{"location":"software/container-engine/resource-hook/#aws-ofi-nccl-hook","title":"AWS OFI NCCL hook","text":"<pre><code>com.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"   # (1)\n</code></pre> <ol> <li><code>com.hooks.aws_ofi_nccl.variant</code> may vary depending on vClusters. Details below.</li> </ol> <p>The\u00a0AWS OFI NCCL plugin is a software extension that allows the NCCL\u00a0and RCCL libraries to use libfabric as a network provider and, through libfabric, to access the Slingshot high-speed interconnect. Also see NCCL and libfabric for more information on using the libraries on Alps.</p> <p>The Container Engine includes a hook program to inject the AWS OFI NCCL plugin in containers; since the plugin must also be compatible with the GPU programming software stack being used, the\u00a0<code>com.hooks.aws_ofi_nccl.variant</code> annotation is used to specify a plugin variant suitable for a given container image. At the moment of writing, 4 plugin variants are configured:\u00a0<code>cuda11</code>, <code>cuda12</code> (to be used on NVIDIA GPU nodes), <code>rocm5</code>, and <code>rocm6</code> (to be used on AMD GPU nodes alongside RCCL).</p> <p>Tip</p> <p>It implicitly enables the CXI hook, therefore exposing the Slingshot interconnect to container applications. In other words, when enabling the AWS OFI NCCL hook, it\u2019s unnecessary to also enable the CXI hook separately in the EDF.</p> <p>Note</p> <p>It sets environment variables to control the behavior of NCCL and the libfabric CXI provider for Slingshot. In particular, the <code>NCCL_NET_PLUGIN</code> variable (link) is set to force NCCL to load the specific network plugin mounted by the hook. This is useful because certain container images (for example, those from NGC repositories) might already ship with a default NCCL plugin. Other environment variables help prevent application stalls and improve performance when using GPUDirect for RDMA communication.</p> <p>EDF for the NGC PyTorch 22.12 image with Cuda 11</p> <pre><code>image = \"nvcr.io#nvidia/pytorch:22.12-py3\"\nmounts = [\"/capstor/scratch/cscs/${USER}:/capstor/scratch/cscs/${USER}\"]\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda11\"\n</code></pre> <p></p>"},{"location":"software/container-engine/resource-hook/#ssh-hook","title":"SSH hook","text":"<pre><code>com.hooks.ssh.enabled = \"true\"\ncom.hooks.ssh.authorize_ssh_key = \"&lt;public-key&gt;\"    # (1)\n</code></pre> <ol> <li>Replace <code>&lt;public-key&gt;</code> with the path to your SSH public key file.</li> </ol> <p>The SSH hook runs a lightweight, statically-linked SSH server (a build of Dropbear) inside the container. While the container is running, it\u2019s possible to connect to it from a remote host using a private key matching the public one authorized in the EDF annotation. It can be useful to add SSH connectivity to containers (for example, enabling remote debugging) without bundling an SSH server into the container image or creating ad-hoc image variants for such purposes.</p> <p>The\u00a0<code>com.hooks.ssh.authorize_ssh_key</code> annotation allows the authorization of a custom public SSH key for remote connections. The annotation value must be the absolute path to a text file containing the public key (just the public key without any extra signature/certificate). The annotation value should not be the public SSH key itself. After the container starts, it is possible to get a remote shell inside the container by connecting with\u00a0SSH to the listening port.</p> <p>By default, the server started by the SSH hook listens to port 15263, but this setting can be controlled through the <code>com.hooks.ssh.port</code> annotation in the EDF.</p> <p>Warning</p> <p>The <code>srun</code> command launching an SSH-connectable container should set the <code>--pty</code> option in order for the hook to initialize properly.</p> <p>Note</p> <p>The container must be writable (default) to use the SSH hook.</p> <p>Info</p> <p>In order to establish connections through Visual Studio Code Remote - SSH extension, the <code>scp</code> program must be available inside the container. This is required to send and establish the VS Code Server into the remote container.</p> <p>Logging into a sleeping container via SSH</p> <ul> <li> <p>On the cluster EDF: ubuntu-ssh.toml<pre><code>image = \"ubuntu:latest\"\n\n[annotations]\ncom.hooks.ssh.enabled = \"true\"\ncom.hooks.ssh.authorize_ssh_key = \"&lt;public-key&gt;\"\n</code></pre> Command-line<pre><code>$ srun --environment=ubuntu-ssh --pty sleep 30\n</code></pre></p> </li> <li> <p>On the remote shell <pre><code>$ ssh -p 15263 &lt;host-of-container&gt;\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/resource-hook/#nvidia-cuda-mps-hook","title":"NVIDIA CUDA MPS hook","text":"<pre><code>com.hooks.nvidia_cuda_mps.enabled = \"true\"\n</code></pre> <p>On several Alps vClusters, NVIDIA GPUs by default operate in \u201cExclusive process\u201d mode, that is, the CUDA driver is configured to allow only one process at a time to use a given GPU. For example, on a node with 4 GPUs, a maximum of 4 CUDA processes can run at the same time.</p> <p>In order to run multiple processes concurrently on the same GPU (one example could be running multiple MPI ranks on the same device), the NVIDIA CUDA Multi-Process Service (or MPS, for short) must be started on the compute node.</p> <p>The Container Engine provides a hook to automatically manage the setup and removal of the NVIDIA CUDA MPS components within containers. The hook can be activated by setting the <code>com.hooks.nvidia_cuda_mps.enabled</code> to the string <code>true</code>.</p> <p>Tip</p> <p>When using the NVIDIA CUDA MPS hook it is not necessary to use other wrappers or scripts to manage the Multi-Process Service, as is documented for native jobs on some vClusters.</p> <p>Note</p> <p>The container must be writable (default) to use the CUDA MPS hook.</p> <p>Using the CUDA MPS hook</p> EDF: vectoradd-cuda-mps.toml<pre><code>image = \"nvcr.io#nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04\"\n\n[annotations]\ncom.hooks.nvidia_cuda_mps.enabled = \"true\"\n</code></pre> Command-line<pre><code>$ srun -t2 -N1 -n8 --environment=vectoradd-cuda-mps /cuda-samples/vectorAdd | grep \"Test PASSED\" | wc -l\n8\n</code></pre> Available GPUs and oversubscription error without the CUDA MPS hook EDF: vectoradd-cuda.toml<pre><code>image = \"nvcr.io#nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04\"   # (1)\n</code></pre> <ol> <li>This EDF uses the CUDA vector addition sample from NVIDIA\u2019s NGC catalog.</li> </ol> Command-line<pre><code>$ nvidia-smi -L\nGPU 0: GH200 120GB (UUID: GPU-...)\nGPU 1: GH200 120GB (UUID: GPU-...)\nGPU 2: GH200 120GB (UUID: GPU-...)\nGPU 3: GH200 120GB (UUID: GPU-...)\n\n$ srun -t2 -N1 -n4 --environment=vectoradd-cuda /cuda-samples/vectorAdd | grep \"Test PASSED\"    # (1)\nTest PASSED\nTest PASSED\nTest PASSED\nTest PASSED\n\n$ srun -t2 -N1 -n5 --environment=vectoradd-cuda /cuda-samples/vectorAdd | grep \"Test PASSED\"    # (2)\nFailed to allocate device vector A (error code CUDA-capable device(s) is/are busy or unavailable)!\nsrun: error: ...\n</code></pre> <ol> <li>4 processes run successfully.</li> <li>More than 4 concurrent processes result in oversubscription errors.</li> </ol>"},{"location":"software/container-engine/resource-hook/#accessing-nvidia-gpus","title":"Accessing  NVIDIA GPUs","text":"<p>The Container Engine leverages components from the NVIDIA Container Toolkit to expose NVIDIA GPU devices inside containers. GPU device files are always mounted in containers, and the NVIDIA driver user space components are\u00a0 mounted if the <code>NVIDIA_VISIBLE_DEVICES</code> environment variable is not empty, unset or set to <code>void</code>. <code>NVIDIA_VISIBLE_DEVICES</code> is already set in container images officially provided by NVIDIA to enable all GPUs available on the host system. Such images are frequently used to containerize CUDA applications, either directly or as a base for custom images, thus in many cases no action is required to access GPUs.</p> <p>Cluster with 4 GH200 devices per node</p> EDF: cuda12.5.1.toml<pre><code>image = \"nvidia/cuda:12.5.1-devel-ubuntu24.04\"\n</code></pre> Command-line<pre><code>$ srun --environment=cuda12.5.1 nvidia-smi\nThu Oct 26 17:59:36 2023 \u00a0 \u00a0 \u00a0\u00a0\n+------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03          Driver Version: 535.129.03   CUDA Version: 12.5     |\n|--------------------------------------+----------------------+----------------------+\n| GPU  Name              Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf       Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                      |                      |               MIG M. |\n|======================================+======================+======================|\n|   0  GH200 120GB                 On  | 00000009:01:00.0 Off |                    0 |\n| N/A   24C    P0           89W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   1  GH200 120GB                 On  | 00000019:01:00.0 Off |                    0 |\n| N/A   24C    P0           87W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   2  GH200 120GB                 On  | 00000029:01:00.0 Off |                    0 |\n| N/A   24C    P0           83W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   3  GH200 120GB                 On  | 00000039:01:00.0 Off |                    0 |\n| N/A   24C    P0           85W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n\n+------------------------------------------------------------------------------------+\n| Processes:                                                                         |\n|  GPU   GI   CI        PID   Type   Process name                         GPU Memory |\n|        ID   ID                                                          Usage      |\n|====================================================================================|\n|  No running processes found                                                        |\n+------------------------------------------------------------------------------------+\n</code></pre> <p>It is possible to use environment variables to control which capabilities of the NVIDIA driver are enabled inside containers. Additionally, the NVIDIA Container Toolkit can enforce specific constraints for the container, for example, on versions of the CUDA runtime or driver, or on the architecture of the GPUs. For the full details about using these features, please refer to the official documentation: Driver Capabilities, Constraints.</p>"},{"location":"software/container-engine/run/","title":"Using the Container Engine","text":""},{"location":"software/container-engine/run/#running-containerized-environments","title":"Running containerized environments","text":"<p>Specifying the <code>--environment</code> option to the Slurm command (e.g., <code>srun</code> or <code>salloc</code>) will make it run inside the EDF environment. There are three ways to do so:</p> <ol> <li> <p>Through an absolute path: an absolute path to EDF. </p> <pre><code>$ srun --environment=${HOME}/.edf/ubuntu.toml echo \"Hello\"\n</code></pre> </li> <li> <p>Through a relative path: a path relative from the current working directory (i.e., where the Slurm command is executed). Should be prepended by <code>./</code>.</p> <pre><code>$ srun --environment=./.edf/ubuntu.toml echo \"Hello\"    # from ${HOME}. \n</code></pre> </li> <li> <p>From EDF search paths: the name of EDF in the EDF search path. Notice that in this way, <code>--environment</code> accepts the EDF filename without the <code>.toml</code> extension.</p> <pre><code>$ srun --environment=ubuntu echo \"Hello\" \n</code></pre> </li> </ol> <p>Shared container at the node-level</p> <p>For memory efficiency reasons, all Slurm tasks on an individual compute node share the same container, including its filesystem. As a consequence, any write operation to the container filesystem by one task will eventually become visible to all other tasks on the same node.</p> <p>Container start failure with <code>id: cannot find name for user ID</code></p> <p>Containers may fail to start due to user database issues on compute nodes. See this section for more details.</p>"},{"location":"software/container-engine/run/#use-from-batch-scripts","title":"Use from batch scripts","text":"<p>Use <code>--environment</code> with the Slurm command (e.g., <code>srun</code> or <code>salloc</code>):</p> <p><code>srun</code> inside a batch script with EDF</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=edf-example\n#SBATCH --time=00:01:00\n...\nsrun --environment=ubuntu cat /etc/os-release\n</code></pre> <p>Multiple Slurm commands may have different EDF environments; this is useful when a single environment is not feasible due to compatibility issues or keep EDF files modular.</p> <p><code>srun</code>s with different EDFs</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=edf-example\n#SBATCH --time=00:01:00\n...\nsrun --environment=env1 ... # (1)! \n...\nsrun --environment=env2 ... # (2)!\n</code></pre> <ol> <li>Assuming <code>env1.toml</code> is at <code>EDF_PATH</code>. See EDF search path below.</li> <li>Assuming <code>env2.toml</code> is at <code>EDF_PATH</code>. See EDF search path below.</li> </ol> <p>Specifying the <code>--environment</code> option with an <code>#SBATCH</code> option is experimental.  Such usage is discouraged as it may result in unexpected behaviors.</p> <p>Warning</p> <p>The use of <code>--environment</code> as an <code>#SBATCH</code> option is reserved for highly customized workflows, and it may result in several counterintuitive, hard-to-diagnose failures. See Why <code>--environment</code> as <code>#SBATCH</code> is discouraged for details.</p> <p></p>"},{"location":"software/container-engine/run/#edf-search-path","title":"EDF search path","text":"<p>By default, the EDFs for each user are looked up in <code>${HOME}/.edf</code>. The default EDF search path can be changed through the <code>EDF_PATH</code> environment variable. <code>EDF_PATH</code> must be a colon-separated list of absolute paths to directories, where the CE searches each directory in order. If an EDF is located in the search path, its name can be used in the <code>--environment</code> option without the <code>.toml</code> extension.</p> <p>Using <code>EDF_PATH</code> to control the default search path</p> <pre><code>$ ls ~/.edf\nubuntu.toml\n\n$ ls ~/example-project\nfedora-env.toml\n\n$ export EDF_PATH=\"$HOME/example-project\"\n\n$ srun --environment=fedora-env cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"40 (Container Image)\"\nID=fedora\n...\n</code></pre>"},{"location":"software/container-engine/run/#managing-container-images","title":"Managing container images","text":"<p>By default, images defined in the EDF as remote registry references (e.g. a Docker reference) are automatically pulled and locally cached. A cached image would be preferred to pulling the image again in later usage.</p> <p>An image cache is automatically created at <code>.edf_imagestore</code> in the user\u2019s scratch folder (i.e., <code>${SCRATCH}/.edf_imagestore</code>). Cached images are stored with the corresponding CPU architecture suffix (e.g., <code>x86</code> and <code>aarch64</code>). Remove the cached image to force re-pull.</p> <p>An alternative image store path can be specify by defining the environment variable <code>EDF_IMAGESTORE</code>. <code>EDF_IMAGESTORE</code> must be an absolute path to an existing folder. Image caching may also be disable by setting <code>EDF_IMAGESTORE</code> to <code>void</code>.</p> <p>Note</p> <ul> <li>If the CE cannot create a directory for the image cache, it operates in cache-free mode, meaning that it pulls an ephemeral image before every container launch and discards it upon termination.</li> <li>Local container images are not cached. See the section below on how to use local images in EDF.</li> </ul>"},{"location":"software/container-engine/run/#pulling-images-manually","title":"Pulling images\u00a0manually","text":"<p>To bypass any caching behavior, users can manually pull an image and directly plug it into their EDF. To do so, users may execute <code>enroot import docker://[REGISTRY#]IMAGE[:TAG]</code> to pull container images from OCI registries to the current directory.</p> <p>After the import is complete, images are available in Squashfs format in the current directory and can be used in EDFs:</p> <p>Manually pulling an <code>nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04</code> image</p> <ol> <li> <p>Pull the image.</p> <pre><code>$ cd ${SCRATCH}\n\n$ enroot import docker://nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n[INFO] Querying registry for permission grant\n[INFO] Authenticating with user: &lt;anonymous&gt;\n...\nNumber of gids 1\n    root (0)\n\n$ ls *.sqsh\nnvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh\n</code></pre> </li> <li> <p>Create an EDF referencing the pulled image.</p> <pre><code>image = \"${SCRATCH}/nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh\"\n</code></pre> </li> </ol> <p>Note</p> <p>It is recommended to save images in <code>${SCRATCH}</code> or its subdirectories before using them.</p> <p></p>"},{"location":"software/container-engine/run/#third-party-and-private-registries","title":"Third-party and private registries","text":"<p>Docker Hub is the default registry from which remote images are imported.</p> <p>Registry rate limits</p> <p>Some registries will rate limit image pulls by IP address. Since public IPs are a shared resource we recommend authenticating even for publicly available images. For example, Docker Hub applies its rate limits per user when authenticated.</p> <p>To use an image from a different registry, the corresponding registry URL has to be prepended to the image reference, using a hash character (#) as a separator:</p> <p>Using a third-party registry</p> <ul> <li>Within an EDF</li> </ul> <pre><code>image = \"nvcr.io#nvidia/nvhpc:23.7-runtime-cuda11.8-ubuntu22.04\"\n</code></pre> <ul> <li>On the command line</li> </ul> <pre><code>$ enroot import docker://nvcr.io#nvidia/nvhpc:23.7-runtime-cuda11.8-ubuntu22.04\n</code></pre> <p>To import images from private repositories, access credentials should be configured by individual users in the <code>$HOME/.config/enroot/.credentials</code> file, following the netrc file format. Using the <code>enroot import</code> documentation page as a reference:</p> <code>netrc</code> example <pre><code># NVIDIA NGC catalog (both endpoints are required)\nmachine nvcr.io login $oauthtoken password &lt;token&gt;\nmachine authn.nvidia.com login $oauthtoken password &lt;token&gt;\n\n# DockerHub\nmachine auth.docker.io login &lt;login&gt; password &lt;password&gt;\n\n# Google Container Registry with OAuth\nmachine gcr.io login oauth2accesstoken password $(gcloud auth print-access-token)\n# Google Container Registry with JSON\nmachine gcr.io login _json_key password $(jq -c '.' $GOOGLE_APPLICATION_CREDENTIALS | sed 's/ /\\\\u0020/g')\n\n# Amazon Elastic Container Registry\nmachine 12345.dkr.ecr.eu-west-2.amazonaws.com login AWS password $(aws ecr get-login-password --region eu-west-2)\n\n# Azure Container Registry with ACR refresh token\nmachine myregistry.azurecr.io login 00000000-0000-0000-0000-000000000000 password $(az acr login --name myregistry --expose-token --query accessToken  | tr -d '\"')\n# Azure Container Registry with ACR admin user\nmachine myregistry.azurecr.io login myregistry password $(az acr credential show --name myregistry --subscription mysub --query passwords[0].value | tr -d '\"')\n\n# Github.com Container Registry (GITHUB_TOKEN needs read:packages scope)\nmachine ghcr.io login &lt;username&gt; password &lt;GITHUB_TOKEN&gt;\n\n# GitLab Container Registry (GITLAB_TOKEN needs a scope with read access to the container registry)\n# GitLab instances often use different domains for the registry and the authentication service, respectively\n# Two separate credential entries are required in such cases, for example:\n# Gitlab.com\nmachine registry.gitlab.com login &lt;username&gt; password &lt;GITLAB TOKEN&gt;\nmachine gitlab.com login &lt;username&gt; password &lt;GITLAB TOKEN&gt;\n\n# ETH Zurich GitLab registry\nmachine registry.ethz.ch login &lt;username&gt; password &lt;GITLAB_TOKEN&gt;\nmachine gitlab.ethz.ch login &lt;username&gt; password &lt;GITLAB_TOKEN&gt;  \n</code></pre>"},{"location":"software/container-engine/run/#working-with-storage","title":"Working with storage","text":"<p>Directories outside a container can be mounted inside a container so that the job inside the container can read/write on them. The directories to mount should be specified in EDF with <code>mounts</code>. </p> <p>Specifying directories to mount in EDF</p> <ul> <li>Mount <code>${SCRATCH}</code> to <code>/scratch</code> inside the container</li> </ul> <pre><code>mounts = [\"${SCRATCH}:/scratch\"]\n</code></pre> <ul> <li>Mount <code>${SCRATCH}</code> to <code>${SCRATCH}</code> inside the container</li> </ul> <pre><code>mounts = [\"${SCRATCH}:${SCRATCH}\"]\n</code></pre> <ul> <li>Mount <code>${SCRATCH}</code> to <code>${SCRATCH}</code> and <code>${HOME}/data</code> to <code>${HOME}/data</code></li> </ul> <pre><code>mounts = [\"${SCRATCH}:${SCRATCH}\", \"${HOME}/data:${HOME}/data\"]\n</code></pre> <p>Note</p> <p>The source (before <code>:</code>) should be present on the cluster: the destination (after <code>:</code>) doesn\u2019t have to be inside the container.</p> <p>See the EDF reference for the full specification of the <code>mounts</code> EDF entry.</p> <p></p>"},{"location":"software/container-engine/run/#mounting-a-squashfs-image","title":"Mounting a SquashFS image","text":"<p>A SquashFS image, essentially being a compressed data archive, can also be mounted as a directory so that the image contents are readable inside the container. For this, <code>:sqsh</code> should be appended after the destination.</p> <p>Mounting a SquashFS image <code>${SCRATCH}/data.sqsh</code> to <code>/data</code></p> <pre><code>mounts = [\"${SCRATCH}/data.sqsh:/data:sqsh\"]\n</code></pre> <p>This is particularly useful if a job should read multiple data files frequently, which may cause severe file access overheads. Instead, it is recommended to pack data files into one data SquashFS image and mount it inside a container. See the \u201cmagic phrase\u201d in this documentation for creating a SquashFS image.</p>"},{"location":"software/container-engine/run/#differences-from-upstream-pyxis","title":"Differences from upstream Pyxis","text":"<p>The Container Engine currently uses a customized version of NVIDIA Pyxis to integrate containers with Slurm.</p> <p>Compared to the original, upstream Pyxis code, the following user-facing differences should be noted:</p> <p>Note</p> <p>As of September 10th, 2025, these items apply only to the Clariden and Santis vClusters.</p> <ul> <li> <p>Disabled remapping of PyTorch-related variables: upstream Pyxis automatically remaps the <code>RANK</code> and <code>LOCAL_RANK</code> environment variables used by PyTorch to match the <code>SLURM_PROCID</code> and <code>SLURM_LOCALID</code> variables, respectively, if the <code>PYTORCH_VERSION</code> variable is detected in the container\u2019s environment.   This behavior has been disabled by default.   The remapping can be reactivated by setting the annotation <code>com.pyxis.pytorch_remap_vars=\"true\"</code> in the EDF.</p> </li> <li> <p>Logging container entrypoint output through EDF annotation: by default, Pyxis hides the output of the container\u2019s entrypoint, if the latter is used.   To make the entrypoint output printed on the stdout stream of the Slurm job, upstream Pyxis provides the <code>--container-entrypoint-log</code> CLI option for <code>srun</code>.   In the Pyxis version used by the Container Engine, entrypoint output printing can also be enabled by setting the annotation <code>com.pyxis.entrypoint_log=\"true\"</code> in the EDF.</p> </li> </ul>"},{"location":"software/cw/","title":"Index","text":""},{"location":"software/cw/#climate-and-weather-software","title":"Climate and weather software","text":"<p>Under-construction</p> <p>We will add guides for more weather and climate appllications and codes in collaboration with CWP partners.</p>"},{"location":"software/cw/netcdf-tools/","title":"netcdf-tools","text":""},{"location":"software/cw/netcdf-tools/#netcdf-tools","title":"netcdf-tools","text":"<p>The <code>netcdf-tools</code> uenv provides a set of CLI tools and GUI tools frequently used in climate and weather workflows.</p> <p>The release schedule is not fixed, with new releases when required.</p> version node types system status 2025 zen2, gh200 daint, eiger, santis, clariden 2024 zen2, gh200 daint, eiger, santis, clariden DEPRECATED <p>Warning</p> <p>The <code>netcdf-tools/2024</code> version uses an old interface that won\u2019t load views correctly.</p> <p>All users of version <code>2024</code> should upgrade to <code>2025</code> as soon as possible.</p> <p>The packages exposed via the <code>default</code> and <code>modules</code> views in <code>netcdf-tools/2025:v1</code> are:</p> <ul> <li>cdo@2.5.2</li> <li>cray-mpich@8.1.32</li> <li>eccodes@2.41.0</li> <li>ferret@7.6.0<ul> <li>Only provided in Eiger because its build configuration hard-codes x86 instructions.</li> </ul> </li> <li>gcc@14.3.0</li> <li>gdal@3.11.0</li> <li>geos@3.13.1</li> <li>hdf5@1.14.6</li> <li>nco@5.3.3</li> <li>ncview@2.1.9</li> <li>netcdf-c@4.9.2</li> <li>netcdf-cxx4@4.3.1</li> <li>netcdf-fortran@4.6.1</li> <li>python@3.13.5</li> <li>udunits@2.2.28</li> </ul>"},{"location":"software/cw/netcdf-tools/#how-to-use","title":"How to use","text":"<p>Use the different views to access the software</p> the <code>netcdf</code> viewmodules <p>The simplest way to get started is to use the <code>netcdf</code> file system view, which automatically loads all of the packages when the uenv is started.</p> <p>test mpi compilers and python provided by netcdf-tools/2025</p> <pre><code># start using the netcdf view\n$ uenv start --view=netcdf netcdf-tools/2025:v1\n\n# the software is available\n$ which cdo\n/user-environment/env/netcdf/bin/cdo\n$ which gdal\n/user-environment/env/netcdf/bin/gdal\n$ gdal --version\nGDAL 3.11.0 \"Eganville\", released 2025/05/06\n</code></pre> <p>run applications directly using uenv run</p> <pre><code># run ncview without having to start a uenv session\n$ uenv run netcdf-tools/2025:v1 --view=netcdf -- ncview\n\n# create an alias that launches tools in netcdf-tools (add it to bashrc)\n$ alias ncx='uenv run --view=netcdf netcdf-tools/2025:v1 --'\n# then run commands:\n$ ncx ncview\n$ ncx cdo\n</code></pre> <p>The uenv provides modules for all of the software packages, which can be made available by using the <code>modules</code> view in  No modules are loaded when a uenv starts, and have to be loaded individually using <code>module load</code>.</p> <p>starting netcdf-tools and using the provided modules</p> <pre><code>$ uenv start netcdf-tools/2025:v1 --view=modules\n$ module avail\n---------------------------- /user-environment/modules -----------------------------\n   cdo/2.5.2            gdal/3.11.0         ncview/2.1.9            squashfs/4.6.1\n   cray-mpich/8.1.32    geos/3.13.1         netcdf-c/4.9.2          udunits/2.2.28\n   eccodes/2.41.0       hdf5/1.14.6         netcdf-cxx4/4.3.1\n   ferret/7.6.0         libfabric/1.22.0    netcdf-fortran/4.6.1\n   gcc/14.3.0           nco/5.3.3           python/3.13.5\n$ module load gdal\n$ gdal --version\nGDAL 3.11.0 \"Eganville\", released 2025/05/06\n</code></pre>"},{"location":"software/cw/wrf/","title":"WRF","text":""},{"location":"software/cw/wrf/#wrf","title":"WRF","text":"<p>WRF is user software on Alps. This guide is provided based on our experiences helping users \u2013 however we can\u2019t provide the same level of support as we do for supported software. See the main applications page for more information.</p> <p>The Weather Research &amp; Forecasting Model (WRF) is a numerical weather prediction system designed for both atmospheric research and weather forecasting.</p> <p>It is used for CPU-only simulation on Eiger, for which this guide applies.</p>"},{"location":"software/cw/wrf/#using-spack","title":"Using Spack","text":"<p>Spack provides the wrf package, which we can install using the uenv-spack tool.</p> <p>First create a working directory where you will install the software. Here, we create it in your project\u2019s Store path, where the package can be accessed by all users in your project. <pre><code>mkdir $STORE/wrf\ncd $STORE/wrf\n</code></pre></p> <p>Then follow the steps in the uenv-spack guide to install <code>uenv-spack</code></p> <pre><code>git clone https://github.com/eth-cscs/uenv-spack.git\n(cd uenv-spack &amp;&amp; ./bootstrap)\n</code></pre> <p>Warning</p> <p>Run the step of downloading and running the bootstrap script for <code>uenv-spack</code> before starting a uenv.</p> <p>The <code>prgenv-gnu</code> uenv is suitable for building WRF. <pre><code>uenv start prgenv-gnu/24.11:v2 --view=spack\n</code></pre> In this example we use the latest version of <code>prgenv-gnu</code> on Eiger at the time of writing\u2014check the <code>prgenv-gnu</code> guide for the latest version.</p> <pre><code># build the latest version provided by the version of Spack used by prgenv-gnu\n$ uenv-spack/uenv-spack $PWD/build --uarch=zen2 --specs=wrf\n\n# build a specific version\n$ uenv-spack/uenv-spack $PWD/build --uarch=zen2 --specs=wrf@4.5.2\n\n# build a specific version with WRF-Chem enabled\n$ uenv-spack/uenv-spack $PWD/build --uarch=zen2 --specs=wrf@4.5.2 +chem\n</code></pre> <p>Note</p> <p>See the wrf Spack package documentation for information about options that can be enabled or disabled.</p> <p>Finally, it is time to build the software:</p> <pre><code>cd build\n./build\n</code></pre> <p>This will take 30-60 minutes, while Spack builds some dependencies then WRF.</p>"},{"location":"software/cw/wrf/#using-the-spack-installation","title":"Using the Spack installation","text":"<p>The installation creates a module file in the <code>wrf/build</code> path, that you created. Assuming you have installed it in the <code>$STORE</code> path for your project, add the following to the top of your sbatch script:</p> <pre><code>#SBATCH --uenv=prgenv-gnu/24.11:v2\n\nmodule use $STORE/wrf/build/modules\nmodule load wrf\n</code></pre> <p>Modules installed by Spack</p> <p>Spack creates a module for every installed package: <pre><code>$ module use $STORE/wrf/build/modules\n$ module avail\n\n------------------ /capstor/store/cscs/cscs/csstaff/wrf/build/modules ------------------\n   boost/1.86.0             kokkos-tools/develop    netlib-scalapack/2.2.0\n   cmake/3.30.5             kokkos/4.4.01           ninja/1.12.1\n   cray-mpich/8.1.30        libtree/3.1.1           openblas/0.3.28\n   fftw/3.3.10              lua/5.4.6               osu-micro-benchmarks/5.9\n   fmt/11.0.2               lz4/1.10.0              python/3.12.5\n   gcc/13.3.0               meson/1.5.1             superlu/5.3.0\n   gsl/2.8                  netcdf-c/4.9.2          wrf/4.6.1\n   hdf5/1.14.5              netcdf-cxx/4.2          zlib-ng/2.2.1\n   kokkos-kernels/4.4.01    netcdf-fortran/4.6.1\n\n$ module load wrf\n$ which wrf.exe\n/capstor/store/cscs/cscs/csstaff/wrf/build/store/linux-sles15-zen2/gcc-13.3.0/wrf-4.6.1-owj2dsfeslzkulaobdqbad4kh6ojh6n5/main/wrf.exe\n</code></pre></p>"},{"location":"software/cw/wrf/#cryowrf","title":"CRYOWRF","text":"<p>CRYOWRF is a coupled atmosphere-snow cover model with WRF acting as the atmospheric core and SNOWPACK acting as snow cover model.</p> <p>Building CRYOWRF is a three step process:</p> <ol> <li>install the dependencies like <code>parallel-netcdf</code></li> <li>build the SNOWPACK extension</li> <li>Build the bundled WRF</li> </ol> <p>Note</p> <p>This workflow was developed in July 2025 using the most recent commit <code>8f83858f</code> of CRYOWRF (committed in August 2023).</p> <p>It isn\u2019t very easy to install, and we have tried to streamline the process as much as possible, so take your time and follow the instructions closely.</p> <p>Eiger only</p> <p>This guide is for building on Eiger, which is an x86-based system.</p> <p>Building on the Grace-Hopper clusters like Daint is </p> <p>We use <code>prgenv-gnu/24.11:v2</code> uenv.</p>"},{"location":"software/cw/wrf/#step-1-install-required-packages","title":"Step 1: install required packages","text":"Full script <p>Here is the full script for the steps described in this step.</p> <pre><code># the root directory under which all dependencies and WRF/CRYOWRF will be installed\nexport WRFROOT=$STORE/wrf\n# the directory where we will install the dependencies of WRF/CRYOWRF/WPS\nexport WRFDEPS=$WRFROOT/dependencies\n\n# assume that we are crating WRFROOT for the first time\nmkdir $WRFROOT\n\n# download and install uenv-spack\ngit clone https://github.com/eth-cscs/uenv-spack.git $WRFROOT/uenv-spack\n(cd $WRFROOT/uenv-spack &amp;&amp; ./bootstrap)\n\n# start the uenv with the spack view enabled\n# warning: bootstrap the uenv-spack tool _before_ starting the uenv\nuenv start prgenv-gnu/24.11:v2 --view=spack\n\n# configure the environment and build it\n$WRFROOT/uenv-spack/uenv-spack $WRFDEPS --uarch=zen2 --specs='parallel-netcdf,jasper~shared,libpng,zlib-ng'\ncd $WRFDEPS\n./build\n\n# finish the session\nexit\n</code></pre> <p>The first step is to create an empty directory where everything will be installed. Here, we create it in your project\u2019s Store path, where the package can be accessed by all users in your project. <pre><code># the root directory under which all dependencies and WRF/CRYOWRF will be installed\nexport WRFROOT=$STORE/wrf\n# the directory where we will install the dependencies of WRF/CRYOWRF/WPS\nexport WRFDEPS=$WRFROOT/dependencies\n# assume that we are crating WRFROOT for the first time\nmkdir $WRFROOT\n\ncd $WRFROOT\n</code></pre></p> <p>The following dependencies that are not provided by <code>prgenv-gnu</code> are required:</p> <ul> <li><code>parallel-netcdf</code>: used by WRF.</li> <li><code>jasper~shared</code>: used by WPS (<code>~shared</code> will build static libraries, required by WPS).</li> <li><code>zlib-ng</code> and <code>libpng</code>: used by WPS.</li> </ul> <p>Then follow the steps in the uenv-spack guide to install <code>uenv-spack</code>, which will be used to install the dependencies</p> <pre><code># download and install uenv-spack\ngit clone https://github.com/eth-cscs/uenv-spack.git $WRFROOT/uenv-spack\n(cd $WRFROOT/uenv-spack &amp;&amp; ./bootstrap)\n</code></pre> <p>Warning</p> <p>Run the step of downloading and running the bootstrap script for <code>uenv-spack</code> before starting a uenv.</p> <p>Now we configure and build the environment, note that the final \u201cbuild\u201d phase will take a around 5-10 minutes.</p> <pre><code># start the uenv with the spack view enabled\nuenv start prgenv-gnu/24.11:v2 --view=spack\n\n$WRFROOT/uenv-spack/uenv-spack $WRFDEPS --uarch=zen2 --specs='parallel-netcdf,jasper~shared,libpng,zlib-ng'\ncd $WRFDEPS\n./build\n</code></pre> <p>Now the dependencies are installed, finish the uenv spack session:</p> <pre><code>exit\n</code></pre> <p>Note</p> <p>This step is performed once, and will install the software in <code>$WRFDEPS</code>, where they can be used to build and run WRF.</p>"},{"location":"software/cw/wrf/#step-2-build-snowpack","title":"Step 2: build SNOWPACK","text":"Full script <p>Here is the full script, that is described in detail in this step.</p> <pre><code>uenv start --view-default prgenv-gnu/24.11:v2\n\n# set the paths to match those used in Step 1\nexport WRFROOT=$STORE/wrf\nexport WRFDEPS=$WRFROOT/dependencies\n\ngit clone https://gitlabext.wsl.ch/atmospheric-models/CRYOWRF.git $WRFROOT/CRYOWRF\ncd $WRFROOT/CRYOWRF\n\nexport NETCDF=/user-environment/env/default\nexport HDF5=/user-environment/env/default\nexport PNETCDF=$WRFDEPS/view\n\nexport WRF_EM_CORE=1\nexport WRF_NMM_CORE=0\nexport WRF_DA_CORE=0\n\nexport WRF_CHEM=0\nexport WRF_KPP=0\n\nexport NETCDF4=1\nexport WRFIO_NCD_LARGE_FILE_SUPPORT=1\nexport WRFIO_NCD_NO_LARGE_FILE_SUPPORT=0\n\nexport CC=mpicc\nexport FC=mpifort\nexport CXX=mpic++\n\n./clean.sh\n./compiler_snow_libs.sh\n\n# finish the session\nexit\n</code></pre> <p>Use the <code>default</code> view of <code>prgenv-gnu</code> to build SNOWPACK, WRF and WPS:</p> <pre><code># set the paths to match those used in Step 1\nexport WRFROOT=$STORE/wrf\nexport WRFDEPS=$WRFROOT/dependencies\n\nuenv start prgenv-gnu/24.11:v2 --view=default\n</code></pre> <p>Note</p> <p>You don\u2019t need to load any modules: the <code>default</code> view will add everything to your environment.</p> <p>First download the CRYOWRF software:</p> <pre><code>git clone https://gitlabext.wsl.ch/atmospheric-models/CRYOWRF.git $WRFROOT/CRYOWRF\ncd $WRFROOT/CRYOWRF\n</code></pre> <p>Set the following environment variables:</p> <pre><code>export NETCDF=/user-environment/env/default\nexport HDF5=/user-environment/env/default\nexport PNETCDF=$WRFDEPS/view\n\nexport WRF_EM_CORE=1\nexport WRF_NMM_CORE=0\nexport WRF_DA_CORE=0\n\nexport WRF_CHEM=0\nexport WRF_KPP=0\n\nexport NETCDF4=1\nexport WRFIO_NCD_LARGE_FILE_SUPPORT=1\nexport WRFIO_NCD_NO_LARGE_FILE_SUPPORT=0\n\nexport CC=mpicc\nexport FC=mpifort\nexport CXX=mpic++\n</code></pre> <p>Then compile SNOWPACK:</p> <pre><code>./clean.sh\n./compiler_snow_libs.sh\n</code></pre>"},{"location":"software/cw/wrf/#step-3-build-wrf","title":"Step 3: build WRF","text":"Full script <p>Here is the full script, that is described in detail in this step.</p> <pre><code>uenv start --view-default prgenv-gnu/24.11:v2\n\n# set the paths to match those used in Step 1\nexport WRFROOT=$STORE/wrf\nexport WRFDEPS=$WRFROOT/dependencies\n\n# required for the CRYOWRF build to find SNOWPACK built in step 1\nexport SNOWLIBS=$WRFROOT/CRYOWRF/snpack_for_wrf\n\n# set variables used by the WRF build tool\nexport NETCDF=/user-environment/env/default\nexport HDF5=/user-environment/env/default\nexport PNETCDF=$WRFDEPS/view\n\nexport WRF_EM_CORE=1\nexport WRF_NMM_CORE=0\nexport WRF_DA_CORE=0\n\nexport WRF_CHEM=0\nexport WRF_KPP=0\n\nexport NETCDF4=1\nexport WRFIO_NCD_LARGE_FILE_SUPPORT=1\nexport WRFIO_NCD_NO_LARGE_FILE_SUPPORT=0\n\nexport CC=mpicc\nexport FC=mpifort\nexport CXX=mpic++\n\ngit clone https://gitlabext.wsl.ch/atmospheric-models/CRYOWRF.git $WRFROOT/CRYOWRF\ncd $WRFROOT/CRYOWRF/WRF\n\n./clean -a\n# [choose option 34][nesting: choose option 1] when prompted by configure\n./configure\n\n# edit configure.wrf\nvim configure.wrf\n# SFC             =    gfortran\n# SCC             =    gcc\n# CCOMP           =    gcc\n# DM_FC           =    mpif90\n# DM_CC           =    mpicc\n# FC              =    mpif90\n# FCBASEOPTS      =    $(FCBASEOPTS_NO_G) $(FCDEBUG) -fallow-argument-mismatch -fallow-invalid-boz -g\n# NETCDFPATH      =    /user-environment/env/default\n\nsed -i 's|hdf5hl|hdf5_hl|g' configure.wrf\n./compile em_real -j 64 &amp;&gt; log_compile\n</code></pre> <p>The CRYOWRF repository includes a copy of WRF v4.2.1, that has been modified to integrate the SNOWPACK extension build in the previous step.</p> <pre><code>export SNOWLIBS=$WRFROOT/CRYOWRF/snpack_for_wrf\ncd $WRFROOT/CRYOWRF/WRF\n./clean -a\n# [choose option 34][nesting: choose option 1] when prompted by configure\n./configure\n</code></pre> <p>Set <code>SNOWLIBS</code></p> <p>The <code>SNOWLIBS</code> environment variable needs to be set so that WRF can find the extension we compiled earlier.</p> <p>Open the configure.wrf file that was generated by calling <code>./configure</code>, and update the following lines:</p> <pre><code>SFC             =    gfortran\nSCC             =    gcc\nCCOMP           =    gcc\nDM_FC           =    mpif90\nDM_CC           =    mpicc\nFC              =    mpif90\nFCBASEOPTS      =    $(FCBASEOPTS_NO_G) $(FCDEBUG) -fallow-argument-mismatch -fallow-invalid-boz -g\nNETCDFPATH      =    /user-environment/env/default\n</code></pre> <p>And apply the following \u201cpatch\u201d: <pre><code>sed -i 's|hdf5hl|hdf5_hl|g' configure.wrf\n</code></pre></p> <p>Now compile WRF, which will take a while:</p> <pre><code>./compile em_real -j 64 &amp;&gt; log_compile\n</code></pre> <p>The compilation output is captured in <code>log_compile</code>. On success, the log should have the message <code>Executables successfully built</code>:</p> <pre><code>$ tail -n14 log_compile\n\n==========================================================================\nbuild started:   Thu 10 Jul 2025 04:54:53 PM CEST\nbuild completed: Thu 10 Jul 2025 05:17:41 PM CEST\n\n---&gt;                  Executables successfully built                  &lt;---\n\n-rwxr-xr-x 1 bcumming csstaff 121952104 Jul 10 17:16 main/ndown.exe\n-rwxr-xr-x 1 bcumming csstaff 121728120 Jul 10 17:17 main/real.exe\n-rwxr-xr-x 1 bcumming csstaff 120519144 Jul 10 17:17 main/tc.exe\n-rwxr-xr-x 1 bcumming csstaff 141159472 Jul 10 17:14 main/wrf.exe\n\n==========================================================================\n</code></pre>"},{"location":"software/cw/wrf/#step-4-build-wps","title":"Step 4: build WPS","text":"Full script <p>Here is the full script, that is described in detail in this step.</p> <pre><code>uenv start --view-default prgenv-gnu/24.11:v2\n\n# set the paths to match those used in Step 1\nexport WRFROOT=$STORE/wrf\nexport WRFDEPS=$WRFROOT/dependencies\n\n# set variables used by the WRF build tool\nexport NETCDF=/user-environment/env/default\nexport HDF5=/user-environment/env/default\nexport PNETCDF=$WRFDEPS/view\nexport JASPERLIB=$WRFDEPS/view/lib64\nexport JASPERINC=$WRFDEPS/view/include\n\nexport CC=mpicc\nexport FC=mpifort\nexport CXX=mpic++\n\ncd $WRFROOT/CRYOWRF/WPS-4.2\n./configure # choose option 1\n\n# edit configure.wrf to ensure the following variables are set\nvim ./configure.wrf\n\n# SFC                 = gfortran\n# SCC                 = gcc\n# DM_FC               = mpif90\n# DM_CC               = mpicc\n# FC                  = gfortran\n# CC                  = gcc\n# LD                  = $(FC)\n# FFLAGS              = -ffree-form -O -fconvert=big-endian -frecord-marker=4 -fallow-argument-mismatch -fallow-invalid-boz\n# F77FLAGS            = -ffixed-form -O -fconvert=big-endian -frecord-marker=4 -fallow-argument-mismatch -fallow-invalid-boz\n\n./compile &amp;&gt; log_compile\n</code></pre> <p>Using the same environment as above</p> <pre><code>export WRFDEPS=$WRFROOT/dependencies\n\nexport NETCDF=/user-environment/env/default\nexport HDF5=/user-environment/env/default\nexport PNETCDF=$WRFDEPS/view\nexport JASPERLIB=$WRFDEPS/view/lib64\nexport JASPERINC=$WRFDEPS/view/include\n\ncd $WRFROOT/CRYOWRF/WPS-4.2\n./configure # choose option 1\n</code></pre> <p>Update <code>configure.wps</code> as follows: <pre><code>SFC                 = gfortran\nSCC                 = gcc\nDM_FC               = mpif90\nDM_CC               = mpicc\nFC                  = gfortran\nCC                  = gcc\nLD                  = $(FC)\nFFLAGS              = -ffree-form -O -fconvert=big-endian -frecord-marker=4 -fallow-argument-mismatch -fallow-invalid-boz\nF77FLAGS            = -ffixed-form -O -fconvert=big-endian -frecord-marker=4 -fallow-argument-mismatch -fallow-invalid-boz\n</code></pre></p> <p>Note the arguments <code>-fallow-argument-mismatch -fallow-invalid-boz</code> added to <code>FFLAGS</code> and <code>F77FLAGS</code>.</p> <p>Then compile: <pre><code>./compile &amp;&gt; log_compile\n</code></pre></p>"},{"location":"software/cw/wrf/#running-cryowrf","title":"Running CRYOWRF","text":"<p>Add the following to your SBATCH job script: <pre><code>#SBATCH --uenv=prgenv-gnu/24.11:v2\n#SBATCH --view=default\n\n# set LD_LIBRARY_PATH to find the dependencies installed in step 1\nexport WRFROOT=$STORE/wrf\nexport WRFDEPS=$WRFROOT/dependencies\nexport LD_LIBRARY_PATH=$WRFDEPS/view/lib:$WRFDEPS/view/lib64:$LD_LIBRARY_PATH\n\n# set WRF variables\nexport WRF_EM_CORE=1\nexport WRF_NMM_CORE=0\nexport WRF_DA_CORE=0\n\nexport WRF_CHEM=0\nexport WRF_KPP=0\n\nexport NETCDF4=1\nexport WRFIO_NCD_LARGE_FILE_SUPPORT=1\nexport WRFIO_NCD_NO_LARGE_FILE_SUPPORT=0\n\n# set other environment variables\n\n# then run wrf.exe\nwrf.exe\n</code></pre></p>"},{"location":"software/devtools/","title":"Index","text":""},{"location":"software/devtools/#debugging-and-performance-analysis-tools","title":"Debugging and Performance Analysis tools","text":"<p>Debugging and performance analysis tools can assist users in developing and optimizing scientific parallel applications, especially in a high-performance computing (HPC) environment. These tools can significantly improve workflows and save valuable computational resources.</p> <p>CSCS provides debuggers and performance analysis tools on Alps clusters.</p> <p>Get in touch</p> <p>If you have issues or questions about debugging or performance analysis tools, please do not hesitate to contact us.</p> <p></p>"},{"location":"software/devtools/#debugging","title":"Debugging","text":"<p>Parallel debugging tools designed for parallel and distributed applications can help you diagnose issues and verify the correctness of your code - whether you are using MPI, OpenMP, or accelerated programming models.</p> <p>Learning to debug a code effectively will not only help you quickly resolve issues but also build a deeper understanding of how your code interacts with the underlying hardware. In this section we introduce the various debugging tools available at CSCS.</p> <ul> <li>Linaro Forge DDT</li> </ul> <p></p>"},{"location":"software/devtools/#performance-analysis","title":"Performance Analysis","text":"<p>Performance analysis tools are essential to gain insight into how an application leverages a distributed system with CPUs and GPUs and should be integrated to the development and optimization workflow of the application. This ensures that computational resources are utilized to their fullest potential.</p> <p>Learning to analyze the performance of an applications effectively is crucial to build a deeper understanding of how your code interacts with the underlying hardware. In this section we introduce the various performance analysis solutions available at CSCS.</p> <ul> <li>NVIDIA Nsight Developer Tools</li> <li>Linaro Forge MAP</li> <li>VI-HPS Tools</li> </ul>"},{"location":"software/devtools/linaro-ddt/","title":"Linaro debugger","text":""},{"location":"software/devtools/linaro-ddt/#linaro-ddt","title":"Linaro DDT","text":"<p>DDT allows source-level debugging of Fortran, C, C++ and Python codes. It can be used for debugging serial, multi-threaded (OpenMP), multi-process (MPI), and accelerated (CUDA, OpenACC) programs running on research and production systems, including the CSCS Alps system. DDT can be executed either with its graphical user interface or from the command-line.</p> <p>Note</p> <p>Linaro DDT is provided in the <code>linaro-forge</code> uenv. Before using DDT, please read the <code>linaro-forge</code> uenv documentation, which explains how to download and set up the latest version.</p>"},{"location":"software/devtools/linaro-ddt/#user-guide","title":"User guide","text":"<p>The following guide will walk through the steps required to build and debug an application using DDT.</p>"},{"location":"software/devtools/linaro-ddt/#set-up-the-user-environment-and-build-the-executable","title":"Set up the user environment and build the executable","text":"<p>Once the uenv is loaded and activated, the program to debug must be compiled with the <code>-g</code> (for CPU) and <code>-G</code> (for GPU) debugging flags. For example, we can build a CUDA test with a user environment:</p> <pre><code>uenv start prgenv-gnu:24.11:v1 --view=default\nnvcc -c -arch=sm_90 -g -G test_gpu.cu\nmpicxx -g test_cpu.cpp test_gpu.o -o myexe\n</code></pre>"},{"location":"software/devtools/linaro-ddt/#launch-linaro-ddt","title":"Launch Linaro DDT","text":"<p>To use the DDT client with uenv, it must be launched in <code>Manual Launch</code> mode (assuming that it is connected to Alps via <code>Remote Launch</code>):</p> On local machineOn Alps <p>Start DDT, and connect to the target cluster using the drop down menu for <code>Remote Launch</code>. If you don\u2019t have a target cluster, the <code>linaro-forge</code> uenv documentation explains how to set up the connection the first time.</p> <p>Click on <code>Manual launch</code>, set the number of processes to listen to, then wait for the Slurm job to start  (see the \u201con Alps\u201d tab for how to start the Slurm job).</p> <p></p> <p>Log into the system and launch with the <code>srun</code> command:</p> <pre><code>$ uenv start prgenv-gnu/24.11:v1,linaro-forge/24.1.1:v1 --view=prgenv-gnu:default # (1)!\n$ source /user-tools/activate\n$ srun -N1 -n4 -t15 -pdebug ./cuda_visible_devices.sh   ddt-client   ./myexe\n</code></pre> <ol> <li>Start a session with both the uenv used to build your application and the <code>linaro-forge</code> uenv mounted.</li> </ol>"},{"location":"software/devtools/linaro-ddt/#start-debugging","title":"Start debugging","text":"<p>By default, DDT will pause execution on the call to <code>MPI_Init</code>: </p> <p>There are two mechanisms for controlling program execution:</p> BreakpointStop at <p>Breakpoint(s) can be set by clicking in the margin to the left of the line number:</p> <p></p> <p>Execution can be paused in every CUDA kernel launch by activating the default breakpoints from the <code>Control</code> menu:</p> <p></p> Debugging with 128 GPUs <p>This screenshot shows a debugging session on 128 GPUs:</p> <p></p> <p>More informations regarding how to use Linaro DDT are provided in the Forge User Guide.</p>"},{"location":"software/devtools/linaro-ddt/#troubleshooting","title":"Troubleshooting","text":"<p>See the troubleshooting guide for the <code>linaro-forge</code> uenv.</p>"},{"location":"software/devtools/linaro-map/","title":"Linaro performance analysis tool","text":""},{"location":"software/devtools/linaro-map/#linaro-forge-map-and-performance-reports","title":"Linaro Forge MAP and Performance Reports","text":"<p>Linaro MAP can be used for profiling serial, multi-threaded (OpenMP), multi-process (MPI) and accelerated (Cuda, OpenACC) programs running on research and production systems, including the CSCS Alps system. MAP can be executed either with its graphical user interface or from the command-line. In the first case, the user can set the profiling configuration using the GUI and then see the results. In the latter (recommended) case, the user can use the MAP executable to launch the application they want to profile which will generate a report file that can then be opened from the locally installed client.</p> <p>Note</p> <p>Linaro MAP is provided in the <code>linaro-forge</code> uenv. Before using MAP, please read the <code>linaro-forge</code> uenv documentation, which explains how to download and set up the latest version.</p>"},{"location":"software/devtools/linaro-map/#linaro-forge-map","title":"Linaro Forge MAP","text":"<p>We will focus here on the profiling using MAP from the CLI but the same configuration applies in the other case as well. To debug an MPI application on Alps the following script is necessary:</p> <pre><code>&gt; map -n &lt;num_of_procs&gt; --mpi=slurm --mpiargs=\"&lt;slurm_arguments&gt;\" \\\n  --profile &lt;executable&gt; &lt;executable_arguments&gt;\n</code></pre> <p>This will generate a profile report in a binary file with suffix <code>.map</code>.</p> <p>To open this file we can open the Linaro Forge Client on our local machine, navigate to the <code>Linaro MAP</code> tab, connect to the corresponding <code>Remote</code> and then select <code>LOAD PROFILE DATA FILE</code> to locate the file.</p> <p>After loading the report file we will be in the home of Linaro MAP.</p> <p></p>"},{"location":"software/devtools/linaro-map/#linaro-forge-performance-reports","title":"Linaro Forge Performance Reports","text":"<p>Linaro MAP also allows the generation of a high level Performance Report in HTML format that shows key metrics of the profiled application. To see this we can click in the toolbar <code>Reports &gt; View HTML Performance Report in browser</code>.</p> <p>This will look like the following:</p> <p>More informations regarding how to use Linaro MAP and Performance Reports are provided in the Forge User Guide.</p>"},{"location":"software/devtools/linaro-map/#troubleshooting","title":"Troubleshooting","text":"<p>See the troubleshooting guide for the <code>linaro-forge</code> uenv.</p>"},{"location":"software/devtools/linaro-uenv/","title":"Linaro uenv","text":""},{"location":"software/devtools/linaro-uenv/#linaro-forge","title":"Linaro Forge","text":"<p>Linaro Forge is a suite of profiling and debugging tools, that includes the DDT debugger and the MAP performance analysis tool.</p> <p>Linaro DDT debugger and Linaro MAP performance analysis tool</p> <p>We have separate user guides for the tools provided by the <code>linaro-forge</code> uenv. The documentation here shows how to download the uenv, and how to set up your environment.</p> <p>Once you are set up, follow the specific guides:</p> <ul> <li>DDT debugger,</li> <li>MAP performance analysis tool.</li> </ul>"},{"location":"software/devtools/linaro-uenv/#quickstart-guide","title":"Quickstart guide","text":"<p>The Linaro uenv is named <code>linaro-forge</code>, and the available versions can be determined using the <code>uenv image find</code> command, as explained in the uenv documentation.</p> Finding available <code>linaro-forge</code> versions <pre><code>$ uenv image find linaro-forge\nuenv                    arch   system  id                size(MB)  date\nlinaro-forge/24.1.2:v1  gh200  daint   b9c8487cf183a16a     365    2025-04-15\n\n$ uenv image pull linaro-forge/24.1.2:v1\npulling b9c8487cf183a16a 100.00% --- 365/365 (0.00 MB/s)\n</code></pre> <p>This uenv is configured to be mounted in the <code>/user-tools</code> path so that they can be used alongside application and development uenv mounted at <code>/user-environment</code>.</p> <p>When using alongside another uenv, start a uenv session with both uenv. In the following example, the <code>prgenv-gnu</code> and <code>linaro-forge</code> uenv will be mounted at <code>/user-environment</code> and <code>/user-tools</code>  respectively:</p> <pre><code>$ uenv start prgenv-gnu/24.11:v2,linaro-forge/24.1.2:v1 \\\n    --view=prgenv-gnu:default # (1)!\n$ source /user-tools/activate\n\n$ uenv status # (2)!\n\n$ ddt --version # (3)!\nLinaro DDT Part of Linaro Forge.\nCopyright (c) 2023-2025 Linaro Limited. All rights reserved.\nVersion: 24.1.2\n</code></pre> <ol> <li>Loading the <code>activate</code> script instead of loading a view for the linaro-forge uenv allows to keep using the compilers from the prgenv-gnu uenv.</li> <li>Test that everything has been mounted correctly by looking at <code>uenv status</code>.    There will be warnings if there are problems.</li> <li>Check that the DDT debugger is in the path.</li> </ol> <p>Note</p> <p>The <code>linaro-forge</code> uenv is always mounted at the <code>/user-tools</code> mount point, and a script <code>/user-tools/activate</code> is provided to load both ddt and map into your environment, without needing to use a view.</p>"},{"location":"software/devtools/linaro-uenv/#install-and-configure-the-linaro-client-on-your-local-machine","title":"Install and configure the Linaro client on your local machine","text":"<p>We recommend installing the Linaro desktop client on your local workstation or laptop. It can be downloaded for a selection of operating systems.</p> <p>Warning</p> <p>Make sure you download the Linaro desktop client matching the version of the <code>linaro-forge</code> uenv you are planning to use.</p> <p>Mismatched desktop client and uenv versions</p> <p>Mismatches between the client and the uenv version will lead to the following error when trying to establish a remote connection:</p> <pre><code>The local version of Linaro DDT (24.0.6) is not compatible with the remote version (24.1.2).\n</code></pre> <p>The client can be configured to connect with the debug jobs running on Alps, offering a better user experience compared to running with X11 forwarding. Once installed, the client needs to be configured to connect to the vCluster on which you are working.</p> <p>First, start the client on your laptop:</p> LinuxmacOS <p>Warning</p> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>$HOME/linaro/forge/24.1.2/bin/ddt\n</code></pre> <p>Warning</p> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location. Please use the appropriate path and version for your installation.</p> <pre><code>open /Applications/Linaro\\ Forge\\ Client\\ 24.1.2.app/\n</code></pre> <p>Next, configure a connection to the target system. Open the <code>Remote Launch</code> menu and click on <code>Configure...</code> then <code>Add</code>. Examples of the settings are below.</p> DaintSantisClaridenEiger <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>daint</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@daint.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>santis</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@santis.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>clariden</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@clariden.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>eiger</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@eiger.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Tip</p> <p>It is recommended to log into Alps using <code>ela.cscs.ch</code> as a ssh Jump host, as explained here. In that case, you can remove <code>cscsusername@ela.cscs.ch</code> from the Linaro client configuration above.</p> <p>Some notes on the examples above:</p> <ul> <li>SSH forwarding via <code>ela.cscs.ch</code> is used to access the cluster;</li> <li>replace the username <code>cscsusername</code> with your CSCS user name that you would normally use to open an SSH connection to CSCS;</li> <li><code>Remote Installation Directory</code> is pointing to the install directory of DDT inside the uenv image;</li> <li>private keys should be the ones generated for CSCS MFA, and this field does not need to be set if you have added the key to your SSH agent.</li> </ul> <p>Once configured, test and save the configuration:</p> <ol> <li>check whether the configuration is correct by clicking <code>Test Remote Launch</code> (and then <code>OK</code> when the test is successful),</li> <li>click on <code>OK</code> and then <code>Close</code> to save the configuration.</li> <li>You can now connect by going to <code>Remote Launch</code> and choose the entry (<code>Connection</code> name) you added.    If the client fails to connect, look at the error message, check your SSH    configuration and make sure you can SSH without the client.</li> </ol> <p></p>"},{"location":"software/devtools/linaro-uenv/#troubleshooting","title":"Troubleshooting","text":"<p>Notes about known issues.</p> <p>The proxy type is invalid for this operation</p> <p>If the tool fails to launch with the following error message: </p> <pre><code>Error communicating with Licence Server velan.cscs.ch:\nThe proxy type is invalid for this operation\nAttempting again while ignoring proxies.\n</code></pre> <p>Proxy environment variables need to be set to let the tool connect to the license server, as explained in Compute node proxy configuration.</p> <p>AMD GPU support</p> <p>CSCS does not currently have a Linaro license for AMD GPUs.</p>"},{"location":"software/devtools/nvidia-nsight/","title":"Using NVIDIA Nsight","text":""},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight","title":"NVIDIA Nsight","text":""},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight-systems","title":"NVIDIA Nsight Systems","text":"<p>NVIDIA Nsight Systems is a system-wide performance analysis tool that enables developers to gain a deep understanding of how their applications utilize computing resources, such as CPUs, GPUs, memory, and I/O. The tool provides a unified view of application performance across the entire system, capturing detailed trace information that allows users to analyze how different components interact and where performance issues might arise.</p> <p>A key advantage of Nsight Systems is its ability to provide detailed traces of GPU activity, offering deeper insights into GPU utilization. It features a timeline-based visualization, enabling developers to inspect the execution flow, pinpoint latencies, and correlate events across different system components. As a sampling profiler, it can be easily used to profile applications written in C, C++, Python, Fortran, or Julia by wrapping the application with the Nsight Systems profiler executable.</p> <p>Note</p> <p>NVIDIA Nsight Systems is available with any uenv that comes with a CUDA compiler, for instance <code>prgenv-gnu</code>.</p>"},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight-compute","title":"NVIDIA Nsight Compute","text":"<p>NVIDIA Nsight Compute is a performance analysis tool specifically designed for optimizing GPU-accelerated applications. It focuses on providing detailed metrics and insights into the performance of CUDA kernels, helping developers identify performance bottlenecks and improve the efficiency of their GPU code. Nsight Compute offers a kernel-level profiler with customizable reports, enabling in-depth analysis of memory usage, compute utilization, and instruction throughput. As a sampling profiler, it can be easily used to profile applications written in C, C++, Python, Fortran, or Julia by wrapping the application with the Nsight Compute profiler executable.</p> <p>Note</p> <p>NVIDIA Nsight Systems is available with any uenv that comes with a CUDA compiler, for instance <code>prgenv-gnu</code>.</p>"},{"location":"software/devtools/nvidia-nsight/#known-issues-and-common-problems","title":"Known Issues and Common Problems","text":"CrashReporter: Qt initialization failed, Failed to load Qt platform plugin: xcb <p>This workaround is not needed for uenvs that provide <code>xcb-util-cursor</code></p> <p>Uenvs that provide the <code>xcb-util-cursor</code> package don\u2019t require this workaround anymore. Starting with 25.6, the <code>prgenv-gnu</code> provides <code>xcb-util-cursor</code>.</p> <p>While we recommend using <code>ncu</code> instead of <code>ncu-ui</code>, it is possible to use X11 to launch ncu-ui. However, this will fail with the following error message: <code>Failed to load Qt platform plugin: \"xcb\"</code>.</p> <p>To workaround this issue, you can follow these instructions:</p> <pre><code>ssh -Y -J &lt;user&gt;@ela.cscs.ch &lt;user&gt;@daint.alps.cscs.ch\n# the -Y ssh flag enables trusted X11 forwarding.\nwget https://jfrog.svc.cscs.ch/artifactory/cscs-reframe-tests/nvidia/ncu_deps.tar.gz\ntar xf ncu_deps.tar.gz\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/usr/lib64\nncu-ui &amp;\n</code></pre>"},{"location":"software/devtools/vihps/","title":"Using Score-P/Scalasca","text":""},{"location":"software/devtools/vihps/#vi-hps-tools","title":"VI-HPS tools","text":"<p>The VI-HPS Institute (Virtual Institute for High Productivity Supercomputing) provides tools that can assist developers of simulation codes to address their needs in performance analysis.</p>"},{"location":"software/devtools/vihps/#score-p","title":"Score-P","text":"<p>Score-P is a highly scalable instrumentation and measurement infrastructure for profiling, event tracing, and online analysis. It supports a wide range of HPC platforms and programming models. Score-P provides core measurement services for a range of specialized analysis tools, such as Vampir, Scalasca and others.</p>"},{"location":"software/devtools/vihps/#vampir","title":"Vampir","text":"<p>Vampir  is a performance visualizer that allows to quickly study the program runtime behavior at a fine level of details. This includes the display of detailed performance event recordings over time in timelines and aggregated profiles. Interactive navigation and zooming are the key features of the tool, which help to quickly identify inefficient or faulty parts of a program.</p> <p>Info</p> <p>While Score-P does not require a license, Vampir does. CSCS standard license allows to read trace files with up to 256 concurrent threads of execution.</p> <p>The Vampir GUI is currently available only on <code>x86-64</code> CPU based systems and is not provided via a uenv (more details in the Quickstart guide below). You can use Score-P to generate OTF2 traces files on Alps compute nodes and then visualize the results with Vampir on a x86-64 CPU based system (for instance Eiger, LUMI or using your own license). </p>"},{"location":"software/devtools/vihps/#cube-and-scalasca","title":"Cube and Scalasca","text":"<p>Cube and Scalasca support the performance optimization of parallel programs with a collection of scalable trace-based tools for in-depth analyses of concurrent behavior. The analysis identifies potential performance bottlenecks - in particular those concerning communication and synchronization - and offers guidance in exploring their causes.</p>"},{"location":"software/devtools/vihps/#quickstart-guide","title":"Quickstart guide","text":"<p>The VI-HPS uenv is named <code>scorep</code> and it can be loaded into your environment as explained here and in the uenv documentation.</p> <p>Finding and pulling available <code>scorep</code> versions</p> <pre><code>uenv image find scorep\n# uenv                 arch   system  id                 size(MB) date\n# scorep/9.2-gcc12:v1  gh200  daint   bfd3b46d30404f2c   7,602    2025-07-14\n# scorep/9.2-gcc13:v1  gh200  daint   3c0357a490c81f32   7,642    2025-07-14\n\nuenv image pull scorep/9.2-gcc13:v1\n# pulling 3c0357a490c81f32 100.00%\n</code></pre> <p>This uenv is configured to be mounted in the <code>/user-environment</code> path.</p> <p>Start the <code>scorep</code> uenv</p> <pre><code>uenv start scorep/9.2-gcc13:v1 -v default\n\nuenv status # (1)!\nscorep --version # 9.2\nscalasca --version # 2.6.2\ncubelib-config --version # 4.9\notf2-print --version # 3.1.1\n\nfind /user-environment/ -name scorep.pdf # (2)!\n</code></pre> <ol> <li>Test that everything has been mounted correctly and that the tools are in the PATH</li> <li>A PDF version of the user guide is available in the uenv.</li> </ol> <p>Recompile your code with <code>scorep</code> on Alps</p> <pre><code># Building with CMake requires the following steps\n## Invoke cmake with the scorep wrapper disabled:\nSCOREP_WRAPPER=OFF \\\n    cmake -S src -B build \\\n    -DCMAKE_CXX_COMPILER=scorep-mpic++ \\\n    -DCMAKE_C_COMPILER=scorep-mpicc \\\n    -DCMAKE_CUDA_COMPILER=scorep-nvcc \\\n    -DCMAKE_CUDA_ARCHITECTURES=90 # [...]\n\n## Then build with the scorep wrapper enabled:\nSCOREP_WRAPPER=ON \\\n    cmake --build build\n</code></pre> <p>Run your application with <code>scorep</code> on Alps</p> <p>Pick one of the report type in your jobscript before running the executable compiled with <code>scorep</code>:</p> ProfilingTracing <ul> <li>Profiling gives an overview of the performance of your simulation on Alps</li> </ul> <pre><code>export SCOREP_ENABLE_PROFILING=true\n# Call-path profiling: CUBE4 data format (profile.cubex)\n</code></pre> <ul> <li>Then run your job as usual with <code>srun</code> or <code>sbatch</code> on Alps,</li> <li>Copy the generated profile <code>profile.cubex</code> to your laptop,</li> <li>Install the Cube tool on your laptop,</li> <li>Analyze the results with the GUI:<ul> <li>performance metric (left panel)</li> <li>call path (middle panel)</li> <li>system resource (right panel)</li> </ul> </li> </ul> <pre><code>/Applications/Cube/4.9/Cube.app/Contents/MacOS/maccubegui.sh \\\n./profile.cubex\n</code></pre> <p></p> <ul> <li>Tracing allows a detailed analysis of the performance of your simulation on Alps</li> </ul> <pre><code>export SCOREP_ENABLE_TRACING=true\n# Event-based tracing: OTF2 data format (traces.otf2)\n</code></pre> <ul> <li>Then run your job as usual with <code>srun</code> or <code>sbatch</code> on Alps,</li> <li>Analyze the results with the GUI:</li> </ul> <pre><code>ssh -X eiger.cscs.ch  # Vampir GUI requires x86_64 \u26a0\ufe0f\n/capstor/store/cscs/userlab/vampir/10.6.1/bin/vampir \\\n./traces.otf2\n</code></pre> <p>Info</p> <ul> <li>Tracing allows more detailed analysis but will also make your simulation run longer than with profiling,</li> <li><code>scorep-score</code> allows to estimate the size of an OTF2 tracefile from a CUBE profile,   it can also help to reduce the overhead of tracing via filtering: <pre><code>scorep-score -g profile.cubex # generate filter file\nscorep-score -f initial_scorep.filter profile.cubex\nexport SCOREP_FILTERING_FILE='initial_scorep.filter'\n</code></pre></li> <li>The user guide provides more details about how to reduce overhead.</li> </ul> <p></p>"},{"location":"software/ml/","title":"Index","text":""},{"location":"software/ml/#machine-learning-applications-and-frameworks","title":"Machine learning applications and frameworks","text":"<p>CSCS supports a wide range of machine learning (ML) applications and frameworks on its systems. Most ML workloads are containerized to ensure portability, reproducibility, and ease of use across systems.</p> <p>Users can choose between running containers, using provided uenv software stacks, or building custom Python environments tailored to their needs.</p> <p>First time users are recommended to consult the LLM tutorials to get familiar with the concepts of the Machine Learning platform in a series of hands-on examples.</p>"},{"location":"software/ml/#optimizing-data-loading-for-machine-learning","title":"Optimizing Data Loading for Machine Learning","text":"<p>Efficient data management is essential for scalable machine learning workloads. Before beginning ML training, ensure your data access strategy maximizes throughput and minimizes impact on shared filesystems.</p> <p>Key recommendations:</p> <ul> <li>Avoid reading many small files or random small chunks from large files. Instead, structure your datasets so that each access reads large, contiguous blocks of data, of at least 100MB per read request.</li> <li>Use formats designed for efficient sequential access. For computer vision, formats like TFRecords or WebDataset are commonly used. For other modalities, HDF5, Zarr, or even simple formats like a single NPZ file per training step can be effective \u2014 provided the data is chunked to match your reading pattern and that the chunks are large enough.</li> <li>Customize data packaging for your model. Preprocessing and chunking data to fit the specific variables required by your ML model can improve performance, but may require creating a tailored copy of the dataset.</li> <li>Monitor your data loading performance. The most reliable metric is the raw throughput of your dataloader (without the training model). However, be aware that this does not fully capture the impact on the shared filesystem.</li> </ul> <p>Optimizing data access not only improves training speed but also reduces contention on shared storage resources, benefiting all users.</p> <p>For detailed information on storage options and best practices for managing data on CSCS systems, refer to the Storage Guide.</p>"},{"location":"software/ml/#running-ml-applications-with-containers-recommended","title":"Running ML applications with containers (recommended)","text":"<p>Containerization is the recommended approach for ML workloads on Alps, as it simplifies software management and maximizes compatibility with other systems.</p> <p>Users are encouraged to build their own containers, starting from popular sources such as the Nvidia NGC Catalog, which offers a variety of pre-built images optimized for HPC and ML workloads. Examples include:</p> <ul> <li>PyTorch NGC container (Release Notes)</li> <li>JAX NGC container (Release Notes)</li> <li>TensorFlow NGC container (deprecated since 25.02, see Release Notes)</li> </ul> <p>Documented best practices are available for:</p> <ul> <li>PyTorch</li> </ul> <p>Extending a container with a virtual environment</p> <p>For frequently changing Python dependencies during development, consider creating a Virtual Environment (venv) on top of the packages in the container (see this example).</p> <p>Helpful references:</p> <ul> <li>Introduction to concepts of the Machine Learning platform: LLM tutorials</li> <li>Running containers on Alps: Container Engine Guide</li> <li>Building custom container images: Container Build Guide</li> </ul>"},{"location":"software/ml/#using-provided-uenv-software-stacks","title":"Using provided uenv software stacks","text":"<p>Alternatively, CSCS provides pre-configured software stacks (uenvs) that can serve as a starting point for machine learning projects. These environments provide optimized compilers, libraries, and selected ML frameworks.</p> <p>Available ML-related uenvs:</p> <ul> <li>PyTorch \u2014 available on Clariden and Daint</li> </ul> <p>Extending a uenv with a virtual environment</p> <p>To extend these environments with additional Python packages, it is recommended to create a Python Virtual Environment (venv) layered on top of the packages in the uenv. See this PyTorch venv example for details.</p>"},{"location":"software/ml/#building-custom-python-environments","title":"Building custom Python environments","text":"<p>Users may also choose to build entirely custom software stacks using Python package managers such as <code>uv</code> or <code>conda</code>. Most ML libraries are available via the Python Package Index (PyPI).</p> <p>Note</p> <p>While many Python packages provide pre-built binaries for common architectures, some may require building from source.</p> <p>To ensure optimal performance on CSCS systems, we recommend starting from an environment that already includes:</p> <ul> <li>CUDA, cuDNN</li> <li>MPI, NCCL</li> <li>C/C++ compilers</li> </ul> <p>This can be achieved either by:</p> <ul> <li>building a custom container image based on a suitable ML-ready base image,</li> <li>or starting from a provided uenv (e.g., <code>prgenv-gnu</code> or PyTorch uenv),</li> </ul> <p>and extending it with a virtual environment.</p>"},{"location":"software/ml/pytorch/","title":"PyTorch","text":""},{"location":"software/ml/pytorch/#pytorch","title":"PyTorch","text":"<p>PyTorch is supported software on Alps. See the main applications page for more information.</p> <p>PyTorch is available both as a container with the Container Engine (CE) and a uenv software stack. The best choice for your use case depends on the amount of control required over the lower level libraries.</p> <p>While NGC provides an optimized build of PyTorch with many dependencies included, uenv allows a more flexible choice of lower level libraries and represents a thinner layer over the host system. Both options can be customized - a container via a Dockerfile and a uenv (in advanced use cases) via its recipe and both, additionally, via Python virtual environments built on top. Due to the simplicity and reproducible performance, containers are generally the recommended default for most users.</p> <p></p>"},{"location":"software/ml/pytorch/#running-pytorch-with-the-container-engine-recommended","title":"Running PyTorch with the Container Engine (recommended)","text":"<p>Running PyTorch from a container ensures maximum portability, reproducibility, and ease of use across machines. This is achieved by </p> <ol> <li>selecting an appropriate base image and customizing it in a Dockerfile</li> <li>defining the container runtime environment in an EDF</li> <li>(optionally) extending with a virtual environment</li> <li>submitting jobs with CE in SLURM </li> </ol> <p>Example</p> <p>These steps are illustrated in the machine learning tutorials and the instructions detailed in the podman build guide.</p> <p>Preliminary steps</p> <p>Before proceeding with the next steps, make sure you have storage for podman configured as in the build guide and make sure to apply recommended Lustre settings to every directory (e.g. <code>$SCRATCH/ce-images</code>) dedicated to container images before importing them with enroot. This is necessary to guarantee good filesystem performance.</p> <pre><code>lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -S 4M $SCRATCH/ce-images # (1)!\n</code></pre> <ol> <li>This makes sure that files stored subsequently end up on the same storage node (up to 4 MB), on 4 storage nodes (between 4 and 64 MB) or are striped across all storage nodes (above 64 MB)</li> </ol>"},{"location":"software/ml/pytorch/#select-the-base-image","title":"Select the base image","text":"<p>For most applications, the PyTorch NGC container is a good base image as PyTorch comes pre-installed with an optimized build including many dependencies. The Release Notes give an overview of installed packages and compatibility. This image can be further customized in a Dockerfile and built with podman as detailed in the podman build guide.</p>"},{"location":"software/ml/pytorch/#define-container-runtime-environment","title":"Define Container Runtime Environment","text":"<p>Having built and imported a container image with podman and enroot, the next step is to configure the runtime environment with an environment definition file (EDF). In particular, this includes specifying the image, any directories mounted from the host and a working directory for the process in the container to start in as in the quickstart examples for CE.</p> <p>Apart from this, there are specific features relevant for machine learning made available through annotations, which customize the container at runtime.</p> <ul> <li>When using NCCL inside the container, include the aws-ofi-nccl plugin which enables the container to interface with the host\u2019s libfabric and, thus, use the Slingshot high-speed interconnect. This is crucial for multi-node communication performance.</li> <li>An SSH annotation allows adding a light-weight SSH server to the container without the need to modify the container image</li> </ul> <p>A resulting example TOML file following best practices may look like</p> $HOME/my-app/ngc-pytorch-my-app-25.06.toml<pre><code>image = \"${SCRATCH}/ce-images/ngc-pytorch-my-app+25.06.sqsh\" # (1)!\n\nmounts = [\n    \"/capstor\",\n    \"/iopsstor\",\n    \"/users/${USER}/my-app\"\n] # (2)!\n\nworkdir = \"${HOME}/my-app\" # (3)!\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\" # (4)!\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nNCCL_DEBUG = \"INFO\" # (5)!\nCUDA_CACHE_DISABLE = \"1\" # (6)!\nTORCH_NCCL_ASYNC_ERROR_HANDLING = \"1\" # (7)!\nMPICH_GPU_SUPPORT_ENABLED = \"0\" # (8)!\n</code></pre> <ol> <li>It is important to use curly braces for environment variables used in the EDF</li> <li>The path <code>/users</code> is not mounted as a whole since it often contains user-specific initialization scripts for the host environment and many frameworks leave temporary data behind that can lead to non-trivial runtime errors when swapping container images. Thus, it is recommended to selectively mount specific subfolders under <code>${HOME}</code> if needed.</li> <li>You can use <code>${PWD}</code> as an alternative to use the path submitted from when the container is started</li> <li>This enables NCCL installed in the container to make effective use of the Slingshot interconnect on Alps by interfacing with the AWS OFI NCCL plugin. While not strictly needed for single node workloads, it is good practice to keep it always on.</li> <li>This makes NCCL output debug info during initialization, which can be useful to spot communication-related issues in a distributed scenario. Subsystems with debug log can be configured with <code>NCCL_DEBUG_SUBSYS</code>.</li> <li>Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.</li> <li>Async error handling when an exception is observed in NCCL watchdog: aborting NCCL communicator and tearing down process upon error</li> <li>Disable GPU support in MPICH, as it can lead to deadlocks when using together with NCCL</li> </ol> Access to SLURM from inside the container <p>In case access to SLURM is required from inside the container, you can add the following lines to the mounts above:</p> <pre><code>...\n\nmounts = [\n   \"/capstor\",\n   \"/iopsstor\",\n   \"/users/${USER}/my-app\",\n   \"/etc/slurm\", # (1)!\n   \"/usr/lib64/libslurm-uenv-mount.so\",\n   \"/etc/container_engine_pyxis.conf\"\n]\n\n...\n</code></pre> <ol> <li>Enable Slurm commands (together with two subsequent mounts)</li> </ol> <p>Best practice for production jobs</p> <p>For stability and reproducibility, use self-contained containers for production jobs. Using code mounted from the distributed filesystem may leave compiled artefacts behind that can result in unintentional runtime errors when e.g. swapping the container image. In particular, it is recommended to avoid mounting all of <code>$HOME</code>, so that environments are properly isolated and e.g. the Triton cache (that by default ends up in <code>$HOME/.triton</code>) resides in an ephemeral location of the filesystem.</p> <p>Collaborating in Git</p> <p>For reproducibility, it is recommended to always track the Dockerfile, EDF and an optional virtual environment specification alongside your application code in a Git repository.</p> <p></p>"},{"location":"software/ml/pytorch/#optionally-extend-container-with-virtual-environment","title":"(Optionally) extend container with virtual environment","text":"<p>While production jobs should include as many dependencies as possible in the container image, during development it can be convenient to manage frequently changing packages in a virtual environment built on top of the container image. This can include both dependencies and actively developed packages (that can be installed in editable mode with <code>pip install -e .</code>).</p> <p>To create such a virtual environment, inside the container use the Python <code>venv</code> module with the option <code>--system-site-packages</code> to ensure that packages are installed in addition to the existing packages. Without this option, packages may accidentally be re-installed shadowing a version that is already present in the container. A workflow installing additional packages in a virtual environment may look like this:</p> <pre><code>[clariden-lnXXX]$ srun -A &lt;ACCOUNT&gt; \\\n  --environment=./ngc-pytorch-my-app-25.06.toml --pty bash # (1)!\nuser@nidYYYYYY$ python -m venv --system-site-packages venv-ngc-pt-25.06 # (2)!\nuser@nidYYYYYY$ source venv-ngc-pt-25.06/bin/activate # (3)!\n(venv-ngc-pt-25.06) user@nidYYYYYY$ pip install &lt;package&gt;  # (3)!\n(venv-ngc-pt-25.06) user@nidYYYYYY$ exit\n</code></pre> <ol> <li>Allocate an interactive session on a compute node</li> <li>Create a virtual environment on top of the existing Python installation in the container (only necessary the first time)</li> <li>Activate the newly created virtual environment (always necessary when running a Slurm job)</li> <li>Install additional packages (only run this from a single process to avoid race conditions)</li> </ol> <p>The changes made to the virtual environment will outlive the container as they are persisted on the distributed filesystem.</p> <p>Note</p> <p>Keep in mind that:</p> <ul> <li>this virtual environment is specific to this particular container and won\u2019t actually work unless you are using it from inside this container - it relies on the resources packaged inside the container.</li> <li>every Slurm job making use of this virtual environment will need to activate it first (inside the <code>srun</code> command). </li> </ul>"},{"location":"software/ml/pytorch/#submit-jobs-with-the-container-engine-in-slurm","title":"Submit jobs with the Container Engine in Slurm","text":"<p>A general template for a Pytorch distributed training job with Slurm in analogy to the last tutorial may look like</p> $HOME/my-app/submit-dist-train.sh<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=dist-train-ddp\n#SBATCH --time=01:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --output=logs/slurm-%x-%j.log\n# (1)!\n\nset -x\n\nulimit -c 0 # (2)!\n\n # (3)!\n # (4)!\nsrun -ul --environment=./ngc-pytorch-my-app-25.06.toml bash -c \"\n    . venv-ngc-pt-25.06/bin/activate  # activate (optional) venv\n\n    MASTER_ADDR=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n    MASTER_PORT=29500 \\\n    RANK=\\${SLURM_PROCID} \\\n    LOCAL_RANK=\\${SLURM_LOCALID} \\\n    WORLD_SIZE=\\${SLURM_NTASKS} \\\n    python dist-train.py &lt;dist-train-args&gt;\n\"\n</code></pre> <ol> <li>If <code>#SBATCH --error=...</code> is not specified, <code>#SBATCH --output</code> will also contain stderr (error messages)</li> <li>In case the application crashes, it may leave behind large core dump files that contain an image of the process memory at the time of the crash. While these can be useful for debugging the reason of a specific crash (by e.g. loading them with <code>cuda-gdb</code> and looking at the stack trace with <code>bt</code>), they may accumulate over time and occupy a large space on the filesystem. For this reason, it is recommended to disable their creation (unless needed) by adding this line.</li> <li>Loading the virtual environment is mandatory within every <code>srun</code> command if it is used to manage packages.</li> <li>The environment variables are set to initialize PyTorch\u2019s distributed module through the environment (cf. docs).</li> </ol> <p>For further details on execution logic, job monitoring and data management, please refer to the nanotron tutorial (which in particular also explains the usage of <code>torchrun</code> with Slurm). Make sure to apply recommended Lustre settings to datasets, models and container images persisted to the distributed filesystem.</p> <p>#SBATCH \u2013environment</p> <p>The operations performed before the <code>srun</code> command are executed in the host environment of a single compute node in the allocation. If you need to perform these steps in the container environment as well, you can alternatively use the <code>#SBATCH --environment=path/to/ngc-pytorch-my-app-25.06.toml</code> option instead of using <code>--environment</code> with <code>srun</code>.</p> <p>Use of the <code>--environment</code> option for <code>sbatch</code> is still considered experimental and could result in unexpected behavior. In particular, avoid mixing <code>#SBATCH --environment</code> and <code>srun --environment</code> in the same job.</p> <p>Use of <code>--environment</code> is currently only recommended for the <code>srun</code> command. </p> <p>Optimizing large-scale training jobs</p> <p>The following settings were established to improve compute throughput of LLM training in Megatron-LM:</p> <ul> <li> <p>Extensively evaluate all possible parallelization dimensions, including data-, tensor- and pipeline parallelism (including virtual pipeline parallelism) and more, when available. Identify storage-related bottlenecks by isolating data loading/generation operations into a separate benchmark.</p> </li> <li> <p>Disabling transparent huge pages and enabling the Nvidia vboost feature has been observed to improve performance in large-scale LLM training in Megatron-LM. This can be achieved by adding these constraints to the sbatch script:    <pre><code>#SBATCH -C thp_never&amp;nvidia_vboost_enabled\n</code></pre></p> </li> <li> <p>The argument <code>--ddp-bucket-size</code> controls the level of grouping of many small data-parallel communications into bigger ones and setting it to a high value can improve throughput (model-dependent, e.g. <code>10000000000</code>).</p> </li> <li> <p>If in doubt about communication performance with NCCL at scale, use the <code>NCCL_DEBUG</code> environment variable to validate that the aws-ofi-nccl plugin has been properly initialized and libfabric was recognized (further subsystems can be monitored with <code>NCCL_DEBUG_SUBSYS</code>). If the issue persists, use nccl-tests with the relevant communication patterns to check if the scaling behavior can be reproduced and contact CSCS support.</p> </li> </ul> <p>Additionally, consider the best practice for checkpointing and data management:</p> <ul> <li> <p>Following the advice on filesystems, write checkpoints (sequential write) to <code>/capstor/scratch</code> and place randomly accessed training data (many small random reads) on <code>/iopsstor/scratch</code>. Use the data transfer instructions to move data to/from <code>/capstor/store</code>. Make sure to apply recommended Lustre settings on all directories containing significant amount of data, including those containing container images and managed by other tools (e.g. the HuggingFace cache, see <code>HF_HOME</code> in the this tutorial). In case your workload continues to be limited by filesystem performance, contact CSCS support.</p> </li> <li> <p>Regularly adjust checkpoint writing intervals to the current overhead induced by writing a checkpoint (\\(T_1\\)) and mean time between job failures (\\(T_2\\)). As a first order approximation use a checkpointing interval of \\(\\sqrt{2 T_1 T_2}\\) (derived by Young and Daly).</p> </li> <li> <p>Avoid activities that put excessive load on third party services (such as web scraping or bulk downloads) in line with the guidelines on Internet Access on Alps.</p> </li> </ul> <p>Adjust for cluster availability:</p> <ul> <li>Submit your jobs with a Slurm time limit compatible with reservations (such as maintenance windows, cf. <code>scontrol show res</code>) to be able to get scheduled.</li> </ul> Debugging segmentation faults <p>Application crashes with segmentation faults can be investigated by inspecting core dump files that contain an image of the process memory at the time of the crash. For this purpose, you can load the core dump file with <code>cuda-gdb</code> installed in the container and look at the stack trace with <code>bt</code>. Note that in order to generate core dump files the line <code>ulimit -c 0</code> must be commented out in the above sbatch script.</p>"},{"location":"software/ml/pytorch/#known-issues","title":"Known Issues","text":"<p>The Release Notes of every NGC PyTorch container contain a selected list of known issues.</p> Errors hidden by failures in UCX signal handler <p>Application errors may trigger the UCX signal handler in the NGC container, which has caused secondary failures in the past, shadowing the initial error trace. These secondary failures may be significantly harder to fix than the initial problem.</p> <p>An example is the following trace from the NGC PyTorch 25.01 with Megatron-LM: <pre><code>640: [nid007306:244443:0:244443] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x455)\n640: ==== backtrace (tid: 244443) ====\n640:  0  /opt/hpcx/ucx/lib/libucs.so.0(ucs_handle_error+0x2cc) [0x4000d2b214dc]\n640:  1  /opt/hpcx/ucx/lib/libucs.so.0(+0x3168c) [0x4000d2b2168c]\n640:  2  /opt/hpcx/ucx/lib/libucs.so.0(+0x319b8) [0x4000d2b219b8]\n640:  3  linux-vdso.so.1(__kernel_rt_sigreturn+0) [0x4000347707dc]\n640:  4  /usr/local/cuda/lib64/libnvrtc.so.12.8.61(+0x935000) [0x400140a25000]\n640:  5  [0x3d5c5e58]\n640: =================================\nsrun: error: nid007306: task 640: Segmentation fault\nsrun: Terminating StepId=348680.1\n</code></pre> In this case, the segmentation fault in the UCX signal handler (<code>ucs_handle_error</code>) was due to a broken NVRTC in the container. However, to obtain the trace of the initial error (which was unrelated), it was necessary to disable the UCX signal handler by setting the following environment variable in the sbatch script: <pre><code>export UCX_HANDLE_ERRORS=none\n</code></pre></p> Avoid <code>--defer-embedding-wgrad-compute</code> in Megatron-LM <p>In Megatron-LM, avoid using the option <code>--defer-embedding-wgrad-compute</code> to delay the embedding gradient computation as it can lead to an incorrect gradient norm that changes upon resuming at different scale.</p> <p></p>"},{"location":"software/ml/pytorch/#running-pytorch-with-a-uenv","title":"Running PyTorch with a uenv","text":"<p>The PyTorch uenv software stack was designed with the intention of being able to run Megatron-LM-based pre-training workloads out of the box. Thus, it comes with batteries included and does not just provide the bare PyTorch framework.</p> <p>uenv</p> <p>The PyTorch uenv is provided via the tool uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/ml/pytorch/#versioning","title":"Versioning","text":"<p>The PyTorch uenv is versioned according to the PyTorch version it provides.</p> version node types system v2.6.0 gh200 clariden, daint v2.6.0 non-Python packages exposed via the <code>default</code> view Package Version <code>abseil-cpp</code> 20240722.0 <code>alsa-lib</code> 1.2.3.2 <code>autoconf</code> 2.72 <code>automake</code> 1.16.5 <code>aws-ofi-nccl</code> 1.14.0 <code>berkeley-db</code> 18.1.40 <code>bison</code> 3.8.2 <code>boost</code> 1.86.0 <code>bzip2</code> 1.0.8 <code>ca-certificates-mozilla</code> 2023-05-30 <code>cmake</code> 3.30.5 <code>cpuinfo</code> 2024-09-26 <code>cray-gtl</code> 8.1.32 <code>cray-mpich</code> 8.1.32 <code>cray-pals</code> 1.3.2 <code>cray-pmi</code> 6.1.15 <code>cuda</code> 12.6.0 <code>cudnn</code> 9.2.0.82-12 <code>curl</code> 8.10.1 <code>cutensor</code> 2.0.1.2 <code>diffutils</code> 3.10 <code>eigen</code> 3.4.0 <code>elfutils</code> 0.191 <code>expat</code> 2.6.4 <code>faiss</code> 1.8.0 <code>ffmpeg</code> 5.1.4 <code>fftw</code> 3.3.10 <code>findutils</code> 4.9.0 <code>flac</code> 1.4.3 <code>fmt</code> 11.0.2 <code>fp16</code> 2020-05-14 <code>fxdiv</code> 2020-04-17 <code>gawk</code> 4.2.1 <code>gcc</code> 13.3.0 <code>gcc-runtime</code> 13.3.0 <code>gdb</code> 15.2 <code>gdbm</code> 1.23 <code>gettext</code> 0.22.5 <code>git</code> 2.47.0 <code>glibc</code> 2.31 <code>gloo</code> 2023-12-03 <code>gmake</code> 4.4.1 <code>gmp</code> 6.3.0 <code>gmp</code> 6.3.0 <code>gnuconfig</code> 2024-07-27 <code>googletest</code> 1.12.1 <code>gperftools</code> 2.16 <code>hdf5</code> 1.14.5 <code>hwloc</code> 2.11.1 <code>hydra</code> 4.2.1 <code>krb5</code> 1.21.3 <code>libaio</code> 0.3.113 <code>libarchive</code> 3.7.6 <code>libbsd</code> 0.12.2 <code>libedit</code> 3.1-20240808 <code>libfabric</code> 1.15.2.0 <code>libffi</code> 3.4.6 <code>libgit2</code> 1.8.0 <code>libiconv</code> 1.17 <code>libidn2</code> 2.3.7 <code>libjpeg-turbo</code> 3.0.3 <code>libmd</code> 1.0.4 <code>libmicrohttpd</code> 0.9.50 <code>libogg</code> 1.3.5 <code>libpciaccess</code> 0.17 <code>libpng</code> 1.6.39 <code>libsigsegv</code> 2.14 <code>libssh2</code> 1.11.1 <code>libtool</code> 2.4.6 <code>libtool</code> 2.4.7 <code>libunistring</code> 1.2 <code>libuv</code> 1.48.0 <code>libvorbis</code> 1.3.7 <code>libxcrypt</code> 4.4.35 <code>libxml2</code> 2.13.4 <code>libyaml</code> 0.2.5 <code>lz4</code> 1.10.0 <code>lzo</code> 2.10 <code>m</code>4 1.4.19 <code>magma</code> master <code>meson</code> 1.5.1 <code>mpc</code> 1.3.1 <code>mpfr</code> 4.2.1 <code>nasm</code> 2.16.03 <code>nccl</code> 2.26.2-1 <code>nccl-tests</code> 2.13.6 <code>ncurses</code> 6.5 <code>nghttp2</code> 1.63.0 <code>ninja</code> 1.12.1 <code>numactl</code> 2.0.18 <code>nvtx</code> 3.1.0 <code>openblas</code> 0.3.28 <code>openssh</code> 9.9p1 <code>openssl</code> 3.4.0 <code>opus</code> 1.5.2 <code>osu-micro-benchmarks</code> 7.5 <code>patchelf</code> 0.17.2 <code>pcre</code> 8.45 <code>pcre2</code> 10.44 <code>perl</code> 5.40.0 <code>pigz</code> 2.8 <code>pkgconf</code> 2.2.0 <code>protobuf</code> 3.28.2 <code>psimd</code> 2020-05-17 <code>pthreadpool</code> 2023-08-29 <code>python</code> 3.13.0 <code>python-venv</code> 1.0 <code>rdma-core</code> 31.0 <code>re2c</code> 3.1 <code>readline</code> 8.2 <code>rust</code> 1.81.0 <code>rust-bootstrap</code> 1.81.0 <code>sentencepiece</code> 0.1.99 <code>sleef</code> 3.6.0_2024-03-20 <code>sox</code> 14.4.2 <code>sqlite</code> 3.46.0 <code>swig</code> 4.1.1 <code>tar</code> 1.34 <code>texinfo</code> 7.1 <code>util-linux-uuid</code> 2.40.2 <code>util-macros</code> 1.20.1 <code>valgrind</code> 3.23.0 <code>xpmem</code> 2.9.6 <code>xz</code> 5.4.6 <code>yasm</code> 1.3.0 <code>zlib-ng</code> 2.2.1 <code>zstd</code> 1.5.6 Python packages exposed via the <code>default</code> view Package Version <code>aniso8601</code> 9.0.1 <code>annotated-types</code> 0.7.0 <code>apex</code> 0.1 <code>appdirs</code> 1.4.4 <code>astunparse</code> 1.6.3 <code>blinker</code> 1.6.2 <code>certifi</code> 2023.7.22 <code>charset-normalizer</code> 3.3.0 <code>click</code> 8.1.7 <code>coverage</code> 7.2.6 <code>Cython</code> 3.0.11 <code>docker-pycreds</code> 0.4.1 <code>donfig</code> 0.8.1.post1 <code>einops</code> 0.8.0 <code>faiss</code> 1.8.0 <code>filelock</code> 3.12.4 <code>flash_attn</code> 2.6.3 <code>Flask</code> 2.3.2 <code>Flask-RESTful</code> 0.3.9 <code>fsspec</code> 2024.5.0 <code>gitdb</code> 4.0.9 <code>GitPython</code> 3.1.40 <code>huggingface_hub</code> 0.26.2 <code>idna</code> 3.4 <code>importlib_metadata</code> 7.0.1 <code>iniconfig</code> 2.0.0 <code>itsdangerous</code> 2.1.2 <code>Jinja2</code> 3.1.4 <code>joblib</code> 1.2.0 <code>lightning-utilities</code> 0.11.2 <code>MarkupSafe</code> 2.1.3 <code>mpmath</code> 1.3.0 <code>networkx</code> 3.1 <code>nltk</code> 3.9.1 <code>numcodecs</code> 0.15.0 <code>numpy</code> 2.1.2 <code>nvtx</code> 0.2.5 <code>packaging</code> 24.1 <code>pillow</code> 11.0.0 <code>pip</code> 23.1.2 <code>platformdirs</code> 3.10.0 <code>pluggy</code> 1.5.0 <code>protobuf</code> 5.28.2 <code>psutil</code> 7.0.0 <code>pybind11</code> 2.13.6 <code>pydantic</code> 2.10.1 <code>pydantic_core</code> 2.27.1 <code>pytest</code> 8.2.1 <code>pytest-asyncio</code> 0.23.5 <code>pytest-cov</code> 4.0.0 <code>pytest-mock</code> 3.10.0 <code>pytest-random-order</code> 1.0.4 <code>pytz</code> 2023.3 <code>PyYAML</code> 6.0.2 <code>regex</code> 2022.8.17 <code>requests</code> 2.32.3 <code>safetensors</code> 0.4.5 <code>sentencepiece</code> 0.1.99 <code>sentry-sdk</code> 2.22.0 <code>setproctitle</code> 1.1.10 <code>setuptools</code> 69.2.0 <code>six</code> 1.16.0 <code>smmap</code> 5.0.0 <code>sympy</code> 1.13.1 <code>tiktoken</code> 0.4.0 <code>tokenizers</code> 0.21.0 <code>torch</code> 2.6.0 <code>torchaudio</code> 2.6.0a0+d883142 <code>torchmetrics</code> 1.5.2 <code>torchvision</code> 0.21.0 <code>tqdm</code> 4.66.3 <code>transformer_engine</code> 2.3.0.dev0+dd4c17d <code>transformers</code> 4.48.3 <code>triton</code> 3.2.0+gitc802bb4f <code>typing_extensions</code> 4.12.2 <code>urllib3</code> 2.1.0 <code>versioneer</code> 0.29 <code>wandb</code> 0.19.9 <code>Werkzeug</code> 3.0.4 <code>wheel</code> 0.41.2 <code>wrapt</code> 1.15.0 <code>zarr</code> 3.0.1 <code>zipp</code> 3.17.0 <p></p>"},{"location":"software/ml/pytorch/#how-to-use","title":"How to use","text":"<p>There are two ways to access the software provided by the uenv, once it has been started.</p> the default viewSpack <p>The simplest way to get started is to use the <code>default</code> file system view, which automatically loads all of the packages when the uenv is started.</p> Test mpi compilers and python provided by pytorch/v2.6.0<pre><code>$ uenv start pytorch/v2.6.0:v1 --view=default # (1)!\n\n$ which python # (2)!\n/user-environment/env/default/bin/python\n$ python --version\nPython 3.13.0\n\n$ which mpicc # (3)!\n/user-environment/env/default/bin/mpicc\n$ mpicc --version\ngcc (Spack GCC) 13.3.0\n$ gcc --version # the compiler wrapper uses the gcc provided by the uenv\ngcc (Spack GCC) 13.3.0\n\n$ exit # (4)!\n</code></pre> <ol> <li>Start using the default view.</li> <li>The python executable provided by the uenv is the default, and is a recent version.</li> <li>The MPI compiler wrappers are also available.</li> <li>Exit the uenv.</li> </ol> <p>The pytorch uenv can also be used as a base for building software with Spack, because it provides compilers, MPI, Python and common packages like HDF5.</p> <p>Check out the guide for using Spack with uenv.</p> <p></p>"},{"location":"software/ml/pytorch/#adding-python-packages-on-top-of-the-uenv","title":"Adding Python packages on top of the uenv","text":"<p>Uenvs are read-only, and cannot be modified. However, it is possible to add Python packages on top of the uenv using virtual environments analogous to the setup with containers.</p> Creating a virtual environment on top of the uenv<pre><code>$ uenv start pytorch/v2.6.0:v1 --view=default # (1)!\n\n$ python -m venv --system-site-packages venv-uenv-pt2.6-v1 # (2)!\n\n$ source venv-uenv-pt2.6-v1/bin/activate # (3)!\n\n(venv-uenv-pt2.6-v1) $ pip install &lt;package&gt; # (4)!\n\n(venv-uenv-pt2.6-v1) $ deactivate # (5)!\n\n$ exit # (6)!\n</code></pre> <ol> <li>The <code>default</code> view is recommended, as it loads all the packages provided by the uenv.    This is important for PyTorch to work correctly, as it relies on the CUDA and NCCL libraries provided by the uenv.</li> <li>The virtual environment is created in the current working directory, and can be activated and deactivated like any other Python virtual environment.</li> <li>Activating the virtual environment will override the Python executable provided by the uenv, and use the one from the virtual environment instead.    This is important to ensure that the packages installed in the virtual environment are used.</li> <li>The virtual environment can be used to install any Python package.</li> <li>The virtual environment can be deactivated using the <code>deactivate</code> command.    This will restore the original Python executable provided by the uenv.</li> <li>The uenv can be exited using the <code>exit</code> command or by typing <code>ctrl-d</code>.</li> </ol> <p>Squashing the virtual environment</p> <p>Python virtual environments can be slow on the parallel Lustre file system due to the amount of small files and potentially many processes accessing it. If this becomes a bottleneck, consider squashing the venv into its own memory-mapped, read-only file system to enhance scalability and reduce load times.</p> Python packages from uenv shadowing those in a virtual environment <p>When using uenv with a virtual environment on top, the site-packages under <code>/user-environment</code> currently take precedence over those in the activated virtual environment. This is due to the uenv paths being included in the <code>PYTHONPATH</code> environment variable. As a consequence, despite installing a different version of a package in the virtual environment from what is available in the uenv, the uenv version will still be imported at runtime. A possible workaround is to prepend the virtual environment\u2019s site-packages to <code>PYTHONPATH</code> whenever activating the virtual environment. <pre><code>export PYTHONPATH=\"$(python -c 'import site; print(site.getsitepackages()[0])'):$PYTHONPATH\"\n</code></pre> It is recommended to apply this workaround if you are constrained by a Python package version installed in the uenv that you need to change for your application.</p> <p>Note</p> <p>Keep in mind that</p> <ul> <li>this virtual environment is specific to this particular uenv and won\u2019t actually work unless you are using it from inside this uenv - it relies on the resources packaged inside the uenv.</li> <li>every Slurm job making use of this virtual environment will need to activate it first (inside the <code>srun</code> command). </li> </ul> <p>Alternatively one can use the uenv as upstream Spack instance to to add both Python and non-Python packages. However, this workflow is more involved and intended for advanced Spack users.</p>"},{"location":"software/ml/pytorch/#running-pytorch-jobs-with-slurm","title":"Running PyTorch jobs with Slurm","text":"Slurm sbatch script<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=dist-train-ddp\n#SBATCH --time=01:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --output=logs/slurm-%x-%j.log\n# (1)!\n#SBATCH --uenv=pytorch/v2.6.0:/user-environment\n#SBATCH --view=default\n\nset -x\n\nulimit -c 0 # (2)!\n\n#################################\n# OpenMP environment variables #\n#################################\nexport OMP_NUM_THREADS=8 # (3)!\n\n#################################\n# PyTorch environment variables #\n#################################\nexport TORCH_NCCL_ASYNC_ERROR_HANDLING=1 # (4)!\nexport TRITON_HOME=/dev/shm/ # (5)!\n\n#################################\n# MPICH environment variables   #\n#################################\nexport MPICH_GPU_SUPPORT_ENABLED=0 # (6)!\n\n#################################\n# CUDA environment variables    #\n#################################\nexport CUDA_CACHE_DISABLE=1 # (7)!\n\n############################################\n# NCCL and Fabric environment variables    #\n############################################\n# (8)!\n# This forces NCCL to use the libfabric plugin, enabling full use of the\n# Slingshot network. If the plugin can not be found, applications will fail to\n# start. With the default value, applications would instead fall back to e.g.\n# TCP, which would be significantly slower than with the plugin. More information\n# about `NCCL_NET` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net\nexport NCCL_NET=\"AWS Libfabric\"\n# Use GPU Direct RDMA when GPU and NIC are on the same NUMA node. More\n# information about `NCCL_NET_GDR_LEVEL` can be found at:\n# https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-net-gdr-level-formerly-nccl-ib-gdr-level\nexport NCCL_NET_GDR_LEVEL=PHB\nexport NCCL_CROSS_NIC=1\n# Starting with nccl 2.27 a new protocol (LL128) was enabled by default, which\n# typically performs worse on Slingshot. The following disables that protocol.\nexport NCCL_PROTO=^LL128\n# These `FI` (libfabric) environment variables have been found to give the best\n# performance on the Alps network across a wide range of applications. Specific\n# applications may perform better with other values.\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_DEFAULT_TX_SIZE=32768\nexport FI_CXI_DISABLE_HOST_REGISTER=1\nexport FI_CXI_RX_MATCH_MODE=software\nexport FI_MR_CACHE_MONITOR=userfaultfd\n\n\n# (9)!\n# (10)!\nsrun -ul bash -c \"\n    . ./venv-uenv-pt2.6-v1/bin/activate\n\n    MASTER_ADDR=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n    MASTER_PORT=29500 \\\n    RANK=\\${SLURM_PROCID} \\\n    LOCAL_RANK=\\${SLURM_LOCALID} \\\n    WORLD_SIZE=\\${SLURM_NTASKS} \\\n    python dist-train.py &lt;dist-train-args&gt;\n\"\n</code></pre> <ol> <li>The <code>--uenv</code> option is used to specify the uenv to use for the job.    The <code>--view=default</code> option is used to load all the packages provided by the uenv.</li> <li>In case the application crashes, it may leave behind large core dump files that contain an image of the process memory at the time of the crash. While these can be useful for debugging the reason of a specific crash (by e.g. loading them with <code>cuda-gdb</code> and looking at the stack trace with <code>bt</code>), they may accumulate over time and occupy a large space on the filesystem. For this reason, it is recommended to disable their creation (unless needed) by adding this line.</li> <li>Set <code>OMP_NUM_THREADS</code> if you are using OpenMP in your code.    The number of threads should be not greater than the number of cores per task (<code>$SLURM_CPUS_PER_TASK</code>).    The optimal number depends on the workload and should be determined by testing.    Consider for example that typical workloads using PyTorch may fork the processes, so the number of threads should be around the number of cores per task divided by the number of processes.</li> <li>Enable more graceful exception handling, see PyTorch documentation</li> <li>Set the Triton home to a local path (e.g. <code>/dev/shm</code>) to avoid writing to the (distributed) file system.    This is important for performance, as writing to the Lustre file system can be slow due to the amount of small files and potentially many processes accessing it. Avoid this setting with the container engine as it may lead to errors related to mount settings of <code>/dev/shm</code> (use a filesystem path inside the container instead).</li> <li>Disable GPU support in MPICH, as it can lead to deadlocks when using together with nccl.</li> <li>Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.</li> <li>These variables should always be set for correctness and optimal performance when using NCCL with uenv, see the detailed explanation.</li> <li>Activate the virtual environment created on top of the uenv (if any).    Please follow the guidelines for python virtual environments with uenv to enhance scalability and reduce load times. </li> <li>The environment variables are used by PyTorch to initialize the distributed backend.    The <code>MASTER_ADDR</code>, <code>MASTER_PORT</code> variables are used to determine the address and port of the master node.    Additionally we also need <code>RANK</code> and <code>LOCAL_RANK</code> and <code>WORLD_SIZE</code> to identify the position of each rank within the Slurm step and node, respectively.</li> </ol>"},{"location":"software/prgenv/","title":"Index","text":""},{"location":"software/prgenv/#programming-environments","title":"Programming Environments","text":"<p>CSCS provides \u201cprogramming environments\u201d on Alps vClusters that provide compilers, MPI, and commonly used libraries and packages, that can be used to build applications from source.</p> <ul> <li> <p> prgenv-gnu uenv</p> <p>Provides compilers, MPI, tools and libraries built around the GNU compiler toolchain. It is the go to programming environment on all systems and target node types, that is it is the first that you should try out when starting to compile an application or create a python virtual environment.</p> </li> <li> <p> prgenv-nvfortran uenv</p> <p>Provides a set of tools and libraries for building applications that need the NVIDIA Fortran compiler, commonly required for OpenACC and CUDA-Fortran applications.</p> </li> <li> <p> linalg uenv</p> <p>Provides compilers, MPI and Python, along with linear algebra and mesh partitioning libraries for a broad range of use cases.</p> </li> <li> <p> julia uenv</p> <p>Provides a complete HPC setup for running Julia efficiently at scale, using the supercomputer hardware optimally.</p> </li> <li> <p> Cray Programming Environment containers</p> <p>The Cray Programming Environment (CPE) is a suite of compilers, libraries and tools provided by HPE.</p> <p>These are the \u201cCray Modules\u201d, familiar to users of old Piz Daint and other HPE/Cray clusters.</p> </li> </ul>"},{"location":"software/prgenv/cpe/","title":"Cray modules (CPE)","text":""},{"location":"software/prgenv/cpe/#cray-programming-environment-cpe","title":"Cray Programming Environment (CPE)","text":"<p>The Cray Programming Environment (CPE) is a suite of software: programming environments, compilers, libraries and tools.</p> <p>The CPE is provided to users on Alps as containers.</p> <p>CPE is the cray modules</p> <p>The familiar modules that provide the <code>Prgenv-gnu</code> and <code>Prgenv-cray</code>  programming environments, and packages like <code>cray-python</code> and <code>cray-fftw</code>, that will be familiar from the old Piz Daint system.</p> <p>CPE is not supported on Alps</p> <p>The Cray modules were provided on the old Daint system, and CSCS supported their use and provided software built on top of them.</p> <p>Alps is a big change - the CPE modules are not provide as officially supported software. They are provided as is to users who still need to use them, however CSCS will not be able to provide detailed support for issues that arise when using them.</p> <p>The recommended method for building and running software is to use uenv or containers.</p>"},{"location":"software/prgenv/cpe/#cpe-in-a-container","title":"CPE in a container","text":""},{"location":"software/prgenv/cpe/#available-versions","title":"Available versions","text":"<p>The <code>PrgEnv-gnu</code> and <code>PrgEnv-cray</code> programming environments are provided as separate containers, instead of having both in one container, named <code>gnu-$version</code> and <code>cray-$version</code> respectively. The <code>version</code> is the CPE version in the container.</p> <code>zen2</code> <code>gh200</code> <code>cpe-gnu-24.7</code> \u274c \u2705 <code>cpe-cray-24.7</code> \u274c \u2705 <p>only available on gh200</p> <p>The CPE container is only provided on systems with the \u201ccontainer engine\u201d container runtime, which is currently the Grace-Hopper systems Daint, Clariden and Santis.</p>"},{"location":"software/prgenv/cpe/#how-to-use","title":"How to use","text":"<p>Before you start</p> <p>To use the CPE containers as documented below, you need to set the <code>EDF_PATH</code> environment variable. <pre><code>export EDF_PATH=/capstor/scratch/cscs/anfink/shared/cpe/edf:$EDF_PATH\n</code></pre> Note that the current location of the EDF files is temporary, to work around a file system bug. The environment variable will not be required once this issue is fixed.</p> <p>To start a session with the CPE (see available-versions above): <pre><code>$ srun --environment=cpe-cray-24.07 --pty bash\n</code></pre> Once the container starts up you can directly use the programming environment, because there will be modules loaded by default at startup.</p> <pre><code>$ srun -p debug --environment=cpe-cray-24.07 --pty bash\n$ module list\n\nCurrently Loaded Modules:\n  1) craype-arm-grace     4) cce/18.0.0          7) cray-mpich/8.1.30\n  2) craype-network-ofi   5) craype/2.7.32       8) cuda/12.6\n  3) xpmem/2.9.6          6) PrgEnv-cray/8.5.0   9) craype-accel-nvidia90\n\n\n$ module avail\n\n---- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/17.0/ofi/1.0/cray-mpich/8.0 ----\n   cray-hdf5-parallel/1.14.3.1    cray-parallel-netcdf/1.12.3.13\n\n---------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/17.0/ofi/1.0 ----------\n   cray-mpich-abi/8.1.30    cray-mpich/8.1.30 (L)\n\n------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/17.0 -------------\n   cray-hdf5/1.14.3.1    cray-libsci/24.07.0\n\n------------------ /opt/cray/pe/lmod/modulefiles/mix_compilers ------------------\n   cce-mixed/18.0.0\n\n---------------- /opt/cray/pe/lmod/modulefiles/cpu/arm-grace/1.0 ----------------\n   cray-fftw/3.3.10.8\n\n------------- /opt/cray/pe/lmod/modulefiles/craype-targets/default --------------\n   craype-accel-amd-gfx908        craype-hugepages16M     craype-network-none\n   craype-accel-amd-gfx90a        craype-hugepages1G      craype-network-ofi  (L)\n   craype-accel-amd-gfx940        craype-hugepages256M    craype-network-ucx\n   craype-accel-amd-gfx942        craype-hugepages2G      craype-x86-genoa\n   craype-accel-host              craype-hugepages2M      craype-x86-milan-x\n   craype-accel-nvidia70          craype-hugepages32M     craype-x86-milan\n   craype-accel-nvidia80          craype-hugepages4M      craype-x86-rome\n   craype-accel-nvidia90   (L)    craype-hugepages512M    craype-x86-spr-hbm\n   craype-arm-grace        (L)    craype-hugepages64M     craype-x86-spr\n   craype-hugepages128M           craype-hugepages8M      craype-x86-trento\n\n---------------------- /opt/cray/pe/lmod/modulefiles/core -----------------------\n   PrgEnv-cray/8.5.0 (L)    cray-libsci_acc/24.07.0    cray-python/3.11.7\n   cce/18.0.0        (L)    cray-pmi/6.1.15.19         craype/2.7.32      (L)\n\n----------------------------- /opt/cray/modulefiles -----------------------------\n   xpmem/2.9.6 (L)\n\n----------------------------- /opt/cscs/modulefiles -----------------------------\n   cuda/12.6 (L)\n\n$ CC --version\nCray clang version 18.0.0  (0e4696aa65fa9549bd5e19c216678cc98185b0f7)\nTarget: aarch64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /opt/cray/pe/cce/18.0.0/cce-clang/aarch64/share/../bin\n</code></pre> <p>The recommended way of using CPE in a container is to start the container, and use <code>$SCRATCH</code> and <code>$STORE</code> to interact with persistent data. Please remember that any data that is written to a directory that is not mounted from the host system will be lost, after the container stops.</p> <p>Note</p> <p>By default, the paths <code>/capstor</code>, <code>/iopsstor</code> are mounted to the same paths inside the container.</p> <p>Additionally <code>/users</code> will be mounted at <code>/users.host</code>, so you can access data in your home folder, but with a slightly different path. This is on purpose, and you can override this behaviour by writing your own EDF file, especially using the key <code>base_environment</code>, referencing the predefined CPE environment files and override what you would like to change.</p>"},{"location":"software/prgenv/julia/","title":"julia","text":""},{"location":"software/prgenv/julia/#julia","title":"julia","text":"<p>The <code>julia</code> uenv provides a complete HPC setup for running Julia efficiently at scale, using the supercomputer hardware optimally.  Unlike in traditional approaches, this Julia HPC setup enables you to update Julia yourself using the included preconfigured community tool <code>juliaup</code>.  It also does not preinstall any packages site-wide. Instead, for HPC key packages that benefit from using locally built libraries (<code>MPI.jl</code>, <code>CUDA.jl</code>, <code>AMDGPU.jl</code>, <code>HDF5.jl</code>, <code>ADIOS2.jl</code>, etc.), this uenv provides the libraries and presets package preferences and environment variables for an automatic optimal installation and usage of these packages using these local libraries.  As a result, you only need to type, e.g., <code>] add CUDA</code> in the Julia REPL, in order to install <code>CUDA.jl</code> optimally.  The <code>julia</code> uenv internally relies on the community scripting project JUHPC to achieve this.</p>"},{"location":"software/prgenv/julia/#versioning","title":"Versioning","text":"<p>The naming scheme is <code>julia/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> has the <code>YY.M[M]</code> format, for example September 2024 is <code>24.9</code>, and May 2025 would be <code>25.5</code>.  The release schedule is not fixed; new versions will be released, when there is a compelling reason to update.</p> version node types system 24.9 gh200, zen2 daint, eiger, todi 25.5 gh200, zen2 daint, eiger, santis, clariden, bristen 25.5 <p>The key updates in version <code>25.5:v1</code> from the version <code>24.9</code> were:</p> <ul> <li>enabling compatibility with the latest <code>uenv</code> version <code>8.0</code></li> <li>changing the installation directory</li> <li>adding the <code>jupyter</code> view</li> <li>upgrading to <code>cuda@12.8</code> and <code>cray-mpich@8.1.30</code></li> </ul> <p>HPC key libraries included</p> <ul> <li>cray-mpich/8.1.30</li> <li>cuda/12.8.0</li> <li>hdf5/1.14.5</li> <li>adios2/2.10.2</li> </ul>"},{"location":"software/prgenv/julia/#how-to-use","title":"How to use","text":"<p>Find and pull a Julia uenv image, e.g.: <pre><code>uenv image find julia       # list available julia images\nuenv image pull julia/25.5  # copy version[:tag] from the list above\n</code></pre></p> <p>Start the image and activate the Julia[up] HPC setup by loading the following view(s):</p> <code>juliaup</code><code>juliaup</code> and <code>modules</code> <pre><code>uenv start julia/25.5:v1 --view=juliaup\n</code></pre> <p>This activates also modules for the available libraries like, e.g, <code>cuda</code>.</p> <pre><code>uenv start julia/25.5:v1 --view=juliaup,modules\n</code></pre> <p>There is also a view <code>jupyter</code> available, which is required for using Julia in JupyterHub.</p> <p>Automatic installation of Juliaup and Julia</p> <p>The installation of <code>juliaup</code> and the latest <code>julia</code> version happens automatically the first time when <code>juliaup</code> is called: <pre><code>    juliaup\n</code></pre></p> <p>Note that the <code>julia</code> uenv is built extending the <code>prgenv-gnu</code> uenv.  As a result, it provides also all the features of <code>prgenv-gnu</code>.  Please see the <code>prgenv-gnu</code> documentation for details.  You can for example load the <code>modules</code> view to see the exact versions of the libraries available in the uenv.</p>"},{"location":"software/prgenv/julia/#background-on-julia-for-hpc","title":"Background on Julia for HPC","text":"<p>Julia is a programming language that was designed to solve the \u201ctwo-language problem\u201d, the problem that prototypes written in an interactive high-level language like MATLAB, R or Python need to be partly or fully rewritten in lower-level languages like C, C++ or Fortran when a high-performance production code is required.  Julia, which has its origins at MIT, can however reach the performance of C, C++ or Fortran despite being high-level and interactive.  This is possible thanks to Julia\u2019s just-ahead-of-time compilation: code can be executed in an interactive shell as usual for prototyping languages, but functions and code blocks are compiled to machine code right before their first execution instead of being interpreted (note that modules are pre-compiled).</p> <p>Julia is optimally suited for parallel computing, supporting, e.g., MPI (via <code>MPI.jl</code>) and threads similar to OpenMP.  Moreover, Julia\u2019s GPU packages (<code>CUDA.jl</code>, <code>AMDGPU.jl</code>, etc.) enables writing native Julia code for GPUs [1], which can reach similar efficiency as CUDA C/C++ [2] or the analog for other vendors.  Julia was shown to be suitable for scientific GPU supercomputing at large scale, enabling near optimal performance and nearly ideal scaling on thousands of GPUs on Piz Daint [2,3,4,5].  Packages like ParallelStencil.jl [4] and ImplicitGlobalGrid.jl [3] enable to unify prototype and high-performance production code in one single codebase.  Furthermore, Julia permits direct calling of C/C++ and Fortran libraries without glue code.  It also features similar interfaces to prototyping languages as, e.g., Python, R and MATLAB.  Finally, the Julia PackageCompiler enables to compile Julia modules in order to create shared libraries that are callable from C or other languages (a comprehensive Proof of Concept was already available in 2018 and the PackageCompiler has matured very much since).</p>"},{"location":"software/prgenv/julia/#references","title":"References","text":"<p>[1] Besard, T., Foket, C., &amp; De Sutter B. (2018). Effective Extensible Programming: Unleashing Julia on GPUs. IEEE Transactions on Parallel and Distributed Systems, 30(4), 827-841</p> <p>[2] R\u00e4ss, L., Omlin, S., &amp; Podladchikov, Y. Y. (2019). Porting a Massively Parallel Multi-GPU Application to Julia: a 3-D Nonlinear Multi-Physics Flow Solver. JuliaCon Conference, Baltimore, US.</p> <p>[3] Omlin, S., R\u00e4ss, L., Utkin I. (2024). Distributed Parallelization of xPU Stencil Computations in Julia. The Proceedings of the JuliaCon Conferences, 6(65), 137, https://doi.org/10.21105/jcon.00137</p> <p>[4] Omlin, S., R\u00e4ss, L. (2024). High-performance xPU Stencil Computations in Julia. The Proceedings of the JuliaCon Conferences, 6(64), 138, https://doi.org/10.21105/jcon.00138</p> <p>[5] Omlin, S., R\u00e4ss, L., Kwasniewski, G., Malvoisin, B., &amp; Podladchikov, Y. Y. (2020). Solving Nonlinear Multi-Physics on GPU Supercomputers with Julia. JuliaCon Conference, virtual.</p>"},{"location":"software/prgenv/linalg/","title":"linalg","text":""},{"location":"software/prgenv/linalg/#linalg","title":"linalg","text":"<p>The <code>linalg</code> and <code>linalg-complex</code> uenvs are similar to the <code>prgenv-gnu</code> and <code>prgenv-nvfortran</code> uenvs in that they don\u2019t provide a specific application, but common libraries useful as a base for building other applications. They contain linear algebra and mesh partitioning libraries for a broad range of use cases.</p> <p>The two uenvs contain otherwise identical packages, except that <code>linalg-complex</code> contains <code>petsc</code> and <code>trilinos</code> with complex types enabled, but without the <code>hypre</code> package. <code>hypre</code> only supports double precision. See below for the full list of packages in each version of the uenv. Note that many of the packages available in <code>linalg</code> and <code>linalg-complex</code> are also available in <code>prgenv-gnu</code>.</p>"},{"location":"software/prgenv/linalg/#versioning","title":"Versioning","text":"<p>The uenvs are available in the following versions on the following systems:</p> version node types system 25.10 gh200, zen2 daint, eiger 24.11 gh200, zen2 daint, eiger 25.1024.11 <p>In version 25.10, the common set of packages in both uenvs is:</p> <ul> <li>arpack-ng@3.9.1</li> <li>blaspp@2025.05.28</li> <li>blt@0.7.1</li> <li>boost@1.88.0</li> <li>camp@2025.03.0</li> <li>cmake@3.31.8</li> <li>cray-mpich@8.1.32</li> <li>dla-future@0.10.0</li> <li>dla-future-fortran@0.5.0</li> <li>eigen@3.4.0</li> <li>fftw@3.3.10</li> <li>fmt@11.0.2</li> <li>gsl@2.8</li> <li>hdf5@1.14.6</li> <li>hwloc@2.12.2</li> <li>kokkos@4.3.01</li> <li>kokkos-kernels@4.3.01</li> <li>kokkos-tools@develop</li> <li>lapackpp@2025.05.28</li> <li>libfabric@1.22.0</li> <li>libtree@3.1.1</li> <li>libxml2@2.13.5</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>lzo@2.10</li> <li>meson@1.8.5</li> <li>metis@5.1.0</li> <li>mimalloc@3.1.5</li> <li>mumps@5.8.1</li> <li>nco@5.3.4</li> <li>netcdf-c@4.9.3</li> <li>netlib-scalapack@2.2.2</li> <li>ninja@1.13.0</li> <li>openblas@0.3.30</li> <li>osu-micro-benchmarks@7.5.1</li> <li>p4est@2.8</li> <li>parmetis@4.0.3</li> <li>petsc@3.24.0</li> <li>pika@0.34.0</li> <li>python@3.14.0</li> <li>stdexec@25.03.rc1</li> <li>suite-sparse@7.3.1</li> <li>superlu@7.0.0</li> <li>superlu-dist@9.1.0</li> <li>swig@4.1.1</li> <li>trilinos@16.0.0</li> <li>umpire@2025.03.0</li> <li>zlib-ng@2.2.4</li> </ul> <p>In version 24.11, the common set of packages in both uenvs is:</p> <ul> <li>arpack-ng</li> <li>aws-ofi-nccl</li> <li>blaspp</li> <li>blt</li> <li>boost</li> <li>camp</li> <li>cmake</li> <li>cuda</li> <li>dla-future-fortran</li> <li>dla-future</li> <li>eigen</li> <li>fftw</li> <li>fmt</li> <li>gsl</li> <li>hdf5</li> <li>hwloc</li> <li>kokkos-kernels</li> <li>kokkos-tools</li> <li>kokkos</li> <li>lapackpp</li> <li>libtree</li> <li>lua</li> <li>lz4</li> <li>meson</li> <li>metis</li> <li>mimalloc</li> <li>mumps</li> <li>nccl-tests</li> <li>nccl</li> <li>nco</li> <li>netcdf-c</li> <li>netlib-scalapack</li> <li>ninja</li> <li>openblas</li> <li>osu-micro-benchmarks</li> <li>p4est</li> <li>papi</li> <li>parmetis</li> <li>petsc</li> <li>pika</li> <li>python</li> <li>slepc</li> <li>spdlog</li> <li>stdexec</li> <li>suite-sparse</li> <li>superlu-dist</li> <li>superlu</li> <li>swig</li> <li>trilinos</li> <li>umpire</li> <li>whip</li> <li>zlib-ng</li> </ul>"},{"location":"software/prgenv/linalg/#how-to-use","title":"How to use","text":"<p>Using the <code>linalg</code> and <code>linalg-complex</code> uenvs is similar to <code>prgenv-gnu</code>. Like <code>prgenv-gnu</code>, the <code>linalg</code> and <code>linalg-complex</code> uenvs provide <code>default</code> and <code>modules</code> views. Please see the <code>prgenv-gnu</code> documentation for details on different ways of accessing the packages available in the uenv. You can for example load the <code>modules</code> view to see the exact versions of the packages available in the uenv.</p>"},{"location":"software/prgenv/prgenv-gnu/","title":"prgenv-gnu","text":""},{"location":"software/prgenv/prgenv-gnu/#prgenv-gnu","title":"prgenv-gnu","text":"<p>Provides a set of tools and libraries built around the GNU compiler toolchain. It is the go to programming environment on all systems and target node types, that is it is the first that you should try out when starting to compile an application or create a python virtual environment.</p> <p>alternatives to prgenv-gnu</p> <p>The <code>prgenv-nvfortran</code> is for applications that require the NVIDIA Fortran compiler - typically because they need to use OpenACC or CUDA Fortran.</p> <p>The <code>linalg</code> environment is similar to prgenv-gnu, with additional linear algebra and mesh partitioning algorithms.</p> <p></p>"},{"location":"software/prgenv/prgenv-gnu/#versioning","title":"Versioning","text":"<p>The naming scheme is <code>prgenv-gnu/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> has the <code>YY.M[M]</code> format, for example November 2024 is <code>24.11</code>, and January 2025 would be <code>25.1</code>.</p> <p>The release schedule is not fixed, with new versions will be released roughly every 3-6 months, when there is a compelling reason to update.</p> version node types system 25.6 gh200 daint, eiger, santis, clariden 24.11 a100, gh200, zen2 daint, eiger, santis, clariden, bristen 24.7 gh200, zen2 daint, eiger, todi"},{"location":"software/prgenv/prgenv-gnu/#deprecation-policy","title":"Deprecation policy","text":"<p>We will provide full support for 12 months after the uenv image is released, and remove the images when they are no longer being used or when system upgrades break their functionality on the system.</p> <ul> <li>It is recommended to document how you compiled and set up your workflow using <code>prgenv-gnu</code> so that you can recreate it with future versions.</li> <li>The 24.7 release is no longer supported and will be removed at the end of July 2025 - users are encouraged to update to 24.11 or later, before requesting support.</li> </ul>"},{"location":"software/prgenv/prgenv-gnu/#versions","title":"Versions","text":"25.624.11 <p>The key updates in version 25.6 compared to 24.11 are:</p> <ul> <li>upgrading GCC to version 14 and CUDA to version 12.9</li> <li>upgrading cray-mpich to version 8.1.32</li> <li>adding xcb-util-cursor to the default view to allow the NVIDIA Nsight UIs to be used without manual workarounds</li> </ul> <p>The spack version used to build the packages was also upgraded to 1.0.</p> <p>v2 of the uenv fixes the <code>modules</code> view, where <code>gcc</code> was missing in v1.</p> all packages exposed via the <code>default</code> and <code>modules</code> views in <code>v1</code> <ul> <li>aws-ofi-nccl@1.16.0</li> <li>boost@1.88.0</li> <li>cmake@3.31.8</li> <li>cray-mpich@8.1.32</li> <li>cuda@12.9.0</li> <li>fftw@3.3.10</li> <li>fmt@11.2.0</li> <li>gcc@14.2.0</li> <li>gsl@2.8</li> <li>hdf5@1.14.6</li> <li>kokkos@4.6.01</li> <li>kokkos-kernels@4.6.01</li> <li>kokkos-tools@develop</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.7.0</li> <li>nccl@2.27.5-1</li> <li>nccl-tests@2.16.3</li> <li>netcdf-c@4.9.2</li> <li>netcdf-cxx@4.2</li> <li>netcdf-fortran@4.6.1</li> <li>netlib-scalapack@2.2.2</li> <li>ninja@1.12.1</li> <li>openblas@0.3.29</li> <li>osu-micro-benchmarks@7.5</li> <li>papi@7.1.0</li> <li>python@3.13.5</li> <li>superlu@7.0.0</li> <li>xcb-util-cursor@0.1.5</li> <li>zlib-ng@2.2.4</li> </ul> <p>The key updates in version 24.11:v1 from the 24.7 version were:</p> <ul> <li>upgrading the versions of gcc@13 and cuda@12.6</li> <li>upgrading cray-mpich to version 8.1.30</li> <li>adding kokkos</li> <li>adding gsl</li> </ul> all packages exposed via the <code>default</code> and <code>modules</code> views in <code>v1</code> <ul> <li>aws-ofi-nccl@git.v1.9.2-aws_1.9.2</li> <li>boost@1.86.0</li> <li>cmake@3.30.5</li> <li>cray-mpich@8.1.30</li> <li>cuda@12.6.2<ul> <li>in the <code>gh200</code> and <code>a100</code> images</li> </ul> </li> <li>fftw@3.3.10</li> <li>fmt@11.0.2</li> <li>gcc@13.3.0</li> <li>gsl@2.8</li> <li>hdf5@1.14.5</li> <li>kokkos-kernels@4.4.01</li> <li>kokkos-tools@develop</li> <li>kokkos@4.4.01</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.5.1</li> <li>nccl-tests@2.13.6</li> <li>nccl@2.22.3-1</li> <li>netlib-scalapack@2.2.0</li> <li>ninja@1.12.1</li> <li>openblas@0.3.28<ul> <li>built with the OpenMP threading back end</li> </ul> </li> <li>osu-micro-benchmarks@5.9</li> <li>papi@7.1.0</li> <li>python@3.12.5</li> <li>superlu@5.3.0</li> <li>zlib-ng@2.2.1</li> </ul> 24.7:v2 changelog <p>The <code>v2</code> update added <code>netcdf</code>, specifically the following packages:</p> <ul> <li>netcdf-c@4.9.2</li> <li>netcdf-cxx@4.2</li> <li>netcdf-fortran@4.6.1</li> </ul> <p></p>"},{"location":"software/prgenv/prgenv-gnu/#how-to-use","title":"How to use","text":"<p>The environment is designed as a fairly minimal set of </p> <p>There are three ways to access the software provided by prgenv-gnu, once it has been started.</p> the <code>default</code> viewmodulesSpack <p>The simplest way to get started is to use the <code>default</code> file system view, which automatically loads all of the packages when the uenv is started.</p> <p>test mpi compilers and python provided by prgenv-gnu/24.11</p> <pre><code># start using the default view\n$ uenv start --view=default prgenv-gnu/25.6:v1\n\n# the python executable provided by the uenv is the default, and is a recent version\n$ which python\n/user-environment/env/default/bin/python\n$ python --version \nPython 3.13.5\n\n# the mpi compiler wrappers are also available\n$ which mpicc\n/user-environment/env/default/bin/mpicc\n$ mpicc --version\ngcc (Spack GCC) 14.2.0\n$ gcc --version # the compiler wrapper uses the gcc provided by the uenv\ngcc (Spack GCC) 14.2.0\n</code></pre> <p>The uenv provides modules for all of the software packages, which can be made available by using the <code>modules</code> view in  No modules are loaded when a uenv starts, and have to be loaded individually using <code>module load</code>.</p> <p>starting prgenv-gnu and listing the provided modules</p> <pre><code>$ uenv start prgenv-gnu/25.6:v1 --view=modules\n$ module avail\n    ---------------------------- /user-environment/modules ----------------------------\n       aws-ofi-nccl/1.16.0      meson/1.7.0\n       boost/1.88.0             nccl-tests/2.16.3\n       cmake/3.31.8             nccl/2.27.5-1\n       cray-mpich/8.1.32        netcdf-c/4.9.2\n       cuda/12.9.0              netcdf-cxx/4.2\n       fftw/3.3.10              netcdf-fortran/4.6.1\n       fmt/11.2.0               netlib-scalapack/2.2.2\n       gsl/2.8                  ninja/1.12.1\n       hdf5/1.14.6              openblas/0.3.29\n       kokkos-kernels/4.6.01    osu-micro-benchmarks/7.5\n       kokkos-tools/develop     papi/7.1.0\n       kokkos/4.6.01            python/3.13.5\n       libfabric/1.22.0         squashfs/4.6.1\n       libtree/3.1.1            superlu/7.0.0\n       lua/5.4.6                xcb-util-cursor/0.1.5\n       lz4/1.10.0               zlib-ng/2.2.4\n</code></pre> <p>The gnu programming environment is a very good base for building software with Spack, because it provides compilers, MPI, Python and common packages like hdf5.</p> <p>Check out the guide for using Spack with uenv.</p>"},{"location":"software/prgenv/prgenv-nvfortran/","title":"prgenv-nvfortran","text":""},{"location":"software/prgenv/prgenv-nvfortran/#prgenv-nvfortran","title":"prgenv-nvfortran","text":"<p>The <code>prgenv-nvfortran</code> uenv provides a set of tools and libraries for building applications that need the NVIDIA Fortran compiler Specifically, it is intended for building and running applications that require one of the following: * OpenACC for GPU acceleration; * CUDA Fortran for GPU acceleration.</p> <p>Note</p> <p>By default, use the <code>prgenv-gnu</code> toolchain for a generic environment for building GPU applications. It provides CUDA and libraries with GPU support enabled for the Grace-Hopper nodes, the gnu compiler toolchain that it provides has better C and C++ standards compliance, and it also provides more libraries and tools than this <code>nvfortran</code> uenv.</p>"},{"location":"software/prgenv/prgenv-nvfortran/#versioning","title":"Versioning","text":"<p>The naming scheme is <code>prgenv-nvfortran/&lt;version&gt;:v&lt;i&gt;</code>, where <code>&lt;version&gt;</code> matches the version of the NVIDIA HPC SDK.</p> <ul> <li>the SDK is released every two months, and is numbered in the <code>YY.M[M]</code> format, e.g. <code>24.1</code> and <code>24.11</code>.</li> <li>the <code>prgenv-nvfortran</code> will be released two or three times a year (every second NVHPC release).</li> </ul> <p>The currently supported versions are:</p> <code>prgenv-nvfortran</code> NVHPC 24.11 24.11 25.7 25.7 I need a different version <p>If you need a version of the NVHPC SDK that is not provided, e.g. the 25.1 version that falls between 24.11 and 25.4, make a request on the CSCS service desk.</p> 25.724.11 <p>Version 24.11 provides the following software:</p> <ul> <li>cmake@3.31.8</li> <li>cray-mpich@8.1.32</li> <li>cuda@12.9.0</li> <li>fftw@3.3.10</li> <li>fmt@11.2.0</li> <li>gcc@13.4.0</li> <li>hdf5@1.14.6</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.7.0</li> <li>netcdf-c@4.9.2</li> <li>netcdf-fortran@4.6.1</li> <li>ninja@1.12.1</li> <li>nvhpc@25.7</li> <li>nvpl-blas@0.4.0.1</li> <li>nvpl-lapack@0.3.0</li> <li>osu-micro-benchmarks@7.5</li> <li>python@3.13.5</li> <li>zlib-ng@2.2.4</li> </ul> <p>The package main update from the previous version 24.11 is the addition of NCCL, the addition of <code>libfabric</code> to the uenv, and the SquashFS tools:</p> <ul> <li>aws-ofi-nccl@1.16.0</li> <li>nccl-tests@2.16.3</li> <li>nccl@2.27.5-1</li> <li>libfabric@1.22.0</li> <li>squashfs@4.6.1</li> </ul> <p>Note</p> <p>Version <code>25.7</code> upgraded to Spack 1.0, so any Spack workflows based on the previous version will probably need to be updated to the latest Spack version.</p> <p>Version 24.11 provides the following software:</p> <ul> <li>cmake@3.30.5</li> <li>cray-mpich@8.1.30</li> <li>cuda@12.6.0</li> <li>fftw@3.3.10</li> <li>fmt@11.0.2</li> <li>gcc@13.2.0</li> <li>hdf5@1.14.5</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.5.1</li> <li>netcdf-c@4.9.2</li> <li>netcdf-fortran@4.6.1</li> <li>ninja@1.12.1</li> <li>nvhpc@24.11</li> <li>nvpl-blas@0.3.0</li> <li>nvpl-lapack@0.2.3.1</li> <li>osu-micro-benchmarks@5.9</li> <li>python@3.12.5</li> <li>zlib-ng@2.2.1</li> </ul>"},{"location":"software/prgenv/prgenv-nvfortran/#how-to-use","title":"How to use","text":"<p>The image is only provided on Alps systems that have NVIDIA GPUs. To see which versions have been installed on a system:</p> <pre><code># search for uenv on the current system\nuenv image find prgenv-nvfortran\n\n# search for uenv on all systems\nuenv image find prgenv-nvfortran@*\n\n# pull a version\nuenv image pull prgenv-nvfortran/25.7:v1\n</code></pre> the <code>nvfort</code> viewthe modules view <p>The <code>nvfort</code> view loads all of the packages into your environment (equivalent to loading all the modules at once):</p> <pre><code>uenv start prgenv-nvfortran/25.7:v1 --view=nvfort\nmpif90 --version\n</code></pre> <p>The above example shows that the MPI compiler wrappers are using the underlying NVIDIA compiler. The following wrappers are available:</p> <ul> <li><code>mpif77</code></li> <li><code>mpif90</code></li> <li><code>mpifort</code></li> </ul> <p>And the following C/C++ wrappers are available:</p> <ul> <li><code>mpicc</code></li> <li><code>mpicxx</code></li> </ul> <p>The modules view will start the uenv, and make a set of modules available:</p> <pre><code>$ uenv start prgenv-nvfortran/25.7:v1 --view=nvfort,modules\n$ module avail\n---------------------------- /user-environment/modules ------------------------\n     aws-ofi-nccl/1.16.0    libtree/3.1.1           nvhpc/25.7\n     cmake/3.31.8           lua/5.4.6               nvpl-blas/0.4.0.1\n     cray-mpich/8.1.32      lz4/1.10.0              nvpl-lapack/0.3.0\n     cuda/12.9.0            meson/1.7.0             osu-micro-benchmarks/7.5\n     fftw/3.3.10            nccl-tests/2.16.3       python/3.13.5\n     fmt/11.2.0             nccl/2.27.5-1           squashfs/4.6.1\n     gcc/13.4.0             netcdf-c/4.9.2          zlib-ng/2.2.4\n     hdf5/1.14.6            netcdf-fortran/4.6.1\n     libfabric/1.22.0       ninja/1.12.1\n</code></pre> <p>None of the modules are loaded by default, so you will have to load the required modules</p>"},{"location":"software/sciapps/","title":"Index","text":""},{"location":"software/sciapps/#scientific-applications","title":"Scientific Applications","text":"<p>CSCS provides and supports a selection of scientific applications on the computing systems: we usually build community codes that are adopted by several users on our systems.</p> <p>Please have a look at the individual application page on the menu to find out how to run in a production environment.</p> <p>CSCS staff can also help users with performance analysis to optimise their workflow in production.</p> <ul> <li>CP2K</li> <li>GROMACS</li> <li>LAMMPS</li> <li>NAMD</li> <li>Quantum ESPRESSO</li> <li>VASP</li> </ul>"},{"location":"software/sciapps/cp2k/","title":"CP2K","text":""},{"location":"software/sciapps/cp2k/#cp2k","title":"CP2K","text":"<p>CP2K is supported software on Alps. See the main applications page for more information.</p> <p>CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.</p> <p>CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. See CP2K Features for a detailed overview.</p> <p>uenvs</p> <p>CP2K is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p> <p>Known issues</p> <p>Please check CP2K\u2019s known issues and whether they are relevant to your work. They may impact your calculations in subtle ways, potentially leading to a waste of resources.</p> Changelog 2025.1 <ul> <li>The <code>cp2k@2025.1:v1</code> uenv has been removed and replaced by <code>cp2k@2025.1:v2</code><ul> <li>The DLA-Future integration required special care by the user; without it, the performance is sub-optimal</li> <li>The DLA-Future integration has been moved to a separate <code>cp2k-dlaf</code> view</li> <li>The two views available in <code>cp2k@2025.1:v1</code> are still available, without DLA-Future: <code>cp2k</code> and <code>develop</code></li> <li>Two new views with DLA-Future are available in <code>cp2k@2025.1:v2</code>: <code>cp2k-dlaf</code> and <code>develop-dlaf</code></li> </ul> </li> <li>The default BLAS/LAPACK library on Eiger is now OpenBLAS (changed from Intel oneAPI MKL, oneMKL)<ul> <li>Performance on some workloads is better and on others is comparable</li> <li>Brings uniformity across platforms </li> </ul> </li> </ul>"},{"location":"software/sciapps/cp2k/#dependencies","title":"Dependencies","text":"<p>On our systems, CP2K is built with the following dependencies:</p> <ul> <li>COSMA</li> <li>Cray MPICH</li> <li>DBCSR</li> <li>DLA-Future (from <code>cp2k@2025.1</code> onwards, only in <code>cp2k-dlaf</code> view)</li> <li>dftd4 (from <code>cp2k@2025.1</code> onwards)</li> <li>ELPA</li> <li>FFTW</li> <li>Libxc</li> <li>libint</li> <li>OpenBLAS</li> <li>PLUMED (from <code>cp2k@2024.1</code> onwards)</li> <li>ScaLAPACK</li> <li>SIRIUS</li> <li>Spglib</li> <li>spla</li> </ul> <p>GPU-aware MPI</p> <p>COSMA and DLA-Future are built with GPU-aware MPI, which requires setting <code>MPICH_GPU_SUPPORT_ENABLED=1</code>. On Daint, <code>MPICH_GPU_SUPPORT_ENABLED=1</code> is set by default.</p> <p>CUDA cache path for JIT compilation</p> <p>DBCSR uses JIT compilation for CUDA kernels. The default location is in the home directory, which can put unnecessary burden on the filesystem and lead to performance degradation. Because of this we set <code>CUDA_CACHE_PATH</code> to point to the in-memory filesystem in <code>/dev/shm</code>. On Daint, <code>CUDA_CACHE_PATH</code> is set to a directory under <code>/dev/shm</code> by default.</p>"},{"location":"software/sciapps/cp2k/#running-cp2k","title":"Running CP2K","text":""},{"location":"software/sciapps/cp2k/#running-on-daint","title":"Running on Daint","text":"<p>Daint nodes have four GH200 GPUs that have to be configured properly for best performance. To start a job, two bash scripts are potentially required: a Slurm submission script, and a wrapper to start the CUDA MPS daemon so that multiple MPI ranks can use the same GPU.</p> run_cp2k.sh<pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=cp2k-job\n#SBATCH --time=00:30:00 (1)\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=32 (2)\n#SBATCH --cpus-per-task=8 (3)\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --no-requeue\n#SBATCH --uenv=&lt;CP2K_UENV&gt;\n#SBATCH --view=cp2k\n\nexport CUDA_CACHE_PATH=\"/dev/shm/$USER/cuda_cache\" # (5)!\nexport MPICH_GPU_SUPPORT_ENABLED=1 # (6)!\nexport MPICH_MALLOC_FALLBACK=1\nexport OMP_NUM_THREADS=$((SLURM_CPUS_PER_TASK - 1)) # (4)!\n\nulimit -s unlimited\nsrun --cpu-bind=socket ./mps-wrapper.sh cp2k.psmp -i &lt;CP2K_INPUT&gt; -o &lt;CP2K_OUTPUT&gt;\n</code></pre> <ol> <li> <p>Time format: <code>HH:MM:SS</code></p> </li> <li> <p>Number of MPI ranks per node</p> </li> <li> <p>Number of CPUs per MPI ranks</p> </li> <li> <p>OpenBLAS spawns an extra thread, therefore it is necessary to set <code>OMP_NUM_THREADS</code> to <code>SLURM_CPUS_PER_TASK - 1</code>    for good performance. With Intel MKL, this is not necessary and one can set <code>OMP_NUM_THREADS</code> to    <code>SLURM_CPUS_PER_TASK</code>.</p> </li> <li> <p>DBCSR relies on extensive JIT compilation, and we store the cache in memory to avoid I/O overhead.    This is set by default on Daint, but it\u2019s set here explicitly as it\u2019s essential to avoid performance degradation.</p> </li> <li> <p>CP2K\u2019s dependencies use GPU-aware MPI, which requires enabling support at runtime.    This is set by default on Daint, but it\u2019s set here explicitly as it\u2019s a requirement in general for enabling GPU-aware MPI.</p> </li> <li> <p>Change  to your project account name <li>Change <code>&lt;CP2K_UENV&gt;</code> to the name (or path) of the actual CP2K uenv you want to use</li> <li>Change <code>&lt;PATH_TO_CP2K_DATA_DIR&gt;</code> to the actual path to the CP2K data directory</li> <li>Change <code>&lt;CP2K_INPUT&gt;</code> and <code>&lt;CP2K_OUTPUT&gt;</code> to the actual input and output files</li> <p>With the above scripts, you can launch a CP2K calculation on 4 nodes, with 32 MPI ranks per node and 8 OpenMP threads per rank with</p> <pre><code>sbatch run_cp2k.sh\n</code></pre> <p>Note</p> <p>The <code>mps-wrapper.sh</code> script, required to properly over-subscribe the GPU, is provided at the following page:  NVIDIA GH200 GPU nodes: multiple ranks per GPU.</p> <p>Warning</p> <p>The <code>--cpu-bind=socket</code> option is necessary to get good performance.</p> <p>Warning</p> <p>Each GH200 node has 4 modules, each of them composed of a ARM Grace CPU with 72 cores and a H200 GPU directly attached to it. Please see Alps hardware for more information. It is important that the number of MPI ranks passed to Slurm with <code>--ntasks-per-node</code> is a multiple of 4.</p> Note <p>In the example above, we use 32 MPI ranks with 8 OpenMP threads, for a total of 64 cores per GPU and 256 cores  per node. Experiments have shown that CP2K performs and scales better when the number of MPI ranks is a power  of 2, even if some cores are left idling. </p> Running regression tests <p>If you want to run CP2K regression tests with the CP2K executable provided by the uenv, make sure to use the version of the regression tests corresponding to the version of CP2K provided by the uenv. The regression test data is sometimes adjusted, and using the wrong version of the regression tests can lead to test failures.</p> Scaling of <code>QS/H2O-1024</code> benchmark <p>The <code>QS/H2O-1024</code> benchmark is a DFT molecular dynamics simulation of liquid water. It relies on DBCSR for block sparse matrix-matrix multiplication.</p> <p>All calculations were run with 32 MPI ranks per node, and 8 OpenMP threads per rank (best configuration for this benchmark). </p> <p>Note</p> <p><code>H2O-102.inp</code> is the largest example of DFT molecular dynamics simulation of liquid water that fits on a single GH200 node.</p> Number of nodes Wall time (s) Speedup Efficiency 1 793.1 1.00 1.00 2 535.2 1.48 0.74 4 543.9 1.45 0.36 8 487.3 1.64 0.20 16 616.7 1.28 0.08 <p>Scaling is not ideal on more than two nodes.</p> Scaling of <code>QS_mp2_rpa/128-H2O/H2O-128-RI-MP2-TZ</code> benchmark <p>The <code>QS_mp2_rpa/128-H2O/H2O-128-RI-MP2-TZ</code> benchmark is a straightforward modification of the <code>QS_mp2_rpa/64-H2O/H2O-64-RI-MP2-TZ</code> benchmark.</p> <p>It is a RI-MP2 calculation of a water cluster with 128 atoms.</p> Input file <pre><code>&amp;GLOBAL\n  PRINT_LEVEL MEDIUM\n  PROJECT H2O-128-RI-MP2-TZ\n  RUN_TYPE ENERGY\n&amp;END GLOBAL\n\n&amp;FORCE_EVAL\n  METHOD Quickstep\n  &amp;DFT\n    BASIS_SET_FILE_NAME ./BASIS_H2O\n    POTENTIAL_FILE_NAME ./POTENTIAL_H2O\n    WFN_RESTART_FILE_NAME ./H2O-128-PBE-TZ-RESTART.wfn\n    &amp;MGRID\n      CUTOFF 800\n      REL_CUTOFF 50\n    &amp;END MGRID\n    &amp;QS\n      EPS_DEFAULT 1.0E-12\n    &amp;END QS\n    &amp;SCF\n      EPS_SCF 1.0E-6\n      MAX_SCF 30\n      SCF_GUESS RESTART\n      &amp;OT\n        MINIMIZER CG\n        PRECONDITIONER FULL_ALL\n      &amp;END OT\n      &amp;OUTER_SCF\n        EPS_SCF 1.0E-6\n        MAX_SCF 20\n      &amp;END OUTER_SCF\n      &amp;PRINT\n        &amp;RESTART OFF\n        &amp;END RESTART\n      &amp;END PRINT\n    &amp;END SCF\n    &amp;XC\n      &amp;HF\n        FRACTION 1.0\n        &amp;INTERACTION_POTENTIAL\n          CUTOFF_RADIUS 6.0\n          POTENTIAL_TYPE TRUNCATED\n          T_C_G_DATA ./t_c_g.dat\n        &amp;END INTERACTION_POTENTIAL\n        &amp;MEMORY\n          MAX_MEMORY 16384\n        &amp;END MEMORY\n        &amp;SCREENING\n          EPS_SCHWARZ 1.0E-8\n          SCREEN_ON_INITIAL_P TRUE\n        &amp;END SCREENING\n      &amp;END HF\n      &amp;WF_CORRELATION\n        MEMORY 1200\n        NUMBER_PROC 1\n        &amp;INTEGRALS\n          &amp;WFC_GPW\n            CUTOFF 300\n            EPS_FILTER 1.0E-12\n            EPS_GRID 1.0E-8\n            REL_CUTOFF 50\n          &amp;END WFC_GPW\n        &amp;END INTEGRALS\n        &amp;RI_MP2\n        &amp;END RI_MP2\n      &amp;END WF_CORRELATION\n      &amp;XC_FUNCTIONAL NONE\n      &amp;END XC_FUNCTIONAL\n    &amp;END XC\n  &amp;END DFT\n  &amp;SUBSYS\n    &amp;CELL\n      ABC 15.6404 15.6404 15.6404\n    &amp;END CELL\n    &amp;KIND H\n      BASIS_SET cc-TZ\n      BASIS_SET RI_AUX RI-cc-TZ\n      POTENTIAL GTH-HF-q1\n    &amp;END KIND\n    &amp;KIND O\n      BASIS_SET cc-TZ\n      BASIS_SET RI_AUX RI-cc-TZ\n      POTENTIAL GTH-HF-q6\n    &amp;END KIND\n    &amp;TOPOLOGY\n      COORD_FILE_FORMAT XYZ\n      COORD_FILE_NAME ./H2O-128.xyz\n    &amp;END TOPOLOGY\n  &amp;END SUBSYS\n&amp;END FORCE_EVAL\n</code></pre> <p>All calculations run for this scaling tests were using 32 MPI ranks per node and 8 OpenMP threads per rank.  The smallest amount of nodes necessary to run this calculation is 8.</p> Number of nodes Wall time (s) Speedup Efficiency 8 2037.0 1.00 1.00 16 1096.2 1.85 0.92 32 611.5 3.33 0.83 64 410.5 4.96 0.62 128 290.9 7.00 0.43 <p>MP2 calculations scale well on GH200, up to a large number of nodes (\\(&gt; 50\\%\\) efficiency with 64 nodes).</p> Scaling of <code>QS_mp2_rpa/128-H2O/H2O-128-RI-dRPA-TZ</code> benchmark <p>The <code>QS_mp2_rpa/128-H2O/H2O-128-RI-dRPA-T</code> benchmark is a RPA energy calculation, traditionally used to benchmark the performance of the COSMA library.  It a very large calculation, which requires at least 8 GH200 nodes to run.  The calculations were run with 16 MPI ranks per node and 16 OpenMP threads per rank. For RPA workloads, a higher ratio of threads per rank were beneficial.</p> Number of nodes Wall time (s) Speedup Efficiency 8 575.4 1.00 1.00 16 465.8 1.23 0.61 32 281.1 2.04 0.51 64 205.3 2.80 0.35 128 185.8 3.09 0.19 <p>This RPA input scales well until 32 GH200 nodes.</p>"},{"location":"software/sciapps/cp2k/#running-on-eiger","title":"Running on Eiger","text":"<p>On Eiger, a similar sbatch script can be used:</p> run_cp2k.sh<pre><code>#!/bin/bash -l\n#SBATCH --job-name=cp2k-job\n#SBATCH --time=00:30:00 (1)\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=32 (2)\n#SBATCH --cpus-per-task=4 (3)\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --constraint=mc\n#SBATCH --uenv=&lt;CP2K_UENV&gt;\n#SBATCH --view=cp2k\n\nexport OMP_NUM_THREADS=$((SLURM_CPUS_PER_TASK - 1)) # (4)!\n\nulimit -s unlimited\nsrun --cpu-bind=socket cp2k.psmp -i &lt;CP2K_INPUT&gt; -o &lt;CP2K_OUTPUT&gt;\n</code></pre> <ol> <li> <p>Time format: <code>HH:MM:SS</code></p> </li> <li> <p>Number of MPI ranks per node</p> </li> <li> <p>Number of CPUs per MPI ranks</p> </li> <li> <p>OpenBLAS spawns an extra thread, therefore it is necessary to set <code>OMP_NUM_THREADS</code> to <code>SLURM_CPUS_PER_TASK - 1</code>    for good performance. With Intel MKL, this is not necessary and one can set <code>OMP_NUM_THREADS</code> to    <code>SLURM_CPUS_PER_TASK</code>.</p> </li> <li> <p>Change <code>&lt;ACCOUNT&gt;</code> to your project account name</p> </li> <li>Change <code>&lt;CP2K_UENV&gt;</code> to the name (or path) of the actual CP2K uenv you want to use</li> <li>Change <code>&lt;PATH_TO_CP2K_DATA_DIR&gt;</code> to the actual path to the CP2K data directory</li> <li>Change <code>&lt;CP2K_INPUT&gt;</code> and <code>&lt;CP2K_OUTPUT&gt;</code> to the actual input and output files</li> </ol> <p>Warning</p> <p>The <code>--cpu-bind=socket</code> option is necessary to get good performance.</p> Running regression tests <p>If you want to run CP2K regression tests with the CP2K executable provided by the uenv, make sure to use the version of the regression tests corresponding to the version of CP2K provided by the uenv. The regression test data is sometimes adjusted, and using the wrong version of the regression tests can lead to test failures.</p>"},{"location":"software/sciapps/cp2k/#building-cp2k-from-source","title":"Building CP2K from Source","text":"<p>Warning</p> <p>The following installation instructions are up-to-date with the latest version of CP2K provided by the uenv. That is, they work when manually compiling the CP2K source code corresponding to the CP2K version provided by the uenv. They are not necessarily up-to-date with the latest version of CP2K available on the <code>master</code> branch.</p> <p>If you are trying to build CP2K from source, make sure you understand what is different in <code>master</code> compared to the latest version of CP2K provided by the uenv.</p> <p>The CP2K uenv provides all the dependencies required to build CP2K from source, with several optional features enabled. You can follow these steps to build CP2K from source:</p> <pre><code>uenv start --view=develop &lt;CP2K_UENV&gt; # (1)!\n\ncd &lt;PATH_TO_CP2K_SOURCE&gt; # (2)!\n\nmkdir build &amp;&amp; cd build\nCC=mpicc CXX=mpic++ FC=mpifort cmake \\\n    -GNinja \\\n    -DCMAKE_CUDA_HOST_COMPILER=mpicc \\ # (3)!\n    -DCP2K_USE_LIBXC=ON \\\n    -DCP2K_USE_LIBINT2=ON \\\n    -DCP2K_USE_SPGLIB=ON \\\n    -DCP2K_USE_ELPA=ON \\\n    -DCP2K_USE_SPLA=ON \\\n    -DCP2K_USE_SIRIUS=ON \\\n    -DCP2K_USE_COSMA=ON \\\n    -DCP2K_USE_PLUMED=ON \\\n    -DCP2K_USE_DFTD4=ON \\\n    -DCP2K_USE_DLAF=ON \\\n    -DCP2K_USE_ACCEL=CUDA -DCP2K_WITH_GPU=H100 \\ # (4)!\n    ..\n\nninja -j 32\n</code></pre> <ol> <li> <p>Start the CP2K uenv and load the <code>develop</code> view (which provides all the necessary dependencies)</p> </li> <li> <p>Go to the CP2K source directory</p> </li> <li> <p>The <code>H100</code> option enables the <code>sm_90</code> architecture for the CUDA backend</p> </li> </ol> <p>Eiger: <code>libxsmm</code></p> <p>On <code>x86</code> we deploy with <code>libxmm</code>. Add <code>-DCP2K_USE_LIBXSMM=ON</code> to the CMake invocation to use <code>libxsmm</code>.</p> Eiger: Intel MKL (before <code>cp2k@2025.1</code>) <p>On <code>x86</code> we deployed with <code>intel-oneapi-mkl</code> before <code>cp2k@2025.1</code>.  If you are using a pre-<code>cp2k@2025.1</code> uenv, add <code>-DCP2K_SCALAPACK_VENDOR=MKL</code> to the CMake invocation to find MKL.</p> CUDA architecture for <code>cp2k@2024.1</code> and earlier <p><code>cp2k@2024.1</code> (and earlier) does not support compiling for <code>cuda_arch=90</code>. Use <code>-DCP2K_WITH_GPU=A100</code> instead,  which enables the <code>sm_80</code> architecture.</p> <p>See manual.cp2k.org/CMake for more details.</p>"},{"location":"software/sciapps/cp2k/#known-issues","title":"Known issues","text":""},{"location":"software/sciapps/cp2k/#older-uenv-versions-on-eiger","title":"Older uenv versions on Eiger","text":"<p>After the migration to Eiger.Alps, calculations relying on older uenv versions (<code>2024.1:v1</code>, <code>2024.2:v1</code>, <code>2024.3:v1</code>) sometimes crash unexpectedly with a segmentation fault. The problem has been identify as coming from the <code>libxsmm</code> library, used as a backend in DBCSR. To avoid this issue, it is recommended to upgrade to a newer uenv.</p> <p>In case a specific <code>2024.x</code> version of CP2K is required, crashes can be avoided by switching to the <code>BLAS</code> backend of DBCSR. This can be done by adding the following in the <code>&amp;GLOBAL</code> subsection of the input file:</p> <pre><code>&amp;GLOBAL\n    &amp;DBCSR\n        MM_DRIVER BLAS\n    &amp;END DBCSR\n&amp;END GLOBAL\n</code></pre>"},{"location":"software/sciapps/cp2k/#dla-future","title":"DLA-Future","text":"<p>The <code>cp2k/2025.1:v2</code> uenv provides CP2K with DLA-Future support enabled, in the <code>cp2k-dlaf</code> view. The DLA-Future library is initialized even if you don\u2019t explicitly ask to use it. This can lead to some surprising warnings and failures described below.</p>"},{"location":"software/sciapps/cp2k/#cusolver_status_internal_error-during-initialization","title":"<code>CUSOLVER_STATUS_INTERNAL_ERROR</code> during initialization","text":"<p>If you are heavily over-subscribing the GPU by running multiple ranks per GPU, you may encounter the following error:</p> <pre><code>created exception: cuSOLVER function returned error code 7 (CUSOLVER_STATUS_INTERNAL_ERROR): pika(bad_function_call)\nterminate called after throwing an instance of 'pika::cuda::experimental::cusolver_exception'\nwhat(): cuSOLVER function returned error code 7 (CUSOLVER_STATUS_INTERNAL_ERROR): pika(bad_function_call)\n</code></pre> <p>The reason is that too many cuSOLVER handles are created. If you don\u2019t need DLA-Future, you can manually set the number of BLAS and LAPACK handlers to 1 by setting the following environment variables:</p> <pre><code>DLAF_NUM_GPU_BLAS_HANDLES=1\nDLAF_NUM_GPU_LAPACK_HANDLES=1\n</code></pre>"},{"location":"software/sciapps/cp2k/#warning-about-pika-only-using-one-worker-thread","title":"Warning about pika only using one worker thread","text":"<p>When running CP2K with multiple tasks per node and only one core per task, the initialization of DLA-Future may trigger the following warning:</p> <pre><code>The pika runtime will be started with only one worker thread because the\nprocess mask has restricted the available resources to only one thread. If\nthis is unintentional make sure the process mask contains the resources\nyou need or use --pika:ignore-process-mask to use all resources. Use\n--pika:print-bind to print the thread bindings used by pika.\n</code></pre> <p>This warning is triggered because the runtime used by DLA-Future, pika, should typically be used with more than one thread and indicates a configuration mistake. However, if you are not using DLA-Future, the warning is harmless and can be ignored. The warning cannot be silenced.</p>"},{"location":"software/sciapps/cp2k/#dbcsr-gpu-scaling","title":"DBCSR GPU scaling","text":"<p>On the GH200 architecture, it has been observed that the GPU accelerated version of DBCSR does not perform optimally in some cases. For example, in the <code>QS/H2O-1024</code> benchmark above, CP2K does not scale well beyond 2 nodes.  The CPU implementation of DBCSR does not suffer from this. A workaround was implemented in DBCSR, in order to switch  GPU acceleration on/off with an environment variable:</p> <pre><code>export DBCSR_RUN_ON_GPU=0\n</code></pre> <p>While GPU acceleration is very good on few nodes, the CPU implementation scales better.  Therefore, for CP2K jobs running on many nodes, it is worth investigating the use of the <code>DBCSR_RUN_ON_GPU</code> environment variable.</p> <p>Some niche application cases such as the <code>QS_low_scaling_postHF</code> benchmarks only run efficiently with the CPU version of DBCSR. Generally, if the function <code>dbcsr_multiply_generic</code> takes a significant portion of the timing report (at the end of the CP2K output file), it is worth investigating the effect of the <code>DBCSR_RUN_ON_GPU</code> environment variable.</p>"},{"location":"software/sciapps/cp2k/#cuda-grid-backend-with-high-angular-momenta-basis-sets","title":"CUDA grid backend with high angular momenta basis sets","text":"<p>The CP2K grid CUDA backend is currently buggy on Alps. Using basis sets with high angular momenta (\\(l \\ge 3\\)) result in slow calculations, especially for force calculations with meta-GGA functionals. </p> <p>As a workaround, you can disable CUDA acceleration for the grid backend:</p> <pre><code>&amp;GLOBAL\n    &amp;GRID\n        BACKEND CPU\n    &amp;END GRID\n&amp;END GLOBAL\n</code></pre> Fix available upon request <p>A fix for this issue for the HIP backend is currently being tested by CSCS engineers. If you would like to test it, please contact us and we will be able to provide the source code. The fix will eventually land on the upstream CP2K repository.</p>"},{"location":"software/sciapps/gromacs/","title":"GROMACS","text":""},{"location":"software/sciapps/gromacs/#gromacs","title":"GROMACS","text":"<p>GROMACS is supported software on Alps. See the main applications page for more information.</p> <p>GROMACS (GROningen Machine for Chemical Simulations) is a versatile and widely-used open source package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.</p> <p>It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the non-bonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p> <p>uenvs</p> <p>GROMACS is provided on Alps via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/sciapps/gromacs/#licensing-terms-conditions","title":"Licensing terms &amp; conditions","text":"<p>GROMACS is a joint effort, with contributions from developers around the world: users agree to acknowledge use of GROMACS in any reports or publications of results obtained with the Software (see GROMACS Homepage for details).</p>"},{"location":"software/sciapps/gromacs/#key-features","title":"Key features","text":"<ol> <li> <p>Molecular Dynamics Simulations: GROMACS performs classical MD simulations, which compute the trajectories of atoms based on Newton\u2019s laws of motion. It integrates the equations of motion to simulate the behavior of molecular systems, capturing their dynamic properties and conformational changes.</p> </li> <li> <p>Force Fields: GROMACS supports a wide range of force fields, including CHARMM, AMBER, OPLS-AA, and GROMOS, which describe the potential energy function and force interactions between atoms. These force fields provide accurate descriptions of the molecular interactions, allowing researchers to study various biological processes and molecular systems.</p> </li> <li> <p>Parallelization and Performance: GROMACS is designed for high-performance computing (HPC) and can efficiently utilize parallel architectures, such as multi-core CPUs and GPUs. It employs domain decomposition methods and advanced parallelization techniques to distribute the computational workload across multiple computing resources, enabling fast and efficient simulations.</p> </li> <li> <p>Analysis and Visualization: GROMACS offers a suite of analysis tools to extract and analyze data from MD simulations. It provides functionalities for computing properties such as energy, temperature, pressure, radial distribution functions, and free energy landscapes. GROMACS also supports visualization tools, allowing users to visualize and analyze the trajectories of molecular systems.</p> </li> <li> <p>User-Friendly Interface: GROMACS provides a command-line interface (CLI) and a set of well-documented input and control files, making it accessible to both novice and expert users. It offers flexibility in defining system parameters, simulation conditions, and analysis options through easily modifiable input files.</p> </li> <li> <p>Integration with Other Software: GROMACS can be integrated with other software packages and tools to perform advanced analysis and extend its capabilities. It supports interoperability with visualization tools like VMD and PyMOL, analysis packages like GROMACS Analysis Tools (GROMACS Tools) and MDAnalysis, and scripting languages such as Python, allowing users to leverage a wide range of complementary tools.</p> </li> </ol>"},{"location":"software/sciapps/gromacs/#daint-on-alps-gh200","title":"Daint on Alps (GH200)","text":""},{"location":"software/sciapps/gromacs/#setup","title":"Setup","text":"<p>On Alps, we provide pre-built user environments containing GROMACS alongside all the required dependencies for the GH200 hardware setup. To access the <code>gmx_mpi</code> executable, we do the following:</p> <pre><code>uenv image find                               # list available images\n\nuenv image pull gromacs/VERSION:TAG.          # copy version:tag from the list above\nuenv start gromacs/VERSION:TAG --view=gromacs # load the gromacs view\n\ngmx_mpi --version                             # check GROMACS version\n</code></pre> <p>The images also provide two alternative views, namely <code>plumed</code> and <code>develop</code>.</p> <pre><code>$ uenv status\n/user-environment:gromacs-gh200\n  GPU-optimised GROMACS with and without PLUMED, and the toolchain to build your own GROMACS.\n  modules: no modules available\n  views:\n    develop\n    gromacs\n    plumed\n</code></pre> <p>The <code>develop</code> view has all the required dependencies or GROMACS without the program itself. This is meant for those users who want to use a customized variant of GROMACS for their simulation which they build from source. This view makes it convenient for users as it provides the required compilers (GCC) along with the dependencies such as CMake, CUDA, hwloc, Cray MPICH, among many others which their GROMACS can use during build and installation. Users must enable this view each time they want to use their custom GROMACS installation.</p> <p>The <code>plumed</code> view contains GROMACS patched with PLUMED. The version of GROMACS in this view may be different from the one in the <code>gromacs</code> view due to the compatibility requirements of PLUMED. CSCS will periodically update these user environment images to feature newer versions as they are made available.</p> <p>The <code>gromacs</code> view contains GROMACS 2024.1 that has been configured and tested for the highest performance on the Grace-Hopper nodes.</p> <p>Use <code>exit</code> to leave the user environment and return to the original shell.</p>"},{"location":"software/sciapps/gromacs/#how-to-run","title":"How to run","text":"<p>To start a job, 2 bash scripts are required: a standard Slurm submission script, and a wrapper to start the CUDA MPS daemon (in order to have multiple MPI ranks per GPU).</p> <p>The wrapper script above needs to be made executable with <code>chmod +x mps-wrapper.sh</code>.</p> <p>The Slurm submission script can be adapted from the template below to use the application and the <code>mps-wrapper.sh</code> in conjunction.</p> launch.sbatch<pre><code>#!/bin/bash\n\n#SBATCH --job-name=\"JOB NAME\"\n#SBATCH --nodes=1             # number of GH200 nodes with each node having 4 CPU+GPU\n#SBATCH --ntasks-per-node=8   # 8 MPI ranks per node\n#SBATCH --cpus-per-task 32    # 32 OMP threads per MPI rank\n#SBATCH --account=ACCOUNT\n#SBATCH --hint=nomultithread  \n#SBATCH --uenv=&lt;GROMACS_UENV&gt;\n#SBATCH --view=gromacs\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport FI_CXI_RX_MATCH_MODE=software\n\nexport GMX_GPU_DD_COMMS=true\nexport GMX_GPU_PME_PP_COMMS=true\nexport GMX_FORCE_UPDATE_DEFAULT_GPU=true\nexport GMX_ENABLE_DIRECT_GPU_COMM=1\nexport GMX_FORCE_GPU_AWARE_MPI=1\n\nsrun ./mps-wrapper.sh gmx_mpi mdrun -s input.tpr -ntomp 32 -bonded gpu -nb gpu -pme gpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0123 -npme 1 -nsteps 10000 -update gpu\n</code></pre> <p>This can be run using <code>sbatch launch.sbatch</code> on the login node with the user environment loaded.</p> <p>This submission script is only representative. Users must run their input files with a range of parameters to find an optimal set for the production runs. Some hints for this exploration below:</p> <p>Configuration hints</p> <ul> <li>Each Grace CPU has 72 cores, but a small number of them are used for the underlying processes such as runtime daemons. So all 72 cores are not available for compute. To be safe, do not exceed more than 64 OpenMP threads on a single CPU even if it leads to a handful of cores idling.</li> <li>Each node has 4 Grace CPUs and 4 Hopper GPUs. When running 8 MPI ranks (meaning two per CPU), keep in mind to not ask for more than 32 OpenMP threads per rank. That way no more than 64 threads will be running on a single CPU.</li> <li>Try running both 64 OMP threads x 1 MPI rank and 32 OMP threads x 2 MPI ranks configurations for the test problems and pick the one giving better performance. While using multiple GPUs, the latter can be faster by 5-10%.</li> <li><code>-update gpu</code> may not be possible for problems that require constraints on all atoms. In such cases, the update (integration) step will be performed on the CPU. This can lead to performance loss of at least 10% on a single GPU. Due to the overheads of additional data transfers on each step, this will also lead to lower scaling performance on multiple GPUs.</li> <li>When running on a single GPU, one can either configure the simulation with 1-2 MPI ranks with <code>-gpu_id</code> as <code>0</code>, or try running the simulation with a small number of parameters and let GROMACS run with defaults/inferred parameters with a command like the following in the Slurm script: <code>srun ./mps-wrapper.sh -- gmx_mpi mdrun -s input.tpr -ntomp 64</code></li> <li>Given the compute throughput of each Grace-Hopper module (single CPU+GPU), for smaller-sized problems, it is possible that a single-GPU run is the fastest. This may happen when the overheads of domain decomposition, communication and orchestration exceed the benefits of parallelism across multiple GPUs. In our test cases, a single Grace-Hopper module (1 CPU+GPU) has consistently shown a 6-8x performance speedup over a single node on Piz Daint (Intel Xeon Broadwell + P100).</li> <li>Try runs with and without specifying the GPU IDs explicitly with <code>-gpu_id 0123</code>. For the multi-node case, removing it might yield the best performance.</li> </ul>"},{"location":"software/sciapps/gromacs/#scaling","title":"Scaling","text":"<p>Benchmarking is done with large MD simulations of systems of 1.4 million and 3 million atoms, in order to fully saturate the GPUs, from the HECBioSim Benchmark Suite.</p> <p>In addition, the STMV (~1 million atom) benchmark that NVIDIA publishes on its website was also tested for comparison. </p> <p>The STMV test case is a fairly large problem size, with constraints operating only on a smaller set of atoms (h-bonds) which allows the update step to also take place on GPUs. This makes the simulation almost fully GPU resident with the key performance intensive bits namely the long-range forces (PME), short-range non-bonded forces (NB) and bonded forces all running on the GPU. On a single node, this leads to the following scaling on GROMACS 2024.1.</p>"},{"location":"software/sciapps/gromacs/#stmv-multiple-ranks-single-node-up-to-4-gpus","title":"STMV - Multiple ranks - Single node up to 4 GPUs","text":"#GPUs ns/day Speedup 1 42.855 1x 2 61.583 1.44x 4 115.316 2.69x 8 138.896 3.24x <p>The other benchmark cases from HECBioSim simulates a pair of proteins (hEGFR Dimers/Tetramers of 1IVO and 1NQL) with a large lipid membrane. This also involves a fairly large number of charged ions which increases the proportion of PME in the total compute workload. For these simulations, constraints are applicable on all atoms, which effectively prevents the update from happening in the GPU, thus negatively impacting scaling due large host-to-device data transfers and key computations happening on the CPU. These show the following scaling characteristics on GROMACS 2024.1:</p>"},{"location":"software/sciapps/gromacs/#14m-atom-system-multiple-ranks-single-node","title":"1.4m Atom System - Multiple ranks - Single node","text":"<p>Total number of atoms = 1,403,182</p> <p>Protein atoms = 43,498  Lipid atoms = 235,304  Water atoms = 1,123,392  Ions = 986</p> #GPUs ns/day Speedup 1 31.243 1x 4 55.936 1.79x"},{"location":"software/sciapps/gromacs/#3m-atom-system-single-node-multiple-ranks","title":"3m Atom System - Single node - Multiple ranks","text":"<p>Total number of atoms = 2,997,924</p> <p>Protein atoms = 86,996  Lipid atoms = 867,784  Water atoms = 2,041,230  Ions = 1,914</p> #GPUs ns/day Speedup 1 14.355 1x 4 30.289 2.11x <p>Known Performance/Scaling Issues</p> <ul> <li>The currently provided build of GROMACS allows only one MPI rank to be dedicated for PME with <code>-nmpe 1</code>. This becomes a serious performance limitation for larger systems where the non-PME ranks finish their work before the PME rank leading to unwanted load imbalances across ranks. This limitation is targeted to be fixed in the subsequent releases of our builds of user environments.</li> <li>The above problem is especially critical for large problem sizes (1+ million atom systems) but is far less apparent in small and medium sized runs.</li> <li>If the problem allows the integration step to take place on the GPU with <code>-update gpu</code>, that can lead to significant performance and scaling gains as it allows an even greater part of the computations to take place on the GPU.</li> <li>A single node of the GH200 cluster offers 4x CPU+GPU. For problems that can benefit from scaling beyond a single node, use the flag <code>export FI_CXI_RX_MATCH_MODE=software</code> in the SBATCH script. The best use of resources in terms of node-hours might be achieved on a single node for most simulations.</li> </ul>"},{"location":"software/sciapps/gromacs/#building-gromacs-from-source","title":"Building GROMACS from Source","text":"<p>The GROMACS uenv provides all the dependencies required to build GROMACS from source, with several optional features enabled. This approach is particularly useful for deploying customized variants of GROMACS. You can follow these steps to build GROMACS from source:</p> <pre><code>uenv start --view=develop &lt;GROMACS_UENV&gt; # (1)!\n\ncd &lt;PATH_TO_GROMACS_SOURCE&gt; # (2)!\n\nmkdir build &amp;&amp; cd build\ncmake \\\n    -DCMAKE_C_COMPILER=gcc \\\n    -DCMAKE_CXX_COMPILER=g++ \\\n    -DGMX_MPI=on \\\n    -DGMX_GPU=CUDA \\\n    -GMX_CUDA_TARGET_SM=\"90\" \\ # for the Hopper GPUs\n    -DGMX_SIMD=ARM_NEON_ASIMD \\ # for the Grace CPUs\n    -DGMX_DOUBLE=off \\ # turn on double precision only if useful\n    -DCMAKE_INSTALL_PREFIX=/custom/gromacs/install/path\n    ..\n\nmake\nmake check\nmake install\nsource /custom/gromacs/install/path/bin/GMXRC\n</code></pre> <ol> <li> <p>Start the GROMACS uenv and load the <code>develop</code> view (which provides all the necessary dependencies)</p> </li> <li> <p>Go to the GROMACS source directory</p> </li> </ol> <p>See GROMACS Installation Guide for more details, especially on special configurations.</p>"},{"location":"software/sciapps/gromacs/#further-documentation","title":"Further documentation","text":"<ul> <li>GROMACS Homepage</li> <li>GROMACS Manual</li> </ul>"},{"location":"software/sciapps/lammps/","title":"LAMMPS","text":""},{"location":"software/sciapps/lammps/#lammps","title":"LAMMPS","text":"<p>LAMMPS is supported software on Alps. See the main applications page for more information.</p> <p>LAMMPS is a classical molecular dynamics code that models an ensemble of particles in a liquid, solid, or gaseous state. It can model atomic, polymeric, biological, metallic, granular, and coarse-grained systems using a variety of force fields and boundary conditions.  The current version of LAMMPS is written in C++.</p> <p>uenvs</p> <p>LAMMPS is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p> Licensing terms and conditions <p>LAMMPS is a freely-available open-source code, distributed under the terms of the GNU Public License.</p>"},{"location":"software/sciapps/lammps/#running-lammps","title":"Running LAMMPS","text":""},{"location":"software/sciapps/lammps/#loading-lammps-interactively","title":"Loading LAMMPS Interactively","text":"<p>On Alps, LAMMPS is precompiled and available in a uenv.  LAMMPS has been built with the Kokkos and GPU packages separately.</p> <p>To find which LAMMPS uenv is provided, you can use the following command:</p> <pre><code>uenv image find lammps\n</code></pre> <p>which will list several available LAMMPS uenv images. We recommend that you regularly check for the latest version. Please see the documentation here for further details: https://eth-cscs.github.io/cscs-docs/software/uenv/#finding-uenv.</p> <p>To obtain this image, please run:</p> <pre><code>uenv image pull lammps/2024:v2\n</code></pre> <p>To start the uenv for this specific version of LAMMPS, you can use:</p> <pre><code>uenv start --view kokkos lammps/2024:v2\n</code></pre> <p>You can load the <code>kokkos</code> or <code>gpu</code> view from the uenv to make the <code>lmp</code> executable available. The executable in both these views support GPUs:</p> KokkosGPU <pre><code>#lammps +kokkos package\nuenv start --view kokkos lammps/2024:v2\n</code></pre> <pre><code>#lammps +gpu package\nuenv start --view gpu lammps/2024:v2\n</code></pre> <p>A development view is also provided, which contains all libraries and command-line tools necessary to build LAMMPS from source, without including the LAMMPS executable:</p> KokkosGPU <pre><code># build environment for lammps +kokkos package, without providing lmp executable\nuenv start --view develop-kokkos lammps/2024:v2\n</code></pre> <pre><code># build environment for lammps +gpu package, without providing lmp executable\nuenv start --view develop-gpu lammps/2024:v2\n</code></pre>"},{"location":"software/sciapps/lammps/#running-lammps-with-kokkos-on-daint","title":"Running LAMMPS with Kokkos on Daint","text":"<p>Daint nodes have four GH200 GPUs that have to be configured properly for best performance. To start a job, the following bash Slurm submission script is required:</p> run_lammps_kokkos.sh<pre><code>#!/bin/bash -l\n#SBATCH --job-name=&lt;JOB_NAME&gt;\n#SBATCH --time=01:00:00 (1)\n#SBATCH --nodes=2                                                                        \n#SBATCH --ntasks-per-node=4  (2)\n#SBATCH --gpus-per-task=1\n#SBATCH --account=&lt;ACCOUNT&gt; (3)\n#SBATCH --uenv=&lt;LAMMPS_UENV&gt;:/user-environment (4)\n#SBATCH --view=kokkos (5)\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nulimit -s unlimited\n\nsrun lmp -in lj_kokkos.in -k on g 1 -sf kk -pk kokkos gpu/aware on\n</code></pre> <ol> <li>Time format: <code>HH:MM:SS</code>.</li> <li>For LAMMPS + Kokkos its typical to only use 1 MPI-rank per GPU.</li> <li>Change <code>&lt;ACCOUNT&gt;</code> to your project account name.</li> <li>Change <code>&lt;LAMMPS_UENV&gt;</code> to the name (or path) of the LAMMPS uenv you want to use.</li> <li>Load the <code>kokkos</code> uenv view.</li> </ol> <p>Note</p> <p>Using <code>-k on g 1</code> specifies that we want 1 GPU per MPI-rank.  This is contrary to what is mentioned in the official LAMMPS documentation, however this is required to achieve the propper configuration on Alps.</p> <p>With the above script, you can launch a LAMMPS + Kokkos calculation on 2 nodes, using 4 MPI ranks and 1 GPU per MPI rank with:</p> <pre><code>sbatch run_lammps_kokkos.sh\n</code></pre> LAMMPS + Kokkos input file, defining a 3d Lennard-Jones melt. <p>The following input file for LAMMPS + Kokkos defines a 3D Lennard-Jones system melt.</p> <pre><code>variable        x index 200\nvariable        y index 200\nvariable        z index 200\nvariable        t index 1000\n\nvariable        xx equal 1*$x\nvariable        yy equal 1*$y\nvariable        zz equal 1*$z\n\nvariable        interval equal $t/2\n\nunits           lj\natom_style      atomic/kk\n\nlattice         fcc 0.8442\nregion          box block 0 ${xx} 0 ${yy} 0 ${zz}\ncreate_box      1 box\ncreate_atoms    1 box\nmass            1 1.0\n\nvelocity        all create 1.44 87287 loop geom\n\npair_style      lj/cut/kk 2.5\npair_coeff      1 1 1.0 1.0 2.5\n\nneighbor        0.3 bin\nneigh_modify    delay 0 every 20 check no\n\nfix             1 all nve\n\nthermo          ${interval}\nthermo_style custom step time  temp press pe ke etotal density\nrun_style       verlet/kk\nrun             $t\n</code></pre>"},{"location":"software/sciapps/lammps/#running-lammps-gpu-on-daint","title":"Running LAMMPS + GPU on Daint","text":"<p>To start a job, two bash scripts are required: a Slurm submission script, and a wrapper for CUDA MPS.</p> run_lammps_gpu.sh<pre><code>#!/bin/bash -l\n#SBATCH --job-name=&lt;JOB_NAME&gt;\n#SBATCH --time=01:00:00 (1)\n#SBATCH --nodes=2 (2)                                                                        \n#SBATCH --ntasks-per-node=32\n#SBATCH --gpus-per-node=4\n#SBATCH --account=&lt;ACCOUNT&gt; (3)                                                       \n#SBATCH --uenv=&lt;LAMMPS_UENV&gt;:/user-environment (4)\n#SBATCH --view=gpu (5)\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nulimit -s unlimited\n\nsrun ./mps-wrapper.sh lmp -sf gpu -pk gpu 4 -in lj.in\n</code></pre> <ol> <li>Time format: <code>HH:MM:SS</code>.</li> <li>For LAMMPS + GPU it is often beneficial to use more than 1 MPI rank per GPU. To enable oversubscription of MPI ranks per GPU, you\u2019ll need to use the <code>mps-wrapper.sh</code> script provided in the following section: multiple ranks per GPU.</li> <li>Change <code>&lt;ACCOUNT&gt;</code> to your project account name.</li> <li>Change <code>&lt;LAMMPS_UENV&gt;</code> to the name (or path) of the LAMMPS uenv you want to use.</li> <li>Enable the <code>gpu</code> uenv view.</li> </ol> <p>To enable oversubscription of MPI ranks per GPU, you\u2019ll need to use the <code>mps-wrapper.sh</code> script provided at the following page: NVIDIA GH200 GPU nodes: multiple ranks per GPU.</p> LAMMPS+GPU input file <p>The following input file for LAMMPS + GPU defines a 3D Lennard-Jones system melt.</p> <pre><code># 3d Lennard-Jones melt\nvariable        x index 200\nvariable        y index 200\nvariable        z index 200\nvariable        t index 1000\n\nvariable        xx equal 1*$x\nvariable        yy equal 1*$y\nvariable        zz equal 1*$z\n\nvariable        interval equal $t/2\n\nunits           lj\natom_style      atomic\n\nlattice         fcc 0.8442\nregion          box block 0 ${xx} 0 ${yy} 0 ${zz}\ncreate_box      1 box\ncreate_atoms    1 box\nmass            1 1.0\n\nvelocity        all create 1.44 87287 loop geom\n\npair_style      lj/cut 2.5\npair_coeff      1 1 1.0 1.0 2.5\n\nneighbor        0.3 bin\nneigh_modify    delay 0 every 20 check no\n\nfix             1 all nve\n\nthermo          ${interval}\nthermo_style custom step time  temp press pe ke etotal density\nrun_style       verlet\nrun             $t\n</code></pre>"},{"location":"software/sciapps/lammps/#running-on-eiger","title":"Running on Eiger","text":"<p>On Eiger, the following sbatch script can be used:</p> run_lammps_eiger.sh<pre><code>#!/bin/bash -l\n#SBATCH --job-name=&lt;JOB_NAME&gt;\n#SBATCH --time=01:00:00 (1)\n#SBATCH --nodes=2                   \n#SBATCH --ntasks-per-core=1                                                    \n#SBATCH --ntasks-per-node=32 (2)\n#SBATCH --cpus-per-task=4 (3) \n#SBATCH --account=&lt;ACCOUNT&gt; (4)\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --constraint=mc                                                  \n#SBATCH --uenv=&lt;LAMMPS_UENV&gt;:/user-environment (5)\n#SBATCH --view=kokkos (6)\n\nulimit -s unlimited\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PROC_BIND=spread\nexport OMP_PLACES=threads\n\nsrun --cpu-bind=cores lmp -k on t $OMP_NUM_THREADS -sf kk -in lj_kokkos.in\n</code></pre> <ol> <li>Time format: <code>HH:MM:SS</code>.</li> <li>Number of MPI ranks per node.</li> <li>Number of threads per MPI rank.</li> <li>Change <code>&lt;ACCOUNT&gt;</code> to your project account name.</li> <li>Change <code>&lt;LAMMPS_UENV&gt;</code> to the name (or path) of the LAMMPS uenv you want to use.</li> <li>Enable the <code>kokkos</code> uenv view.</li> </ol> <p>Note that the same input file <code>lj_kokkos.in</code> can be used as with running LAMMPS with Kokkos on Daint.</p>"},{"location":"software/sciapps/lammps/#building-lammps-from-source","title":"Building LAMMPS from source","text":""},{"location":"software/sciapps/lammps/#using-cmake","title":"Using CMake","text":"<p>If you\u2019d like to rebuild LAMMPS from source to add additional packages or to use your own customized code, you can use the develop views contained within the uenv image to provide you with all the necessary libraries and command-line tools you\u2019ll need. For the following, we\u2019d recommend obtaining an interactive node and building inside the tmpfs directory.</p> <pre><code>salloc -N1 -t 60 -A &lt;account&gt;\nsrun --pty bash\nmkdir /dev/shm/lammps_build; cd /dev/shm/lammps_build\n</code></pre> <p>After you\u2019ve obtained a version of LAMMPS you\u2019d like to build, extract it in the above temporary folder and create a build directory.  Load one of the two following views:</p> KokkosGPU <pre><code>#build environment for lammps +kokkos package, without providing lmp executable\nuenv start --view develop-kokkos lammps/2024:v2\n</code></pre> <pre><code>#build environment for lammps +gpu package, without providing lmp executable\nuenv start --view develop-gpu lammps/2024:v2\n</code></pre> <p>and now you can build your local copy of LAMMPS.  For example to build with Kokkos and the <code>MOLECULE</code> package enabled:</p> <pre><code>CC=mpicc CXX=mpic++ cmake \\\n-DCMAKE_CXX_FLAGS=-DCUDA_PROXY \\\n-DBUILD_MPI=yes\\\n-DBUILD_OMP=no \\\n-DPKG_MOLECULE=yes \\\n-DPKG_KOKKOS=yes \\\n-DEXTERNAL_KOKKOS=yes \\\n-DKokkos_ARCH_NATIVE=yes \\\n-DKokkos_ARCH_HOPPER90=yes \\\n-DKokkos_ARCH_PASCAL60=no \\\n-DKokkos_ENABLE_CUDA=yes \\\n-DKokkos_ENABLE_OPENMP=yes \\\n-DCUDPP_OPT=no \\\n-DCUDA_MPS_SUPPORT=yes \\\n-DCUDA_ENABLE_MULTIARCH=no \\\n../cmake  \n</code></pre> <p>Warning</p> <p>If you are downloading LAMMPS from GitHub or their website and intend to use Kokkos for acceleration, there is an issue with Cray MPICH and <code>Kokkos &lt;= 4.3</code>.  For LAMMPS to work correctly on our system, you need a LAMMPS version which provides <code>Kokkos &gt;= 4.4</code>.  Alternatively, the CMake variable <code>-DEXTERNAL_KOKKOS=yes</code> should force CMake to use the Kokkos version provided by the uenv, rather than the one contained within the LAMMPS distribution.</p>"},{"location":"software/sciapps/lammps/#using-lammps-uenv-as-an-upstream-spack-instance","title":"Using LAMMPS uenv as an upstream Spack Instance","text":"<p>If you\u2019d like to extend the existing uenv with additional packages (or your own), you can use the LAMMPS uenv to provide all dependencies needed to build your customization. See here for more information.</p>"},{"location":"software/sciapps/namd/","title":"NAMD","text":""},{"location":"software/sciapps/namd/#namd","title":"NAMD","text":"<p>NAMD is supported software on Alps. See the main applications page for more information.</p> <p>NAMD is a parallel molecular dynamics code based on Charm++, designed for high-performance simulations of large biomolecular systems.</p> <p>Licensing Terms and Conditions</p> <p>NAMD is distributed free of charge for research purposes only and not for commercial use: users must agree to the NAMD license in order to use it at CSCS. Users agree to acknowledge use of NAMD in any reports or publications of results obtained with the Software (see NAMD Homepage for details).</p> <p>User Environments</p> <p>NAMD is provided on ALPS as a uenv. Please have a look at the uenv documentation for more information about UENVs and how to use them.</p> <p>NAMD is provided in two flavours on CSCS systems:</p> <ul> <li>Single-node build</li> <li>Multi-node build</li> </ul> <p>The single-node build works on a single node and benefits from the new GPU-resident mode (see NAMD 3.0b6 GPU-Resident benchmarking results for more details). The multi-node build works on multiple nodes and is based on Charm++\u2019s MPI backend.</p> <p>Prefer the single-node build and exploit GPU-resident mode</p> <p>Unless you have good reasons to use the multi-node build, we recommend using the single-node build with the GPU-resident mode.</p> <p>Eiger</p> <p>The multi-node version is the only version of NAMD available on Eiger - single-node is not provided.</p>"},{"location":"software/sciapps/namd/#single-node-build","title":"Single-node build","text":"<p>The single-node build provides the following views:</p> <ul> <li><code>namd-single-node</code> (standard view, with NAMD)</li> <li><code>develop-single-node</code> (development view, without NAMD)</li> </ul>"},{"location":"software/sciapps/namd/#running-namd-on-a-single-node","title":"Running NAMD on a single node","text":"<p>The following sbatch script shows how to run NAMD on a single node with 4 GPUs:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"namd-example\"\n#SBATCH --time=00:10:00\n#SBATCH --account=&lt;ACCOUNT&gt;          (6)\n#SBATCH --nodes=1                    (1)\n#SBATCH --ntasks-per-node=1          (2)\n#SBATCH --cpus-per-task=288\n#SBATCH --gres=gpu:4                 (3)\n#SBATCH --uenv=&lt;NAMD_UENV&gt;           (4)\n#SBATCH --view=namd-single-node      (5)\n\n\nsrun namd3 +p 29 +pmeps 5 +setcpuaffinity +devices 0,1,2,3 &lt;NAMD_CONFIG_FILE&gt; # (7)! \n</code></pre> <ol> <li>You can only use one node with the <code>single-node</code> build</li> <li>You can only use one task per node with the <code>single-node</code> build</li> <li>Make all GPUs visible to NAMD (by automatically setting <code>CUDA_VISIBLE_DEVICES=0,1,2,3</code>)</li> <li>Load the NAMD UENV (UENV name or path to the UENV). Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> <li>Load the <code>namd-single-node</code> view</li> <li>Change <code>&lt;ACCOUNT&gt;</code> to your project account</li> <li>Make sure you set <code>+p</code>, <code>+pmeps</code>, and other NAMD options optimally for your calculation.    Change <code>&lt;NAMD_CONFIG_FILE&gt;</code> to the name (or path) of the NAMD configuration file for your simulation </li> </ol> Scaling of STMV benchmark with GPU-resident mode from 1 to 4 GPUs <p>Scaling of the tobacco mosaic virus (STMV) benchmark with GPU-resident mode on our system is the following:</p> GPUs ns/day Speedup Parallel efficiency 1 31.1 - - 2 53.7 1.9 86% 4 92.7 3.5 74% 1 GPU2 GPUs4 GPUs <pre><code>srun namd3 +p 8 +setcpuaffinity +devices 0 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre> <pre><code>srun namd3 +p 15 +pmeps 7 +setcpuaffinity +devices 0,1 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre> <pre><code>srun namd3 +p 29 +pmeps 5 +setcpuaffinity +devices 0,1,2,3 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre>"},{"location":"software/sciapps/namd/#building-namd-from-source-with-charms-multicore-backend","title":"Building NAMD from source with Charm++\u2019s multicore backend","text":"<p>Action required</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for the <code>3.0</code> release still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD 3.0\u2019s <code>arch/Linux-ARM64.tcl</code> file as follows: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source.</p> GPU BuildCPU Build <p>Build NAMD:</p> <pre><code>export DEV_VIEW_NAME=\"develop-single-node\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt;\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt;\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <p>Action required</p> <p>Modify the <code>&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <pre><code># Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\n./build charm++ multicore-linux-arm8 gcc --with-production --enable-tracing -j 32\n\n# Configure NAMD build for GPU\ncd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n\n# The namd3 executable (GPU-accelerated) will be built in the Linux-ARM64-g++.cuda directory\n</code></pre> <ul> <li>Change <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> to the path where you have the NAMD source code</li> <li>Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> </ul> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre> <p>Some workflows, such as constant pH MD simulations, might require a CPU-only NAMD build which is used to drive the simulation.</p> <p>Use the CPU-only build only if needed</p> <p>The CPU-only build is optional and should be used only if needed. You should use it in conjunction with the GPU build to drive the simulation. Do not use the CPU-only build for actual simulations as it will be slower than the GPU build.</p> <p>You can build a CPU-only version of NAMD as follows:</p> <pre><code>export DEV_VIEW_NAME=\"develop-single-node\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt;\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt;\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <p>Action required</p> <p>Modify the <code>&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <pre><code># Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\n./build charm++ multicore-linux-arm8 gcc --with-production --enable-tracing -j 32\n\n# Configure NAMD build for GPU\ncd ..\n./config Linux-ARM64-g++ \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++ &amp;&amp; make -j 32\n\n# The namd3 executable (CPU-only) will be built in the Linux-ARM64-g++ directory\n</code></pre> <ul> <li>Change <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> to the path where you have the NAMD source code</li> </ul> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre>"},{"location":"software/sciapps/namd/#multi-node-build","title":"Multi-node build","text":"<p>The multi-node build provides the following views:</p> <ul> <li><code>namd</code> (standard view, with NAMD)</li> <li><code>develop</code> (development view, without NAMD)</li> </ul> <p>GPU-resident mode</p> <p>The multi-node build based on Charm++\u2019s MPI backend can\u2019t take advantage of the new GPU-resident mode. Unless you require the multi-node build or you can prove it is faster for your use case, we recommend using the single-node build with the GPU-resident mode.</p>"},{"location":"software/sciapps/namd/#running-namd-on-eiger","title":"Running NAMD on Eiger","text":"<p>The following sbatch script shows how to run NAMD on Eiger:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=namd-test\n#SBATCH --time=00:30:00\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=128\n#SBATCH --account=&lt;ACCOUNT&gt; (1)\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --constraint=mc\n#SBATCH --uenv=namd/3.0:v1 (2)\n#SBATCH --view=namd (3)\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PROC_BIND=spread\nexport OMP_PLACES=threads\n\nsrun --cpu-bind=cores namd3 +setcpuaffinity ++ppn 4 &lt;NAMD_CONFIG_FILE&gt; # (4)!\n</code></pre> <ol> <li>Change <code>&lt;ACCOUNT&gt;</code> to your project account</li> <li>Load the NAMD UENV (UENV name or path to the UENV). Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> <li>Load the <code>namd</code> view</li> <li>Make sure you set <code>++ppn</code>, and other NAMD options optimally for your calculation.    Change <code>&lt;NAMD_CONFIG_FILE&gt;</code> to the name (or path) of the NAMD configuration file for your simulation </li> </ol>"},{"location":"software/sciapps/namd/#building-namd-from-source-with-charms-mpi-backend","title":"Building NAMD from source with Charm++\u2019s MPI backend","text":"<p>TCL Version</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for some (beta) releases still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD\u2019s <code>arch/Linux-&lt;ARCH&gt;.tcl</code> file: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable, if needed.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source. You can follow these steps to build NAMD from source:</p> gh200 buildzen2 build <pre><code>export DEV_VIEW_NAME=\"develop\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt; # (1)!\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt; # (2)!\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <ol> <li>Substitute <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> with the actual path to the NAMD source code</li> <li>Substitute <code>&lt;NAMD_UENV&gt;</code> with the actual name (or path) of the NAMD UENV you want to use.</li> </ol> <p>Action required</p> <p>Modify the <code>${PATH_TO_NAMD_SOURCE}/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable, if needed.</p> <p>Build Charm++ bundled with NAMD:</p> <pre><code>tar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\nenv MPICXX=mpicxx ./build charm++ mpi-linux-arm8 smp --with-production -j 32\n</code></pre> <p>Finally, you can configure and build NAMD (with GPU acceleration):</p> <pre><code>cd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch mpi-linux-arm8-smp --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n</code></pre> <p>The <code>namd3</code> executable (GPU-accelerated) will be built in the <code>Linux-ARM64-g++.cuda</code> directory.</p> <pre><code>export DEV_VIEW_NAME=\"develop\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt; # (1)!\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt; # (2)!\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <ol> <li>Substitute <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> with the actual path to the NAMD source code</li> <li>Substitute <code>&lt;NAMD_UENV&gt;</code> with the actual name (or path) of the NAMD UENV you want to use.</li> </ol> <p>Action required</p> <p>Modify the <code>${PATH_TO_NAMD_SOURCE}/arch/Linux-x86_64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable, if needed.</p> <p>Build Charm++ bundled with NAMD:</p> <pre><code>tar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\nenv MPICXX=mpicxx ./build charm++ mpi-linux-x86_64 smp --with-production -j 32\n</code></pre> <p>Finally, you can configure and build NAMD:</p> <pre><code>cd .. \n./config Linux-x86_64-g++ \\\n    --charm-arch mpi-linux-x86_64-smp --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH}\ncd Linux-x86_64-g++ &amp;&amp; make -j 32\n</code></pre> <p>The <code>namd3</code> executable will be built in the <code>Linux-x86_64-g++</code> directory.</p> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre>"},{"location":"software/sciapps/namd/#useful-links","title":"Useful Links","text":"<ul> <li>NAMD Homepage</li> <li>NAMD Tutorials</li> <li>Running Charm++ Programs</li> <li>What you should know about NAMD and Charm++ but were hoping to ignore by J. C. Phillips</li> <li>NAMD Spack package</li> <li>Charm++ Spack package</li> </ul>"},{"location":"software/sciapps/quantumespresso/","title":"Quantum ESPRESSO","text":""},{"location":"software/sciapps/quantumespresso/#quantum-espresso","title":"Quantum ESPRESSO","text":"<p>Quantum ESPRESSO is supported software on Alps. See the main applications page for more information.</p> <p>Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials:</p> <ul> <li><code>pw.x</code>: Plane-Wave Self-Consistent Field (PWscf)</li> <li><code>pw.x</code> First Principles Molecular Dynamics (FPMD)</li> <li><code>cp.x</code> Car-Parrinello (CP)</li> </ul> <p>uenvs</p> <p>Quantum ESPRESSO is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/sciapps/quantumespresso/#how-to-run","title":"How to run","text":"<p>The following sbatch script can be used as a template.</p> GH200Eiger <p><pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=71\n#SBATCH --gpus-per-task=1\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=quantumespresso/v7.4:v2\n#SBATCH --view=default\n\nexport OMP_NUM_THREADS=20\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport OMP_PLACES=cores\n\nsrun -u --cpu-bind=socket /user-environment/env/default/bin/pw.x &lt; pw.in\n</code></pre> Current observation is that best performance is achieved using one MPI rank per GPU. How to run multiple ranks per GPU is described here.</p> <pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=128\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=quantumespresso/v7.3.1\n#SBATCH --view=default\n#SBATCH --hint=nomultithread\n\nexport OMP_NUM_THREADS=1\n\nsrun -u /user-environment/env/default/bin/pw.x &lt; pw.in\n</code></pre>"},{"location":"software/sciapps/quantumespresso/#building-qe-from-source","title":"Building QE from Source","text":""},{"location":"software/sciapps/quantumespresso/#using-modules","title":"Using modules","text":"GH200A100 <pre><code>uenv start --view=modules quantumespresso/v7.4:v2\nmodule load cmake \\\n    fftw \\\n    nvhpc \\\n    nvpl-lapack \\\n    nvpl-blas \\\n    cray-mpich \\\n    netlib-scalapack \\\n    libxc\n\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_ENABLE_PROFILE_NVTX=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre> <pre><code>uenv start --view=modules quantumespresso/v7.3.1:v2\nmodule load cmake \\\n    cray-mpich\n    cuda \\\n    fftw \\\n    gcc \\\n    libxc \\\n    nvhpc \\\n    openblas\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre>"},{"location":"software/sciapps/quantumespresso/#using-spack","title":"Using spack","text":"<ol> <li> <p>Clone spack using the same version that has been used to build the uenv. <pre><code>uenv start quantumespresso/v7.3.1\n# clone the same spack version as has been used to build the uenv\ngit clone -b $(jq -r .spack.commit /user-environment/meta/configure.json) $(jq -r .spack.repo /user-environment/meta/configure.json) $SCRATCH/spack\n</code></pre></p> </li> <li> <p>Activate spack with the uenv configured as upstream <pre><code># ensure spack is using the uenv as upstream repository (always required)\nexport SPACK_SYSTEM_CONFIG_PATH=/user-environment/config\n# active spack (always required)\n. $SCRATCH/spack/share/spack/setup-env.sh\n</code></pre></p> </li> <li> <p>Create an anonymous environment for QE <pre><code>spack env create -d $SCRATCH/qe-env\nspack -e $SCRATCH/qe-env add quantum-espresso%nvhpc +cuda\nspack -e $SCRATCH/qe-env config add packages:all:prefer:cuda_arch=90\nspack -e $SCRATCH/qe-env develop -p /path/to/your/QE-src quantum-espresso@=develop\nspack -e $SCRATCH/qe-env concretize -f\n</code></pre> Check the output of <code>spack concretize -f</code>. All dependencies should have been picked up from spack upstream, marked either by a green <code>[^]</code> or <code>[e]</code>. Next we create a local filesystem view, this instructs spack to create symlinks for binaries and libraries in a local directory <code>view</code>. <pre><code>spack -e $SCRATCH/qe-env env view enable view\nspack -e $SCRATCH/qe-env install\n</code></pre> To recompile QE after editing the source code re-run <code>spack -e $SCRATCH/qe-env install</code>.</p> </li> <li> <p>Run <code>pw.x</code> using the filesystem view generated in 3. <pre><code>uenv start quantumespresso/v7.3.1\nMPICH_GPU_SUPPORT_ENABLED=1 srun [...] $SCRATCH/qe-env/view/bin/pw.x &lt; pw.in\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>pw.x</code> is linked to the uenv, it won\u2019t work without activating the uenv, also it will only work with the exact same version of the uenv. </p> <p>Warning</p> <p>The physical installation path is in <code>$SCRATCH/spack</code>, deleting this directory will leave the anonymous spack environment created in 3. with dangling symlinks.</p>"},{"location":"software/sciapps/vasp/","title":"VASP","text":""},{"location":"software/sciapps/vasp/#vasp","title":"VASP","text":"<p>VASP is supported software on Alps. See the main applications page for more information.</p> <p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.</p> <p>VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green\u2019s functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP.</p> <p>In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method. To determine the electronic groundstate, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle.</p> <p>Licensing Terms and Conditions</p> <p>Access to VASP is restricted to users who have purchased a license from VASP Software GmbH. CSCS cannot provide free access to the code and needs to inform VASP Software GmbH with an updated list of users. Once you have a license, submit a request on the CSCS service desk (with a copy of your license) to be added to the <code>vasp6</code> unix group, which will grant access to the <code>vasp</code> uenv. Please refer to the VASP web site for more information about licensing. Therefore, access to precompiled <code>VASP.6</code> executables and library files will be available only to users who have already purchased a <code>VASP.6</code> license and upon request will become members of the CSCS unix group <code>vasp6</code>.</p> <p>To access VASP follow the <code>Accessing Restricted Software</code> guide. Please refer to the VASP web site for more information.</p>"},{"location":"software/sciapps/vasp/#running-vasp","title":"Running VASP","text":""},{"location":"software/sciapps/vasp/#running-on-daint","title":"Running on Daint","text":"<p>A precompiled uenv containing VASP with MPI, OpenMP, OpenACC, HDF5 and Wannier90 support is available. Due to license restrictions, the VASP images are not directly accessible in the same way as other applications.</p> <p>For accessing VASP uenv images, please see the guide to accessing restricted software.</p> <p>To load the VASP uenv: <pre><code>uenv start vasp/v6.5.0:v1 --view=vasp\n</code></pre> The <code>vasp_std</code> , <code>vasp_ncl</code>  and <code>vasp_gam</code>  executables are now available for use. Loading the uenv can also be directly done inside of a Slurm script.</p> Slurm script for running VASP on a single node<pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=vasp\n#SBATCH --time=24:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=16\n#SBATCH --gpus-per-task=1\n#SBATCH --uenv=vasp/v6.5.0:v1\n#SBATCH --view=vasp\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --partition=normal\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nsrun vasp_std\n</code></pre> <p>Note</p> <p>It\u2019s recommended to use the Slurm option <code>--gpus-per-task=1</code>, since VASP may fail to properly assign ranks to GPUs when running on more than one node. This is not required when using the CUDA MPS wrapper for oversubscription of GPUs.</p> <p>Note</p> <p>VASP relies on CUDA-aware MPI, which requires <code>MPICH_GPU_SUPPORT_ENABLED=1</code> to be set when using Cray MPICH. On Daint, this is set by default and does not have to be included in Slurm scripts.</p>"},{"location":"software/sciapps/vasp/#multiple-tasks-per-gpu","title":"Multiple Tasks per GPU","text":"<p>Using more than one task per GPU is possible with VASP and may lead to better GPU utilization. However, VASP relies on NCCL for efficient communication, but falls back to MPI when using multiple tasks per GPU. In many cases, this drawback is the greater factor and it\u2019s best to use one task per GPU.</p> <p>To run with multiple tasks per GPU, a wrapper script is required to start a CUDA MPS service. This script can be found at NVIDIA GH200 GPU nodes: multiple ranks per GPU.</p> Slurm script for running VASP on a single node with two tasks per GPU<pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=vasp\n#SBATCH --time=24:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --cpus-per-task=16\n#SBATCH --uenv=vasp/v6.5.0:v1\n#SBATCH --view=vasp\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --partition=normal\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nsrun ./mps-wrapper.sh vasp_std\n</code></pre>"},{"location":"software/sciapps/vasp/#building-vasp-from-source","title":"Building VASP from source","text":"<p>To build VASP from source, the <code>develop</code> view must first be loaded: <pre><code>uenv start vasp/v6.5.0:v1 --view=develop\n</code></pre></p> <p>All required dependencies can now be found in <code>/user-environment/env/develop</code>. Note that shared libraries might not be found when executing VASP, if the makefile does not include additional rpath linking options or <code>LD_LIBRARY_PATH</code> has not been extended.</p> <p>Warning</p> <p>The detection of MPI CUDA support does not work properly with Cray MPICH. After compiling from source, it\u2019s also required to set <code>export PMPI_GPU_AWARE=1</code> at runtime to disable the CUDA support check within VASP. Alternatively, since version 6.5.0, the build option <code>-DCRAY_MPICH</code> can be added to disable the check at compile time. The provided precompiled binaries of VASP are patched and do not require special settings.</p> <p>Examples for Makefiles that set the necessary rpath and link options on GH200:</p> Makefile for v6.5.0 <pre><code># Default precompiler options\nCPP_OPTIONS = -DHOST=\\\"LinuxNV\\\" \\\n             -DMPI -DMPI_INPLACE -DMPI_BLOCK=8000 -Duse_collective \\\n             -DscaLAPACK \\\n             -DCACHE_SIZE=4000 \\\n             -Davoidalloc \\\n             -Dvasp6 \\\n             -Dtbdyn \\\n             -Dqd_emulate \\\n             -Dfock_dblbuf \\\n             -D_OPENMP \\\n             -DACC_OFFLOAD \\\n             -DNVCUDA \\\n             -DUSENCCL \\\n             -DCRAY_MPICH\n\nCPP         = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\nCPP         = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\n\nCUDA_VERSION = $(shell nvcc -V | grep -E -o -m 1 \"[0-9][0-9]\\.[0-9],\" | rev | cut -c 2- | rev)\n\nCC          = mpicc -acc -gpu=cc90,cuda${CUDA_VERSION} -mp\nFC          = mpif90 -acc -gpu=cc90,cuda${CUDA_VERSION} -mp\nFCL         = mpif90 -acc -gpu=cc90,cuda${CUDA_VERSION} -mp -c++libs\n\nFREE        = -Mfree\n\nFFLAGS      = -Mbackslash -Mlarge_arrays\n\nOFLAG       = -fast\n\nDEBUG       = -Mfree -O0 -traceback\n\nLLIBS       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\n# Redefine the standard list of O1 and O2 objects\nSOURCE_O1  := pade_fit.o minimax_dependence.o\nSOURCE_O2  := pead.o\n\n# For what used to be vasp.5.lib\nCPP_LIB     = $(CPP)\nFC_LIB      = $(FC)\nCC_LIB      = $(CC)\nCFLAGS_LIB  = -O -w\nFFLAGS_LIB  = -O1 -Mfixed\nFREE_LIB    = $(FREE)\n\nOBJECTS_LIB = linpack_double.o\n\n# For the parser library\nCXX_PARS    = nvc++ --no_warnings\n\n##\n## Customize as of this point! Of course you may change the preceding\n## part of this file as well if you like, but it should rarely be\n## necessary ...\n##\n# When compiling on the target machine itself , change this to the\n# relevant target when cross-compiling for another architecture\n#\n# NOTE: Using \"-tp neoverse-v2\" causes some tests to fail. On GH200 architecture, \"-tp host\"\n# is recommended.\nVASP_TARGET_CPU ?= -tp host\nFFLAGS     += $(VASP_TARGET_CPU)\n\n# Specify your NV HPC-SDK installation (mandatory)\n#... first try to set it automatically\nNVROOT      =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n\n# If the above fails, then NVROOT needs to be set manually\n#NVHPC      ?= /opt/nvidia/hpc_sdk\n#NVVERSION   = 21.11\n#NVROOT      = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\n## Improves performance when using NV HPC-SDK &gt;=21.11 and CUDA &gt;11.2\n#OFLAG_IN   = -fast -Mwarperf\n#SOURCE_IN  := nonlr.o\n\n# Software emulation of quadruple precision (mandatory)\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd -Wl,-rpath,$(QD)/lib\nINCS       += -I$(QD)/include/qd\n\n# BLAS (mandatory)\nBLAS        = -lnvpl_blas_lp64_gomp -lnvpl_blas_core\n\n# LAPACK (mandatory)\nLAPACK      = -lnvpl_lapack_lp64_gomp -lnvpl_lapack_core\n\n# scaLAPACK (mandatory)\nSCALAPACK   = -lscalapack\n\nLLIBS      += $(SCALAPACK) $(LAPACK) $(BLAS) -Wl,-rpath,/user-environment/env/develop/lib -Wl,-rpath,/user-environment/env/develop/lib64 -Wl,--disable-new-dtags\n\n# FFTW (mandatory)\nFFTW_ROOT  ?= /user-environment/env/develop\nLLIBS      += -L$(FFTW_ROOT)/lib -lfftw3 -lfftw3_omp\nINCS       += -I$(FFTW_ROOT)/include\n\n# Use cusolvermp (optional)\n# supported as of NVHPC-SDK 24.1 (and needs CUDA-11.8)\n#CPP_OPTIONS+= -DCUSOLVERMP -DCUBLASMP\n#LLIBS      += -cudalib=cusolvermp,cublasmp -lnvhpcwrapcal\n\n# HDF5-support (optional but strongly recommended)\nCPP_OPTIONS+= -DVASP_HDF5\nHDF5_ROOT  ?= /user-environment/env/develop\nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran\nINCS       += -I$(HDF5_ROOT)/include\n\n# For the VASP-2-Wannier90 interface (optional)\nCPP_OPTIONS    += -DVASP2WANNIER90\nWANNIER90_ROOT ?= /user-environment/env/develop\nLLIBS          += -L$(WANNIER90_ROOT)/lib -lwannier\n\n# For the fftlib library (recommended)\n#CPP_OPTIONS+= -Dsysv\n#FCL        += fftlib.o\n#CXX_FFTLIB  = nvc++ -mp --no_warnings -std=c++11 -DFFTLIB_THREADSAFE\n#INCS_FFTLIB = -I./include -I$(FFTW_ROOT)/include\n#LIBS       += fftlib\n#LLIBS      += -ldl\n</code></pre> Makefile for v6.4.3 <pre><code># Default precompiler options\nCPP_OPTIONS = -DHOST=\\\"LinuxNV\\\" \\\n             -DMPI -DMPI_INPLACE -DMPI_BLOCK=8000 -Duse_collective \\\n             -DscaLAPACK \\\n             -DCACHE_SIZE=4000 \\\n             -Davoidalloc \\\n             -Dvasp6 \\\n             -Duse_bse_te \\\n             -Dtbdyn \\\n             -Dqd_emulate \\\n             -Dfock_dblbuf \\\n             -D_OPENMP \\\n             -D_OPENACC \\\n             -DUSENCCL -DUSENCCLP2P\n\nCPP         = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\n\nCUDA_VERSION = $(shell nvcc -V | grep -E -o -m 1 \"[0-9][0-9]\\.[0-9],\" | rev | cut -c 2- | rev)\n\nCC          = mpicc -acc -gpu=cc90,cuda${CUDA_VERSION} -mp\nFC          = mpif90 -acc -gpu=cc90,cuda${CUDA_VERSION} -mp\nFCL         = mpif90 -acc -gpu=cc90,cuda${CUDA_VERSION} -mp -c++libs\n\nFREE        = -Mfree\n\nFFLAGS      = -Mbackslash -Mlarge_arrays\n\nOFLAG       = -fast\n\nDEBUG       = -Mfree -O0 -traceback\n\nOBJECTS     = fftmpiw.o fftmpi_map.o fftw3d.o fft3dlib.o\n\nLLIBS       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\n# Redefine the standard list of O1 and O2 objects\nSOURCE_O1  := pade_fit.o minimax_dependence.o\nSOURCE_O2  := pead.o\n\n# For what used to be vasp.5.lib\nCPP_LIB     = $(CPP)\nFC_LIB      = $(FC)\nCC_LIB      = $(CC)\nCFLAGS_LIB  = -O -w\nFFLAGS_LIB  = -O1 -Mfixed\nFREE_LIB    = $(FREE)\n\nOBJECTS_LIB = linpack_double.o\n\n# For the parser library\nCXX_PARS    = nvc++ --no_warnings\n\n##\n## Customize as of this point! Of course you may change the preceding\n## part of this file as well if you like, but it should rarely be\n## necessary ...\n##\n# When compiling on the target machine itself , change this to the\n# relevant target when cross-compiling for another architecture\n#\n# NOTE: Using \"-tp neoverse-v2\" causes some tests to fail. On GH200 architecture, \"-tp host\"\n# is recommended.\nVASP_TARGET_CPU ?= -tp host\nFFLAGS     += $(VASP_TARGET_CPU)\n\n# Specify your NV HPC-SDK installation (mandatory)\n#... first try to set it automatically\nNVROOT      =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n\n# If the above fails, then NVROOT needs to be set manually\n#NVHPC      ?= /opt/nvidia/hpc_sdk\n#NVVERSION   = 21.11\n#NVROOT      = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\n## Improves performance when using NV HPC-SDK &gt;=21.11 and CUDA &gt;11.2\n#OFLAG_IN   = -fast -Mwarperf\n#SOURCE_IN  := nonlr.o\n\n# Software emulation of quadruple precision (mandatory)\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd -Wl,-rpath,$(QD)/lib\nINCS       += -I$(QD)/include/qd\n\n# BLAS (mandatory)\nBLAS        = -lnvpl_blas_lp64_gomp -lnvpl_blas_core\n\n# LAPACK (mandatory)\nLAPACK      = -lnvpl_lapack_lp64_gomp -lnvpl_lapack_core\n\n# scaLAPACK (mandatory)\nSCALAPACK   = -lscalapack\n\nLLIBS      += $(SCALAPACK) $(LAPACK) $(BLAS) -Wl,-rpath,/user-environment/env/develop/lib -Wl,-rpath,/user-environment/env/develop/lib64 -Wl,--disable-new-dtags\n\n# FFTW (mandatory)\nFFTW_ROOT  ?= /user-environment/env/develop\nLLIBS      += -L$(FFTW_ROOT)/lib -lfftw3 -lfftw3_omp\nINCS       += -I$(FFTW_ROOT)/include\n\n# Use cusolvermp (optional)\n# supported as of NVHPC-SDK 24.1 (and needs CUDA-11.8)\n#CPP_OPTIONS+= -DCUSOLVERMP -DCUBLASMP\n#LLIBS      += -cudalib=cusolvermp,cublasmp -lnvhpcwrapcal\n\n# HDF5-support (optional but strongly recommended)\nCPP_OPTIONS+= -DVASP_HDF5\nHDF5_ROOT  ?= /user-environment/env/develop\nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran\nINCS       += -I$(HDF5_ROOT)/include\n\n# For the VASP-2-Wannier90 interface (optional)\nCPP_OPTIONS    += -DVASP2WANNIER90\nWANNIER90_ROOT ?= /user-environment/env/develop\nLLIBS          += -L$(WANNIER90_ROOT)/lib -lwannier\n\n# For the fftlib library (recommended)\n#CPP_OPTIONS+= -Dsysv\n#FCL        += fftlib.o\n#CXX_FFTLIB  = nvc++ -mp --no_warnings -std=c++11 -DFFTLIB_THREADSAFE\n#INCS_FFTLIB = -I./include -I$(FFTW_ROOT)/include\n#LIBS       += fftlib\n#LLIBS      += -ldl\n</code></pre>"},{"location":"software/sciviz/","title":"Index","text":""},{"location":"software/sciviz/#scientific-visualization","title":"Scientific visualization","text":"<p>CSCS provides and supports a selection of scientific visualization applications on the computing systems: we usually build community codes that are adopted by several users on our systems.</p> <p>CSCS staff can also help users with performance tuning and ParaView Python code to optimise their workflow in production.</p> <ul> <li>ParaView</li> </ul>"},{"location":"software/sciviz/#other-applications","title":"Other applications","text":"<p>CSCS provides tools and environments for installing applications that are not on the list of supported applications.</p> <p>Under-construction</p> <p>We are building more guides for installing popular scientific visualization tools and libraries.</p>"},{"location":"software/sciviz/paraview/","title":"ParaView","text":""},{"location":"software/sciviz/paraview/#paraview","title":"ParaView","text":"<p>Paraview is supported software on Alps. See the main applications page for more information.</p> <p>ParaView is an open-source, multi-platform scientific data analysis and visualization tool that enables analysis and visualization of extremely large datasets. ParaView is both a general purpose, end-user application with a distributed architecture that can be seamlessly leveraged by your desktop or other remote parallel computing resources and an extensible framework with a collection of tools and libraries for various applications including scripting (using Python), web visualization (through trame and ParaViewWeb), or in situ analysis (with Catalyst).</p> <p>uenvs</p> <p>ParaView is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/sciviz/paraview/#running-paraview-in-batch-mode-with-python-scripts","title":"Running ParaView in batch mode with Python scripts","text":"<p>The following sbatch script can be used as a template.</p> GH200Eiger <p><pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=72\n#SBATCH --gpus-per-task=1\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=paraview/5.13.2:v2 --view=paraview\n#SBATCH --hint=nomultithread\n\nexport MPICH_GPU_SUPPORT_ENABLED=0\n\nsrun --cpus-per-task=72 --cpu_bind=sockets /user-environment/ParaView-5.13/gpu_wrapper.sh /user-environment/ParaView-5.13/bin/pvbatch ParaViewPythonScript.py\n</code></pre> Current observation is that best performance is achieved using one MPI rank per GPU. How to run multiple ranks per GPU is described here.</p> <pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=128\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=paraview/5.13.2:v2 --view=paraview\n#SBATCH --hint=nomultithread\n\nsrun --cpus-per-task=128 /user-environment/ParaView-5.13/bin/pvbatch ParaViewPythonScript.py\n</code></pre>"},{"location":"software/sciviz/paraview/#using-paraview-in-client-server-mode","title":"Using ParaView in client-server mode","text":"<p>A ParaView server can connect to a remote ParaView client installed on your desktop. Make sure to use the same version on both sides. Your local ParaView GUI client needs to create a SLURM job with appropriate parameters. We recommend that you make a copy of the file <code>/user-environment/ParaView-5.13/rc-submit-pvserver.sh</code> to your $HOME, such that you can further fine-tune it.</p> <p>You will need to add the corresponding XML code to your local ParaView installation, such that the Connect menu entry recognizes the ALPS cluster. The following code would be added to your local <code>$HOME/.config/ParaView/servers.pvsc</code> file</p> <p>XML code to add to your local ParaView settings</p> <pre><code>&lt;Servers&gt;\n  &lt;Server name=\"Reverse-Connect-Daint.Alps\" configuration=\"\" resource=\"csrc://:11111\" timeout=\"-1\"&gt;\n    &lt;CommandStartup&gt;\n      &lt;Options&gt;\n        &lt;Option name=\"MACHINE\" label=\"remote cluster\" save=\"true\"&gt;\n          &lt;String default=\"daint\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"SSH_USER\" label=\"SSH Username\" save=\"true\"&gt;\n          &lt;String default=\"your-userid\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"ACCOUNT\" label=\"Account to be charged\" save=\"true\"&gt;\n          &lt;String default=\"your-projectid\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"RESERVATION\" label=\"reservation name\" save=\"true\"&gt;\n          &lt;Enumeration default=\"none\"&gt;\n            &lt;Entry value=\"\" label=\"none\"/&gt;\n          &lt;/Enumeration&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"SSH_CMD\" label=\"SSH command\" save=\"true\"&gt;\n          &lt;File default=\"/usr/bin/ssh\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"REMOTESCRIPT\" label=\"The remote script which generates the SLURM job\" save=\"true\"&gt;\n          &lt;String default=\"/users/your-userid/rc-submit-pvserver.sh\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"PVNodes\" label=\"Number of cluster nodes\" save=\"true\"&gt;\n          &lt;Range type=\"int\" min=\"1\" max=\"128\" step=\"1\" default=\"1\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"PVTasks\" label=\"Number of pvserver per node\" save=\"true\"&gt;\n          &lt;Range type=\"int\" min=\"1\" max=\"4\" step=\"1\" default=\"4\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"Queue\" label=\"Queue\" save=\"true\"&gt;\n          &lt;Enumeration default=\"normal\"&gt;\n            &lt;Entry value=\"normal\" label=\"normal\"/&gt;\n            &lt;Entry value=\"debug\" label=\"debug\"/&gt;\n          &lt;/Enumeration&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"MemxNode\" label=\"MemxNode\" save=\"true\"&gt;\n          &lt;Enumeration default=\"standard\"&gt;\n            &lt;Entry value=\"high\" label=\"high\"/&gt;\n            &lt;Entry value=\"standard\" label=\"standard\"/&gt;\n          &lt;/Enumeration&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"VERSION\" label=\"VERSION ?\" save=\"true\"&gt;\n          &lt;Enumeration default=\"5.13.2:v2\"&gt;\n            &lt;Entry value=\"5.13.2:v2\" label=\"5.13.2:v2\"/&gt;\n          &lt;/Enumeration&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"PV_SERVER_PORT\" label=\"pvserver port\" save=\"true\"&gt;\n          &lt;Range type=\"int\" min=\"1024\" max=\"65535\" step=\"1\" default=\"1100\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"NUMMIN\" label=\"job wall time\" save=\"true\"&gt;\n          &lt;String default=\"00:29:59\"/&gt;\n        &lt;/Option&gt;\n        &lt;Option name=\"SESSIONID\" label=\"Session id\" save=\"true\"&gt;\n          &lt;String default=\"ParaViewServer\"/&gt;\n        &lt;/Option&gt;\n      &lt;/Options&gt;\n      &lt;Command exec=\"$SSH_CMD$\" delay=\"5\" process_wait=\"0\"&gt;\n        &lt;Arguments&gt;\n          &lt;Argument value=\"-A\"/&gt;\n          &lt;Argument value=\"-l\"/&gt;\n          &lt;Argument value=\"$SSH_USER$\"/&gt;\n          &lt;Argument value=\"-R\"/&gt;\n          &lt;Argument value=\"$PV_SERVER_PORT$:localhost:$PV_SERVER_PORT$\"/&gt;\n          &lt;Argument value=\"$MACHINE$\"/&gt;\n          &lt;Argument value=\"$REMOTESCRIPT$\"/&gt;\n          &lt;Argument value=\"$SESSIONID$\"/&gt;\n          &lt;Argument value=\"$NUMMIN$\"/&gt;\n          &lt;Argument value=\"$PVNodes$\"/&gt;\n          &lt;Argument value=\"$PVTasks$\"/&gt;\n          &lt;Argument value=\"$PV_SERVER_PORT$\"/&gt;\n          &lt;Argument value=\"$MACHINE$\"/&gt;\n          &lt;Argument value=\"$VERSION$\"/&gt;\n          &lt;Argument value=\"$Queue$\"/&gt;\n          &lt;Argument value=\"$MemxNode$\"/&gt;\n          &lt;Argument value=\"$ACCOUNT$\"/&gt;\n          &lt;Argument value=\"$RESERVATION$;\"/&gt;\n          &lt;Argument value=\"sleep\"/&gt;\n          &lt;Argument value=\"6000\"/&gt;\n        &lt;/Arguments&gt;\n      &lt;/Command&gt;\n  &lt;/CommandStartup&gt;\n  &lt;/Server&gt;\n&lt;/Servers&gt;\n</code></pre>"},{"location":"software/uenv/","title":"Index","text":""},{"location":"software/uenv/#uenv","title":"uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools. This page will explain how to find, download and use uenv on the command line, and how to enable them in Slurm jobs.</p> <p>Uenv are typically application-specific, domain-specific or tool-specific - each uenv contains only what is required for the application or tools that it provides.</p> <p>Each uenv is packaged in a single file (in the Squashfs file format), that stores a compressed directory tree that contains all of the software, tools and other information like modules, required to provide a rich environment.</p> <p>Each environment contains a software stack, comprised of compilers, libraries, tools and scientific applications - built using Spack.</p> <ul> <li> <p> Building uenv</p> <p>More adventurous users can create their own uenv for personal use, and for other users in their team and community.</p> <p> Building uenv</p> </li> <li> <p> Configuring uenv</p> <p>Users can customize the behavior of uenv using a configuration file.</p> <p> Configuring uenv</p> </li> <li> <p> Deploying uenv</p> <p>Documentation on how CSCS deploys uenv images, that might also be of interest to users.</p> <p> Deploying uenv</p> </li> <li> <p> Release notes</p> <p>Release notes for the uenv tools (CLI and Slurm plugin).</p> <p> Deploying uenv</p> </li> </ul>"},{"location":"software/uenv/#before-starting","title":"Before starting","text":"<p>After logging into an Alps cluster, you can quickly check the availability of uenv with the following commands:</p> <pre><code>$ uenv status\nthere is no uenv loaded\n$ uenv --version\n8.1.0\n</code></pre> <p>On Alps clusters the most recent version 8.1.0 is installed.</p> Out of date uenv version on Balfrin <p>The uenv tool available on Balfrin is a different version than the one described below, and some commands will be different to those documented here.</p> <p>Please refer to <code>uenv --help</code> for the correct usage on Balfrin.</p>"},{"location":"software/uenv/#finding-uenv","title":"Finding uenv","text":"<p>Uenv for programming environments, tools and applications are provided by CSCS on each Alps system.</p> <p>Info</p> <p>The same uenv are not installed on every system. Instead uenv that are supported for the users of that platform are provided.</p> <p>The available uenv images are stored in a registry, that can be queried using the <code>uenv image find</code>  command:</p> <p>uenv image find</p> <pre><code>$ uenv image find\nuenv                       arch  system  id                size(MB)  date\ncp2k/2024.1:v1             zen2  eiger   2a56f1df31a4c196   2,693    2024-07-01\ncp2k/2024.2:v1             zen2  eiger   f83e95328d654c0f   2,739    2024-08-23\ncp2k/2024.3:v1             zen2  eiger   7c7369b64b5fabe5   2,740    2024-09-18\neditors/24.7:rc1           zen2  eiger   e5fb284962908eed   1,030    2024-07-18\neditors/24.7:v2            zen2  eiger   4f0f2770616135b1   1,062    2024-09-04\njulia/24.9:v1              zen2  eiger   0ff97a74dfcaa44e     539    2024-11-09\nlinalg/24.11:rc1           zen2  eiger   b69f4664bf0cd1c4     770    2024-11-20\nlinalg/24.11:v1            zen2  eiger   c11f6c85028abf5b     776    2024-12-03\nlinalg-complex/24.11:v1    zen2  eiger   846a04b4713d469b     792    2024-12-03\nlinaro-forge/24.0.2:v1     zen2  eiger   65734ce35494a5f5     313    2024-07-18\nlinaro-forge/24.1:v1       zen2  eiger   b65d7c85adfb317a     344    2024-11-27\nnetcdf-tools/2024:v1       zen2  eiger   e7e508c34cf40ccd   3,706    2024-11-14\nprgenv-gnu/24.11:rc4       zen2  eiger   811469b00f030493     570    2024-11-21\nprgenv-gnu/24.11:v1        zen2  eiger   0b6ab5fc4907bb38     572    2024-11-27\nprgenv-gnu/24.7:v1         zen2  eiger   7f68f4c8099de257     478    2024-07-01\nquantumespresso/v7.3.1:v1  zen2  eiger   61d1f21881a65578     864    2024-11-08\n</code></pre> <p>The output above lists all of the uenv that are available on the current system (Eiger in this case). The search can be refined by providing a label.</p> Using labels to refine search <pre><code># find all uenv with name prgenv-gnu\nuenv image find prgenv-gnu\n\n# find all uenv with name and version prgenv-gnu/24.11\nuenv image find prgenv-gnu/24.11\n\n# find all uenv available for daint\nuenv image find @daint\n\n# find all prgenv-gnu uenv available on a cluster\nuenv image find prgenv-gnu@daint\n\n# find all uenv in the service namespace with name myenv\nuenv image find service::myenv\n</code></pre> <p>Info</p> <p>All uenv commands that take a label as an argument use the same flexible syntax label descriptions.</p>"},{"location":"software/uenv/#downloading-uenv","title":"Downloading uenv","text":"Using uenv for the first time on Balfrin <p>With the old version of uenv installed on Balfrin, before downloading your first image, a local directory for storing the images must first be created, otherwise you will receive an error message that the repository does not exist.</p> <p>To create a repo in the default location, use the following command:</p> Create default uenv image repository<pre><code>uenv repo create\n</code></pre> <p>To use a uenv, it first has to be pulled from the registry to local storage where you can access it. For example, to use the <code>prgenv-gnu</code> uenv, use the uenv image pull command:</p> <p>uenv image pull</p> <pre><code># The following commands have the same effect\n\n# method 1: pull using the name of the uenv\nuenv image pull prgenv-gnu/24.2:v1\n\n# method 2: pull using the id of the image\nuenv image pull 3ea1945046d884ee\n</code></pre> <p>Some images can be large, over 10 GB, and it can take a while to download them from the registry.</p> <p>To view all uenv that have been pulled, and are ready to use use the <code>uenv image ls</code> command:</p> <p>listing downloaded uenv</p> <pre><code>$ uenv image ls\nuenv                           arch   system  id                size(MB)  date\neditors/24.7:v2                gh200  daint   e7b0d930df729da5   1,270    2024-09-04\ngromacs/2024:v1                gh200  daint   b58e6406810279d5   3,658    2024-09-12\njulia/24.9:v1                  gh200  daint   7a4269abfdadc046   3,939    2024-11-09\nlinalg/24.11:v1                gh200  daint   e1640cf6aafdca01   4,461    2024-12-03\nlinaro-forge/23.1.2:v1         gh200  daint   fd67b726a90318d6     341    2024-08-26\nnamd/3.0:v3                    gh200  daint   49bc65c6905eb5da   4,028    2024-12-12\nnetcdf-tools/2024:v1           gh200  daint   2a799e99a12b7c13   1,260    2024-09-04\nprgenv-gnu/24.11:v1            gh200  daint   b81fd6ba25e88782   4,191    2024-11-27\nprgenv-gnu/24.7:v3             gh200  daint   b50ca0d101456970   3,859    2024-08-23\nprgenv-nvfortran/24.11:v1      gh200  daint   d2afc254383cef20   8,703    2025-01-30\n</code></pre> <p></p>"},{"location":"software/uenv/#accessing-restricted-software","title":"Accessing restricted software","text":"<p>By default, uenv can be pulled by all users on a system, with no restrictions.</p> <p>Some uenv are not available to all users, for example the <code>vasp</code> images are only available for users with a VASP license, who are added to the <code>vasp</code> group once then have provided CSCS with a copy of their license.</p> <p>To be able to pull such images a token that authorizes access must be provided. Tokens are created by CSCS, and stored on SCRATCH in a file that only users who have access to the software can read.</p> <p>using a token to access VASP</p> <pre><code>uenv image pull \\\n    --token=/capstor/scratch/cscs/bcumming/tokens/vasp6 \\\n    --username=vasp6 \\\n    vasp/v6.4.3:v1\n</code></pre> <p>Note</p> <p>As of June 2025, the only restricted software is VASP.</p> <p>Note</p> <p>Better token management is under development - tokens will be stored in a central location and will be easier to use.</p> <p></p>"},{"location":"software/uenv/#starting-a-uenv-session","title":"Starting a uenv session","text":"<p>The <code>uenv start</code> command will start a new shell with one or more uenv images mounted. This is very useful for interactive sessions, for example if you want to work in the terminal to compile an application, or set up a Python virtual environment.</p> <p>start an interactive shell to compile an application</p> <p>Here we want to compile an MPI + CUDA application \u201caffinity\u201d.</p> <pre><code># start the prgenv-gnu uenv, which provides MPI, cuda and CMake\n# use the \"default\" view, which will load all of the software in the uenv\n$ uenv start prgenv-gnu/24.11:v1 --view=default\n\n# clone the software and set up the build directory\n$ git clone https://github.com/bcumming/affinity.git\n$ mkdir -p affinity/build\n$ cd affinity/build/\n\n# configure the build with CMake, then call make to build\n# mpicc, mpic++ and cmake are all provided by the uenv\n$ CXX=mpic++ CC=mpicc cmake ..\n$ make -j\n\n# run the affinity executable on two nodes - note how the uenv is\n# automatically loaded by Slurm on the compute nodes, because CUDA and MPI from\n# the uenv are required to run.\n$ srun -n2 -N2 ./affinity.cuda\nGPU affinity test for 2 MPI ranks\nrank      0 @ nid005636\n cores   : 0-287\n gpu   0 : GPU-13a62579-bf3c-fb6b-667f-f2c588f4667b\n gpu   1 : GPU-74968c03-7401-9013-0590-8445b3623208\n gpu   2 : GPU-dfbd9ec1-a4b7-4a8d-603e-ebcc360f55a3\n gpu   3 : GPU-6a44522d-bf84-9864-decf-6d3e85078442\nrank      1 @ nid006322\n cores   : 0-287\n gpu   0 : GPU-6d96b1d5-69e9-7bd4-f59a-a37ec1f5da1c\n gpu   1 : GPU-c0508d69-a357-934e-87a0-be04adf4eee9\n gpu   2 : GPU-02a7fd85-ff41-1d81-d010-d7a85f6134d8\n gpu   3 : GPU-e07d996e-4d67-c9f4-cf75-81cfd45a1ae1\n\n# finish the uenv session\n$ exit\n</code></pre> <p>which shell is used</p> <p><code>uenv start</code> starts a new shell, and by default it will use the default shell for the user. You can see the default shell by looking at the <code>$SHELL</code> environment variable. If you want to force a different shell: <pre><code>SHELL=`which zsh` uenv start ...\n</code></pre></p> C Shell / tcsh users <p>uenv is tested extensively with bash (the default shell), and zsh. C shell is not tested properly, and we will not make significant changes to uenv to maintain support for C shell.</p> <p>If your are one of the handful of users using <code>tcsh</code> (C shell) and you want to use uenv, we strongly recommend creating a request at the CSCS service desk to change to either bash or zsh as your default.</p> Failed to unshare the mount namespace <p>If you get the following error message when starting a uenv: <pre><code>$ uenv start linalg/24.11:v1\nsquashfs-mount: Failed to unshare the mount namespace: Operation not permitted\n</code></pre> you most likely already have a uenv mounted. The <code>uenv status</code> command will report that you have a uenv loaded if that is the case: <pre><code>$ uenv status\nprgenv-gnu:/user-environment\n  GNU Compiler toolchain with cray-mpich, Python, CMake and other development tools.\n  views:\n    spack: configure spack upstream\n    modules: activate modules\n    default:\n</code></pre> Unload the active uenv by exiting the current shell before loading the new uenv.</p> <p>The basic syntax of uenv start is <code>uenv start image</code> where <code>image</code> is the uenv to start. The image can be a label, the hash/id of the uenv, or a file:</p> <p>uenv start</p> <pre><code># start the image using the name of the uenv\n$ uenv start netcdf-tools/2024:v1\n\n# or use the unique id of the uenv\n$ uenv start 499c886f2947538e\n\n# or provide the path to a squashfs file\n$ uenv start $SCRATCH/my-uenv/gromacs.squashfs\n</code></pre> what does \u2018uenv start\u2019 actually do? <p>uenv are squashfs images, which are a compressed file that contains a directory tree. The squashfs image of a uenv is a directory that contains all of the software provided by the uenv, along with useful meta data. When you run <code>uenv start</code> (or <code>uenv run</code>, or use the <code>--uenv</code> flag with Slurm) the squashfs file is mounted at the mount location for the uenv, which is most often <code>/user-environment</code>.</p> <pre><code># log into daint\n$ ssh daint.alps.cscs.ch\n\n# /user-environment is empty\n$ ls -l /user-environment\ntotal 0\n\n# start a uenv\n$ uenv start prgenv-nvfortran/24.11:v1\n\n# the uenv software is now available\n$ ls /user-environment/\nbin  config  env  linux-sles15-neoverse_v2  meta  modules  repo\n\n# findmnt verifies that a squashfs image has been mounted\n$ findmnt /user-environment\nTARGET            SOURCE      FSTYPE   OPTIONS\n/user-environment /dev/loop25 squashfs ro,nosuid,nodev,relatime,errors=continue\n\n# end the session and verify that the uenv is not longer mounted\n$ exit\n$ ls -l /user-environment\ntotal 0\n</code></pre> <p>Loading an environment has no impact on other users or other terminal sessions that you have open on the same node \u2013 the mounted environment is only visible in your terminal. This means that multiple users on a login node can mount their own environment at the same mount point, without interfering with one-another.</p>"},{"location":"software/uenv/#views","title":"Views","text":"<p>Running <code>uenv start $label</code> on its own will create a shell with the software at <code>/user-environment</code> or <code>/user-tools</code>, however no changes are made to environment variables like <code>$PATH</code>.</p> <p>Uenv images provide views, which will set environment variables that load the software into your environment. Views are loaded using the <code>--view</code> flag for <code>uenv start</code> (also for <code>uenv run</code> and the Slurm plugin, documented below)</p> <p>loading views</p> <pre><code># activate the view named default in prgenv-gnu\n$ uenv start --view=default prgenv-gnu/24.11:v1\n\n# activate both the spack and modules views in prgenv-gnu using\n# a comma-separated list of view names\n$ uenv start --view=spack,modules prgenv-gnu/24.11:v1\n\n# when starting multiple uenv, you can disambiguate using uenvname:viewname\n$ uenv start --view=prgenv-gnu:default,editors:ed prgenv-gnu/24.11:v1,editors\n</code></pre>"},{"location":"software/uenv/#modules","title":"Modules","text":"<p>Most uenv provide the modules, that can be accessed using the <code>module</code> command. By default, the modules are not activated when a uenv is started, and need to be explicitly activated using the <code>module</code> view.</p> <p>using the module view</p> <pre><code>$ uenv start prgenv-gnu/24.11:v1 --view=modules\n$ module avail\n---------------------------- /user-environment/modules ----------------------------\n   aws-ofi-nccl/git.v1.9.2-aws_1.9.2    lua/5.4.6\n   boost/1.86.0                         lz4/1.10.0\n   cmake/3.30.5                         meson/1.5.1\n   cray-mpich/8.1.30                    nccl-tests/2.13.6\n   cuda/12.6.2                          nccl/2.22.3-1\n   fftw/3.3.10                          netlib-scalapack/2.2.0\n   fmt/11.0.2                           ninja/1.12.1\n   gcc/13.3.0                           openblas/0.3.28\n   gsl/2.8                              osu-micro-benchmarks/5.9\n   hdf5/1.14.5                          papi/7.1.0\n   kokkos-kernels/4.4.01                python/3.12.5\n   kokkos-tools/develop                 superlu/5.3.0\n   kokkos/4.4.01                        zlib-ng/2.2.1\n   libtree/3.1.1\n$ module load cuda gcc cmake\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCuda compilation tools, release 12.6, V12.6.77\n$ gcc --version\ngcc (Spack GCC) 13.3.0\n$ cmake --version\ncmake version 3.30.5\n</code></pre> <p></p> bash: module: command not found <p>Version 9.0.0 of uenv, installed on October 22 2025, has a bug that removes the module command on Santis and Clariden.</p> <p>Note</p> <p>The issue does not affect uenv in Slurm jobs, or on Daint and Eiger.</p> <p>Note</p> <p>A fix has been implemented, and will be installed as soon as possible.</p> <p>Specifically, the <code>module</code> command is not available inside <code>uenv start</code> sessions: <pre><code>$ uenv start prgenv-gnu/24.11:v2 --view=modules\n$ module avail\nbash: module: command not found\n</code></pre></p> <p>The workaround is to manually load the module tool after starting your uenv session:</p> <pre><code>$ uenv start prgenv-gnu/24.11:v2 --view=modules\n$ source /usr/share/lmod/8.7.17/init/bash\n$ module avail\n\n-------------- /user-environment/modules ---------------\n   aws-ofi-nccl/git.v1.9.2-aws_1.9.2\n   boost/1.86.0\n   cmake/3.30.5\n   cray-mpich/8.1.30\n   cuda/12.6.0\n   fftw/3.3.10\n   fmt/11.0.2\n   gcc/13.3.0\n   ...\n</code></pre>"},{"location":"software/uenv/#spack","title":"Spack","text":"<p>uenv images provide a full upstream Spack configuration to facilitate building your own software with Spack using the packages installed inside as dependencies. No view needs to be loaded to use Spack, however all uenv provide a <code>spack</code> view that sets some environment variables that contain useful information like the location of the Spack configuration, and the version of Spack that was used to build the uenv. For more information, see our guide on building software with Spack and uenv.</p> <p></p>"},{"location":"software/uenv/#running-a-uenv","title":"Running a uenv","text":"<p>The <code>uenv run</code> command can be used to run an application or script in a uenv environment, and return control to the calling shell when the command has finished running.</p> how is <code>uenv run</code> different from <code>uenv start</code>? <p><code>uenv start</code> sets up the uenv environment, then starts an interactive shell in that environment. When you are finished, you can type <code>exit</code> to finish the session.</p> <p><code>uenv run</code> is more generic - instead of running a shell in environment, it takes the executable and arguments to run in the shell. The following commands are equivalent:</p> <pre><code># start a new bash shell in prgenv-gnu\nuenv start prgenv-gnu/24.11\n# start a new bash shell in prgenv-gnu\nuenv run prgenv-gnu/24.11 -- bash\n</code></pre> <p>running cmake</p> <p>Call <code>cmake</code> to configure a build with the <code>default</code> view loaded <pre><code># run a command\n$ uenv run prgenv-gnu/24.11:v1 --view=default -- cmake -DUSE_GPU=cuda ..\n</code></pre></p> <p>running an application executable</p> <p>Run the GROMACS executable from inside the <code>gromacs</code> uenv. <pre><code># run an executable:\n$ uenv run --view=gromacs gromacs/2024:v1 -- gmx_mpi\n</code></pre></p> <p>running applications with different environments</p> <p><code>uenv run</code> is useful for running multiple applications or scripts in a pipeline or workflow, where each application has separate requirements. In this example the pre and post processing stages use <code>prgenv-gnu</code>, while the simulation stage uses the <code>gromacs</code> uenv. <pre><code># run multiple applications, one after the other, that have different requirements\n$ uenv run --view=default prgenv-gnu/24.11:v1 -- ./pre-processing-script.sh\n$ uenv run --view=gromacs gromacs/2024:v1 -- gmx_mpi $gromacs_args\n$ uenv run --view=default prgenv-gnu/24.11:v1 -- ./post-processing-script.sh\n</code></pre></p> <p></p>"},{"location":"software/uenv/#slurm-integration","title":"Slurm integration","text":"<p>The environment to load can be provided directly to Slurm via three arguments:</p> <ul> <li><code>--uenv</code>:  a comma-separated list of uenv to mount</li> <li><code>--view</code>:  a comma-separated list of views to load</li> <li><code>--repo</code>:  an alternative (if not set, the default repo in <code>$SCRATCH/.uenv-images</code> is used)</li> </ul> <p>For example, the flags can be used with srun : <pre><code># mount the uenv prgenv-gnu with the view named default\n&gt; srun --uenv=prgenv-gnu/24.7:v3 --view=default ...\n\n# mount an image at an explicit location (/user-tools)\n&gt; srun --uenv=$IMAGES/myenv.squashfs:/user-tools ...\n\n# mount multiple images: use a comma to separate the different options\n&gt; srun --uenv=prgenv-gnu/24.7:v3,editors/24.7:v2 --view=default,editors:modules ...\n</code></pre></p> <p>The commands can also be used in sbatch scripts to have fine-grained control:</p> <p>sbatch script for uenv</p> <p>It is possible to provide a uenv that is loaded inside the script, and will be loaded by default by all srun commands that do not override it with their own <code>--uenv</code> parameters. <pre><code>#!/bin/bash\n\n#SBATCH --uenv=editors/24.7:v2\n#SBATCH --view=editors:ed\n#SBATCH --ntasks=4\n#SBATCH --nodes=1\n#SBATCH --output=out-%j.out\n#SBATCH --error=out-%j.out\n\necho \"==== test in script ====\"\n# the fd command is provided by the ed view\n# use it to inspect the meta data in the mounted image\nfd . /user-tools/meta/recipe\n\necho \"==== test in srun ====\"\n# use srun to launch the parallel job\nsrun -n4 bash -c 'echo $SLURM_PROCID on $(hostname): $(which emacs)'\n\necho \"==== alternative mount ====\"\nsrun -n4 --uenv=prgenv-gnu --view=prgenv-gnu:default bash -c 'echo $SLURM_PROCID on $(hostname): $(which mpicc)'\nsbatch output\n</code></pre></p> <p>The sbatch job above would generate output like the following: <pre><code>==== test in script ====\n/user-tools/meta/recipe/compilers.yaml\n/user-tools/meta/recipe/config.yaml\n/user-tools/meta/recipe/environments.yaml\n/user-tools/meta/recipe/modules.yaml\n==== test in srun ====\n1 on nid007144: /user-tools/env/ed/bin/emacs\n3 on nid007144: /user-tools/env/ed/bin/emacs\n0 on nid007144: /user-tools/env/ed/bin/emacs\n2 on nid007144: /user-tools/env/ed/bin/emacs\n==== alternative mount ====\n0 on nid007144: /user-environment/env/default/bin/mpicc\n1 on nid007144: /user-environment/env/default/bin/mpicc\n2 on nid007144: /user-environment/env/default/bin/mpicc\n3 on nid007144: /user-environment/env/default/bin/mpicc\n</code></pre></p> <p>In the example above, the <code>#SBATCH --uenv</code>  and <code>#SBATCH --view</code>  parameters in the preamble of the sbatch script set the default uenv to <code>editors</code> with the view <code>ed</code>.</p> <ul> <li><code>editors</code> is mounted and the view set in the script (the \u201ctest in script\u201d part)</li> <li><code>editors</code> is also mounted in the first call to srun (which does not provide a <code>`\u2013-uenv</code> flag)</li> </ul> <p>it is possible to override the default uenv by passing a different <code>--uenv</code>  and <code>--view</code>  flags to an <code>srun</code>  call inside the script, as is done in the second <code>srun</code>  call.</p> <ul> <li>Note how the second call has access to <code>mpicc</code>, provided by <code>prgenv-gnu</code>.</li> </ul> <p></p>"},{"location":"software/uenv/#uenv-labels","title":"uenv labels","text":"<p>Uenv are referred to using labels, where a label has the following form</p> <pre><code>name/version:tag@system%uarch\n</code></pre> <p>The different fields are described in the following table:</p> label meaning <code>uarch</code> node microarchitecture: one of <code>gh200</code>, <code>a100</code>, <code>zen2</code>, <code>mi200</code>, <code>mi300a</code>, <code>zen3</code> <code>cluster</code> name of the cluster, e.g. <code>daint</code>, <code>clariden</code>, <code>eiger</code> or <code>santis</code> <code>name</code> name of the uenv, e.g. <code>gromacs</code>, <code>prgenv-gnu</code> <code>version</code> version of the uenv: e.g. <code>v8.7</code>, <code>2025.01</code> <code>tag</code> a tag applied by CSCS <p>Example labels</p> <code>prgenv-gnu/24.11:v2@daint%gh200</code> <p>The <code>prgenv-gnu</code> programming environment, built on Daint for the Grace-Hopper GH200 (<code>gh200</code>) architecture.</p> <ul> <li>the version <code>24.11</code> refers to the version</li> <li>the tag <code>v2</code> refers to a minor update to the uenv (e.g. a bug fix or addition of a new package).</li> </ul> <code>cp2k/2025.1:v3@eiger%zen2</code> <p>The uenv provides version 2025.1 of the CP2K simulation code, built for the AMD Rome (<code>zen2</code>) architecture of Eiger.</p> <ul> <li>the version <code>2025.1</code> refers to the CP2K version v2025.1</li> <li>the tag <code>v2</code> refers to a minor update by CSCS to the original <code>v1</code> version of the uenv that had a bug.</li> </ul> <p>For more information about the labeling scheme, see the uenv deployment docs.</p> <p></p>"},{"location":"software/uenv/#using-labels","title":"Using labels","text":"<p>The uenv command line has a flexible interface for filtering uenv by providing only part of the full label:</p> <pre><code># search for all uenv on the current system that have the name prgenv-gnu\nuenv image find prgenv-gnu\n\n# search for all uenv with version 24.11\nuenv image find /24.11\n\n# search for all uenv with tag v1\nuenv image find :v1\n\n# search for a specific version\nuenv image find prgenv-gnu/24.11:v1\n</code></pre> <p>By default, the <code>uenv</code> filters results to uenv that were built on the current cluster. The name of the current cluster is always available via the <code>CLUSTER_NAME</code> environment variable.</p> <pre><code># log into the eiger vCluster\nssh eiger\n\n# this command will search for all prgenv-gnu uenv on _eiger_\nuenv image find prgenv-gnu\n\n# use @ to search on a specific system, e.g. on daint:\nuenv image find prgenv-gnu@daint\n\n# this can be used to search for all uenv on daint:\nuenv image find @daint\n\n# the '*' is a wildcard used meaning \"all systems\"\n# this will show all images on all systems\n# NOTE: the * character must be quoted in single quotes\nuenv image find @'*'\n\n# search for all images on Alps that were built for gh200 nodes.\nuenv image find @'*'%gh200\n</code></pre> <p>Note</p> <p>The wild card <code>*</code> used for \u201call systems\u201d must always be escaped in single quotes: <code>@'*'</code>.</p> <p></p>"},{"location":"software/uenv/#custom-environments","title":"Custom environments","text":"<p>Keep bashrc clean</p> <p>It is common practice to add <code>module</code> commands to <code>~.bashrc</code>, for example ~/.bashrc<pre><code># make my custom modules available\nmodule use $STORE/myenv/modules\n# load the modules that I always want in the environment\nmodule load ncview\n</code></pre></p> <p>This will make custom modules available, and load <code>ncview</code>, every time you log in. It is not possible to do the equivalent with <code>uenv start</code>, for example: ~/.bashrc<pre><code># start the uenv that I always use\nuenv start prgenv-gnu/24.11:v2 --view=default\n# ERROR: the following lines will not be executed\nmodule use $STORE/myenv/modules\nmodule load ncview\n</code></pre></p> <p>Why can\u2019t I use <code>uenv start</code> in <code>~/.bashrc</code>?</p> <p>The <code>module</code> command uses some \u201cclever\u201d tricks to modify the environment variables in your current shell. For example, <code>module load ncview</code> will modify the value of environment variables like <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, and <code>PKG_CONFIG_PATH</code>.</p> <p>The <code>uenv start</code> command loads a uenv, and starts a new shell, ready for you to enter commands. This means that lines in the <code>.bashrc</code> that follow the command are never executed.</p> <p>Things are further complicated because if <code>uenv start</code> is executed inside <code>~/.bashrc</code>, the shell is not a tty shell.</p> <p>It is possible to create a custom command that will start a new shell with a uenv loaded, with additional customizations to the environment (e.g. loading modules and setting environment variables).</p> <p>The first step is to create a script that performs the the customization steps to perform once the uenv has been loaded. Here is an example for an environment called <code>myenv</code>:</p> ~/.myenvrc<pre><code># always add this line\nsource ~/.bashrc\n\n# then add customization commands here\nmodule use $STORE/myenv/modules\nmodule load ncview\nexport DATAPATH=$STORE/2025/data\n</code></pre> <p>Then create an alias in <code>~/.bashrc</code> for the <code>myenv</code> environment:</p> ~/.bashrc<pre><code>alias myenv='uenv run prgenv-gnu/24.11:v2 --view=default -- bash --rcfile ~/.myenvrc'\n</code></pre> <p>This alias uses <code>uenv run</code> to start a new bash shell that will apply the customizations in <code>~/.myenvrc</code> once the uenv has been loaded. Then, the environment can be started with a single command once logged in.</p> <pre><code>$ ssh eiger.cscs.ch\n$ myenv\n</code></pre> <p>The benefit of this approach is that you can create multiple environments, whereas modifying <code>.bashrc</code> will lock you into using the same environment every time you log in.</p> <p></p>"},{"location":"software/uenv/#uninstalling-the-uenv-tool","title":"Uninstalling the uenv tool","text":"<p>It is strongly recommended to use the version of uenv installed on the system, instead of installing your own version from source. This guide walks through the process of detecting if you have an old version installed, and removing it.</p> <p>Note</p> <p>In the past CSCS has recommended installing a more recent version of uenv to help fix issues for some users. Some users still have old self-installed versions installed in <code>HOME</code>, so they are missing out on new uenv features, and possibly seeing errors on systems with the most recent <code>v9.0.0</code> uenv installed.</p> <p>error caused by incompatible uenv version</p> <p>If trying to use <code>uenv run</code> or <code>uenv start</code>, and you see an error message like the following:</p> <pre><code>error: unable to exec '/capstor/scratch/cscs/wombat/.uenv-images/images/61d1f21881a6....squashfs:/user-environment':\nNo such file or directory (errno=2)\n</code></pre> <p>Then you are probably using an old version of uenv that is not compatible with the version of <code>squashfs-mount</code> installed on the system.</p>"},{"location":"software/uenv/#detecting-which-version-of-uenv-is-installed","title":"Detecting which version of uenv is installed","text":"<p>First, log into the target cluster, and enter <code>type uenv</code> and inspect the output.</p> <p>The system version of <code>uenv</code> is installed in <code>/usr/bin</code>, so if you see the following you do not need to make any changes:</p> <pre><code>$ type uenv\nuenv is /usr/bin/uenv\n</code></pre> <p>Version 5 of uenv used a bash function called <code>uenv</code>, which will give output that looks like this:</p> <pre><code>$ type uenv\nuenv is a function\nuenv ()\n{\n    local _last_exitcode=$?;\n    function uenv_usage ()\n    {\n...\n</code></pre> <p>If you have installed version 6, 7, 8 or 9, it will be in a different location, for example:</p> <pre><code>$ type uenv\nuenv is /users/voldemort/.local/bin/uenv\n</code></pre> why not use <code>which uenv</code>? <p>The <code>which uenv</code> command searches the directories listed in the <code>PATH</code> environment variable for the <code>uenv</code> executable, and ignores bash functions. If there is a <code>uenv</code> bash function is set, then it will be take precedence over the <code>uenv</code> executable found using <code>which uenv</code>.</p>"},{"location":"software/uenv/#removing-version-6-or-later","title":"Removing version 6 or later","text":"<p>To remove uenv version 6, 7, 8 or 9, delete the executable, then force bash to forget the old command.</p> <pre><code># remove your self-installed executable\n$ rm $(which uenv)\n\n# forget the old uenv command\n$ hash -r\n\n# type and which should point to the same executable in /usr/bin\n$ type uenv\nuenv is /usr/bin/uenv\n$ which uenv\n/usr/bin/uenv\n</code></pre>"},{"location":"software/uenv/#removing-version-5","title":"Removing version 5","text":"<p>To remove version 5, look in your <code>.bashrc</code> file for a line like the following and remove or comment it out:</p> <pre><code># configure the user-environment (uenv) utility\nsource /users/voldemort/.local/bin/activate-uenv\n</code></pre> <p>Log out and back in again, then issue the following command to force bash to forget the old uenv command: <pre><code># forget the old uenv command\n$ hash -r\n\n# type and which should point to the same executable in /usr/bin\n$ type uenv\nuenv is /usr/bin/uenv\n$ which uenv\n/usr/bin/uenv\n</code></pre></p>"},{"location":"software/uenv/build/","title":"Building uenv","text":""},{"location":"software/uenv/build/#building-uenv","title":"Building uenv","text":""},{"location":"software/uenv/build/#uenv-build-service","title":"uenv build service","text":"<p>CSCS provides a build service for uenv that takes as its input a uenv recipe, and builds the uenv using the same pipeline used to build the officially supported uenv.</p> <p>The command takes two arguments:</p> <pre><code>uenv build &lt;recipe&gt; &lt;label&gt;\n</code></pre> <ul> <li><code>recipe</code>: the path to the recipe<ul> <li>A uenv recipe is a description of the software to build in the uenv.   See the stackinator documentation for more information.</li> </ul> </li> <li><code>label</code>: the label to attach, of the form <code>name/version@system%uarch</code> where:<ul> <li><code>name</code> is the name, e.g. <code>prgenv-gnu</code>, <code>gromacs</code>, <code>vistools</code>.</li> <li><code>version</code> is a version string, e.g. <code>24.11</code>, <code>v1.2</code>, <code>2025-rc2</code></li> <li><code>system</code> is the CSCS cluster to build on (e.g. <code>daint</code>, <code>santis</code>, <code>clariden</code>, <code>eiger</code>)</li> <li><code>uarch</code> is the micro-architecture.</li> </ul> </li> </ul> <p>Building a uenv</p> <p>The image will be built on <code>daint</code> for the <code>gh200</code> node type. <pre><code>uenv build $SCRATCH/recipes/myapp myapp/v3@daint%gh200\n</code></pre></p> <p>The output of the above command will print a URL that links to a status page where you can follow the progress of the build. After a successful build, the uenv can be pulled using a name from the status page:</p> <pre><code>uenv image pull service::myapp/v3:1669479716\n</code></pre> <p>Note that the image is given a unique numeric tag, provided on the status page for the build.</p> <p>Info</p> <p>To use an existing uenv recipe as the starting point for a custom recipe, <code>uenv start</code> the uenv and take the contents of the <code>meta/recipe</code> path in the mounted image (this is the recipe that was used to build the uenv).</p> <p>All uenv built by <code>uenv build</code> are pushed into the <code>service</code> namespace, where they can be accessed by all users logged in to CSCS. This makes it easy to share your uenv with other users, by giving them the name, version and tag of the image.</p> <p>Danger</p> <p>If, for whatever reason, your uenv can not be made publicly available, do not use the build service.</p> <p>Search user-built uenv</p> <p>To view all of the uenv on daint that have been built by the service: <pre><code>uenv image find service::@daint\n</code></pre></p>"},{"location":"software/uenv/build/#building-with-stackinator","title":"Building with Stackinator","text":"<p>CSCS develops and maintains the Stackinator tool that is used to configure and build uenv.</p> <p>Note</p> <p>The tool is currently maintained for internal use, and is used by automated pipelines, including the one used by the <code>uenv build</code> command. As such, CSCS provide limited support for the tool.</p>"},{"location":"software/uenv/configure/","title":"Configuration","text":""},{"location":"software/uenv/configure/#configuring-uenv","title":"Configuring uenv","text":"<p>Uenv is designed to work out of the box, with zero configuration for most users. There is support for limited per-user configuration via a configuration file, which will be expanded as we add features that make it easier for groups and communities to manage their own environments.</p>"},{"location":"software/uenv/configure/#user-configuration","title":"User configuration","text":"<p>Uenv is configured using a text configuration file.</p> <p>The location of the configuration file follows the XDG base directory specification, with the location defined as follows:</p> <ul> <li>If the <code>XDG_CONFIG_HOME</code> environment variable is set, use <code>$XDG_CONFIG_HOME/uenv/config</code>.</li> <li>Otherwise use the default location <code>$HOME/.config/uenv/config</code>.</li> </ul>"},{"location":"software/uenv/configure/#syntax","title":"Syntax","text":"<p>The configuration file uses a simple <code>key = value</code> syntax, with comments starting with <code>#</code>:</p> <pre><code># this is a comment\n\n# the following are equivalent\ncolor = true\ncolor=true\n</code></pre> <p>Notes on the syntax:</p> <ul> <li>keys are case-sensitive: <code>color</code> and <code>Color</code> are not equivalent.</li> <li>all keys are lower case</li> <li>white space is trimmed from keys and values, e.g.: <code>color = true</code> will be parsed as <code>key='color'</code> and <code>value='true'</code>.</li> </ul>"},{"location":"software/uenv/configure/#options","title":"Options","text":"key description default values <code>color</code> Use color output automatically chosen according to environment <code>true</code>, <code>false</code> <code>repo</code> The default repo location for downloaded uenv images <code>$SCRATCH/.uenv-images</code> An absolute path"},{"location":"software/uenv/configure/#color","title":"<code>color</code>","text":"<p>By default, uenv will generate color output according to the following:</p> <ul> <li>if <code>--no-color</code> is passed, color output is disabled</li> <li>else if <code>color</code> is set in the config file, use that setting</li> <li>else if the <code>NO_COLOR</code> environment variable is defined color output is disabled</li> <li>else if the terminal is not TTY disable color</li> <li>else enable color output</li> </ul> <p></p>"},{"location":"software/uenv/configure/#repo","title":"<code>repo</code>","text":"<p>The default repo location for downloaded uenv images. The repo is selected according to the following process:</p> <ul> <li>if the <code>--repo</code> CLI argument is given, use that setting</li> <li>else if <code>repo</code> is set in the config file, use that setting</li> <li>else use the default value of <code>$SCRATCH/.uenv-images</code></li> </ul>"},{"location":"software/uenv/deploy/","title":"Deploying uenv","text":""},{"location":"software/uenv/deploy/#deploying-uenv","title":"Deploying uenv","text":""},{"location":"software/uenv/deploy/#versioning-and-labeling","title":"Versioning and labeling","text":"<p>Uenv images have a label of the following form:</p> <pre><code>name/version:tag@system%uarch\n</code></pre> <p>for example:</p> <pre><code>prgenv-gnu/24.11:v2@todi%gh200\n</code></pre>"},{"location":"software/uenv/deploy/#uenv-name","title":"uenv name","text":"<p>The name of the uenv. In this case <code>prgenv-gnu</code>.</p>"},{"location":"software/uenv/deploy/#uenv-version","title":"uenv version","text":"<p>The version of the uenv.</p> <p>The format of <code>version</code> depends on the specific uenv. Often they use the <code>yy.mm</code> format, though they may also use the version of the software being packaged. For example the <code>namd/3.0</code> uenv packages version 3.0 of the popular NAMD simulation tool.</p>"},{"location":"software/uenv/deploy/#uenv-tag","title":"uenv tag","text":"<p>Used to differentiate between releases of a versioned uenv. Some examples of tags include:</p> <ul> <li><code>rc1</code>, <code>rc2</code>: release candidates;</li> <li><code>v1</code>: a first release typically made after some release candidates;</li> <li><code>v2</code>: a second release, that fixes issues in <code>v1</code></li> </ul>"},{"location":"software/uenv/deploy/#uenv-system","title":"uenv system","text":"<p>The name of the Alps cluster for which the uenv was built.</p> <p></p>"},{"location":"software/uenv/deploy/#uenv-uarch","title":"uenv uarch","text":"<p>The node type (microarchitecture) that the uenv is built for.</p> uarch CPU GPU comment gh200 4 72-core NVIDIA Grace (<code>aarch64</code>) 4 NVIDIA H100 GPUs zen2 2 64-core AMD Rome (<code>zen2</code>) - used in Eiger a100 1 64-core AMD Milan (<code>zen3</code>) 4 NVIDIA A100 GPUs mi200 1 64-core AMD Milan (<code>zen3</code>) 4 AMD Mi250x GPUs mi300 4 24-core AMD Genoa (<code>zen4</code>) 4 AMD Mi300 GPUs zen3 2 64-core AMD Milan (<code>zen3</code>) - only in MCH system"},{"location":"software/uenv/deploy/#registries","title":"Registries","text":"<p>The following naming scheme is employed in the OCI container registry for uenv images:</p> <pre><code>namespace/system/uarch/name/version:tag\n</code></pre> <p>Where the fields <code>system</code>, <code>uarch</code>, <code>name</code>, <code>version</code> and <code>tag</code> are defined above.</p> <p>The <code>namespace</code> is one of:</p> <ul> <li><code>build</code>: where the CI/CD pipeline pushes uenv images.</li> <li><code>deploy</code>: where uenv images are copied by CSCS staff when they are officially provided to users.</li> <li><code>service</code>: where the uenv build service pushes images.</li> </ul> <p>JFrog uenv registry</p> <p>The OCI container registry used to host uenv is on JFrog, and can be browsed at jfrog.svc.cscs.ch/artifactory/uenv/ (you may have to log in with CSCS credentials from the a VPN/CSCS network).</p> <p>The address of individual uenv images is of the form <pre><code>https://jfrog.svc.cscs.ch/uenv/namespace/system/uarch/name/version:tag\n</code></pre> For example: <pre><code>https://jfrog.svc.cscs.ch/uenv/deploy/eiger/zen2/cp2k:2024.1\n</code></pre></p>"},{"location":"software/uenv/deploy/#uenv-recipes-and-definitions","title":"uenv recipes and definitions","text":"<p>The uenv recipes are maintained in a public GitHub repository: eth-cscs/alps-uenv.</p> <p>The recipes for each uenv version are stored in the <code>recipes</code> subdirectory.  Specific uenv recipes are stored in <code>recipes/name/version/uarch/</code>.</p> <p>The <code>cluster</code> is specified when building and deploying the uenv, while the <code>tag</code> is specified when deploying the uenv.</p>"},{"location":"software/uenv/deploy/#uenv-deployment","title":"uenv deployment","text":""},{"location":"software/uenv/deploy/#deployment-rules","title":"Deployment rules","text":"<p>A recipe can be built for deployment on different clusters, and for multiple targets. For example:</p> <ul> <li>A multicore recipe could be built for <code>zen2</code> or <code>zen3</code> nodes</li> <li>A GROMACS recipe that is tuned for A100 GPUs can be built and deployed on any vCluster supporting the A100 architecture</li> </ul> <p>However, it is not desirable to build every recipe on every possible target system. For example:</p> <ul> <li>An ICON development environment would only be deployed on the weather and climate platform</li> <li>A GROMACS recipe would not be deployed on the weather and climate platform</li> <li>Development builds only need to run on test and staging clusters</li> </ul> <p>A YAML file <code>config.yaml</code> is maintained in the github.com/eth-cscs/alps-uenv repository that maps recipes to deployed versions on microarchitectures.</p>"},{"location":"software/uenv/deploy/#permissions","title":"Permissions","text":"<p>For CSCS staff</p> <p>This information applies only to CSCS staff.</p> <p>Deployment and deletion of uenv requires elevated permissions. Before you can modify the uenv registry, you need to set up credentials.</p> <ul> <li>Your CSCS username needs to be added to the <code>uenv-admin</code> group on JFrog, and</li> <li>you need to generate a new token for the JFrog registry.</li> </ul> <p>Once you have the token, you can save it in a file.</p> <p>Danger</p> <p>Save the token file somewhere safe, for example in <code>~/.ssh/jfrog-token</code>.</p> <p>The token file can be passed to the <code>uenv</code> command line tool using the <code>--token</code> option.</p> <pre><code>uenv image copy --token=${HOME}/.ssh/jfrog-token &lt;SOURCE&gt; &lt;DESTINATION&gt;\n</code></pre>"},{"location":"software/uenv/deploy/#deploying-a-uenv","title":"Deploying a uenv","text":"<p>For CSCS staff</p> <p>This information applies only to CSCS staff.</p> <p>The CI/CD pipeline for eth-cscs/alps-uenv pushes images to the JFrog uenv registry in the <code>build::</code> namespace.</p> <p>Deploying a uenv copies the uenv image from the <code>build::</code> namespace to the <code>deploy::</code> namespace. The Squashfs image itself is not copied; a new tag for the uenv is created in the <code>deploy::</code> namespace.</p> <p>The deployment is performed using the <code>uenv</code> command line tool, as demonstrated below:</p> <pre><code>uenv image copy build::&lt;SOURCE&gt; deploy::&lt;DESTINATION&gt; # (1)!\n</code></pre> <ol> <li><code>&lt;DESTINATION&gt;</code> must be fully qualified.</li> </ol> <p>Deploy using image ID</p> <p>Deploy a uenv from <code>build::</code> using the ID of the image:</p> <pre><code>uenv image copy build::d2afc254383cef20 deploy::prgenv-nvfortran/24.11:v1@daint%gh200\n</code></pre> <p>Deploy using qualified name</p> <p>Deploy a uenv using the qualified name:</p> <pre><code>uenv image copy build::quantumespresso/v7.4:1653244229 deploy::quantumespresso/v7.4:v1@daint%gh200\n</code></pre> <p>Note</p> <p>The build image uses the CI/CD pipeline ID as the tag. You will need to choose an appropriate tag.</p> <p>Deploy a uenv from one cluster to another</p> <p>You can also deploy a uenv from one vCluster to another. For example, if the <code>uenv</code> for <code>prgenv-gnu</code> has been deployed on <code>daint</code>, to make it available on <code>santis</code>, you can use the following command:</p> <pre><code>uenv image copy deploy::prgenv-gnu/24.11:v1@daint%gh200 deploy::prgenv-gny/24.11@santis%gh200\n</code></pre> <p></p>"},{"location":"software/uenv/deploy/#removing-a-uenv","title":"Removing a uenv","text":"<p>To remove a uenv, you can use the <code>uenv</code> command line tool:</p> <pre><code>uenv image delete --token=${HOME}/.ssh/jfrog-token &lt;NAMESPACE&gt;::&lt;IMAGE&gt;\n</code></pre> <p>Danger</p> <p>Removing a uenv that has been deployed (i.e. is in the <code>deploy::</code> namespace) is potentially disruptive for users. Please see the uenv life cycle guide for more information.</p>"},{"location":"software/uenv/deploy/#source-code-access","title":"Source code access","text":"<p>Some source artifacts are stored in JFrog, including:</p> <ul> <li>source code for software that can\u2019t be downloaded directly from the internet directly;</li> <li>and tar balls for custom software.</li> </ul> <p>These artifacts are stored in a JFrog \u201cgeneric repository\u201d uenv-sources.</p> <p>Each software package has a sub-directory and all image paths are lower case (e.g. <code>uenv-sources/namd</code>).</p> <p>By default, all packages in uenv-sources  are anonymous read access to enable users to build uenv on vClusters without configuring access tokens. However,</p> <ul> <li>access to some packages is restricted by applying access rules to the package path</li> <li>e.g. access to <code>uenv-sources/vasp</code> is restricted to members of the vasp6 group</li> </ul> <p>Permissions to access restricted resources is set on a per-pipeline basis</p> <ul> <li>For example, only the <code>alps-uenv</code> pipeline has access to the VASP source code, while the <code>uenv build</code> pipeline does not.</li> </ul> Package Access Path Notes Contact <code>cray-mpich</code> anonymous <code>uenv-sources/cray-mpich</code> <code>cray-mpich</code>, <code>cray-gtl</code>, <code>cray-pals</code>, <code>cray-mpi</code> Simon Pintarelli, Benjamin Cumming <code>namd</code> <code>uenv-sources-csstaff</code> <code>uenv-sources/namd</code> NAMD requires an account to download the source code Rocco Meli <code>vasp</code> <code>vasp6</code>, <code>cscs-uenv-admin</code> <code>uenv-sources/vasp</code> VASP requires a paid license to access source Simon Frasch <code>vmd</code> <code>uenv-sources-csstaff</code> <code>uenv-sources/vmd</code> VMD requires an account to download the source code Alberto Invernizzi <p></p>"},{"location":"software/uenv/deploy/#uenv-life-cycle","title":"uenv life cycle","text":"<p>Scientific applications and tools have different release cycles (e.g. annual, quarterly or irregular), and the communities that use them have different expectations (e.g. ML users expect tools released in the last 3 months, while some scientific communities value support for old versions of software). For this reason, there is not a universal release and deprecation schedule for supported applications and programming environments - how often to release, how long to provide support, and when to remove old versions is up to the maintainer of the uenv, based on their user\u2019s requirements and the overheads of maintaining old versions.</p> <p>While the frequency of updates and deprecation policy is defined by the uenv maintainer, the following release cycle with tag names should be followed when possible:</p> <ul> <li>[optional] tag pre-releases with <code>:rc1</code>, <code>:rc2</code>, etc<ul> <li>uenv tagged as release candidates are intended for early testing only, and can be removed at any point.</li> </ul> </li> <li>Tag <code>:v1</code> for the first official release.</li> <li>[optional] release updated versions of uenv with tags (<code>:v2</code>, <code>:v3</code>, \u2026) with patches and fixes.</li> <li>Remove old versions according to a deprecation policy.</li> </ul> <p>A uenv\u2019s release and support policy is part of the uenv\u2019s documentation under the \u201cVersioning\u201d section, see prgenv-gnu for example.</p> <p>The versioning section should provide the following information:</p> <ul> <li>the release schedule;</li> <li>the support, update and deprecation policy for the uenv;</li> <li>details about the currently deployed versions, changelogs and relevant information how to upgrade if appropriate.</li> </ul> Example deprecation policy <p>A scientific application <code>sciapp</code> is released annually by the developers, and projects using the software on CSCS systems expect support for two years.</p> <ul> <li>Release a new version annually and version with the year, for example <code>sciapp/2025</code> being the most recent version in 2025.</li> <li>CSCS provides full support for the current release, and partial support for old releases:<ul> <li><code>sciapp/2025</code>: provide updates and fixes with new tags, take feature requests, and respond to questions about usage and how to upgrade to this version.</li> <li><code>sciapp/2024</code>: only support the most recent tag and only create new tags to fix breaking changes.</li> <li><code>sciapp/2023</code>: provided \u201cas is\u201d for the first quarter of 2025 while users have the chance to upgrade.</li> </ul> </li> <li>If there is a major breaking change to the system that would require a large effort to continue providing last year\u2019s version, and would require users to also update their workflow, we might choose to encourage deprecate early and help users upgrade to the latest version.</li> </ul> <p>Generally speaking, uenv are removed (deleted from the <code>deploy::</code> namespace) under the following circumstances:</p> <ul> <li>when they are no longer supported as per the deprecation policy of the uenv;</li> <li>when release candidates are superceded by a new release candidate or an official <code>:v1</code> release;</li> <li>when a new tag of a <code>name/version</code> is deployed to replace a \u201cbroken\u201d uenv:<ul> <li>there might have been a critical issue affecting security, correctness, or performance that means all users should update.</li> <li>do not remove uenv tags that continue to meet the needs of users (e.g. if a new tag was introduced to fix an issue that only affected specific users or use cases).</li> </ul> </li> </ul>"},{"location":"software/uenv/release-notes/","title":"Release notes","text":""},{"location":"software/uenv/release-notes/#uenv-releases-notes","title":"uenv releases notes","text":"<p>The latest version of uenv deployed on Alps clusters is v8.1.0. You can check the version available on a specific system with the <code>uenv --version</code> command.</p> <p></p>"},{"location":"software/uenv/release-notes/#v900","title":"v9.0.0","text":"<p>This version will replace v8.1.0 on Alps clusters.</p>"},{"location":"software/uenv/release-notes/#features","title":"Features","text":"<ul> <li>elastic logging.</li> <li>Add <code>--json</code> option to <code>image ls</code> and <code>image find</code>.</li> <li>add <code>--format</code> flag to uenv status.</li> </ul>"},{"location":"software/uenv/release-notes/#improvements","title":"Improvements","text":"<ul> <li>force unsquashfs to use a single thread when unpacking meta data.</li> <li>reimplement squashfs-mount in the main repository.</li> <li>improve file name completion in bash.</li> </ul>"},{"location":"software/uenv/release-notes/#fixes","title":"Fixes","text":"<ul> <li>Turn some CLI flags into options, so that they can be set with or without <code>=</code>. e.g. <code>uenv --repo=$HOME/uenv</code> or <code>uenv --repo $HOME/uenv</code>.</li> <li>Only use the meta data path adjacent to a uenv image if it contains an env.json file.</li> <li><code>image push</code> was not pushing the correct meta data path.</li> <li>a bug where the <code>--only-meta</code> flag was ignored on <code>image pull</code>.</li> <li>add hints to error message when uenv is not found.</li> </ul>"},{"location":"software/uenv/release-notes/#known-issues","title":"Known issues","text":"<p>user-installed uenv stopped working</p> <p>This version introduced changes to the <code>squashfs-mount</code> tool used by <code>uenv start</code> and <code>uenv -run</code> that are incompatible with older versions of uenv. If you see errors that contain <code>error: unable to exec '...': No such file or directory (errno=2)</code>, follow the guide for uninstalling user-installed uenv.</p> <p>bash: module: command not found</p> <p>This is a known issue with version 9.0.0 that will be fixed in 9.0.1. See the uenv modules docs for a workaround.</p> <p></p>"},{"location":"software/uenv/release-notes/#v810","title":"v8.1.0","text":"<p>This version replaced v7.1.0 on Alps clusters.</p>"},{"location":"software/uenv/release-notes/#features_1","title":"Features","text":"<ul> <li>improved uenv view management</li> <li>automatic generation of default uenv repository the first time uenv is called<ul> <li>this fixes the error message</li> </ul> </li> <li>bash completion</li> <li>support for configuration files<ul> <li>currently only support setting <code>color</code> and default uenv repo</li> </ul> </li> <li>support for <code>SLURM_UENV</code> and <code>SLURM_UENV_VIEW</code> environment variables for use inside CI/CD pipelines.</li> </ul>"},{"location":"software/uenv/release-notes/#small-fixes","title":"Small fixes","text":"<ul> <li>better error messages and small bug fixes</li> <li>relative paths can be used for referring to squashfs images</li> </ul>"},{"location":"storage/","title":"Storage","text":"<ul> <li> <p> File Systems</p> <p>Learn about the filesystems on Alps</p> <p> File Systems</p> </li> <li> <p> Data Transfer</p> <p>Moving data into and out of CSCS, and between CSCS systems.</p> <p> Data Transfer</p> </li> <li> <p> Long Term Storage</p> <p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier.</p> <p> LTS</p> </li> <li> <p> Object Storage</p> <p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway.</p> <p> Object Storage</p> </li> </ul>"},{"location":"storage/filesystems/","title":"File Systems","text":""},{"location":"storage/filesystems/#file-systems","title":"File Systems","text":"<p>Note</p> <p>The different file systems provided on the Alps platforms and policies like quotas and backups are documented here. The file systems available on a cluster and policy details are determined by the cluster\u2019s platform. Please read the documentation for the clusters that you are working on after reviewing this documentation.</p> <ul> <li> <p> File Systems</p> <p>There are three types of file system that are provided on Alps clusters:</p> backups snapshot cleanup access Home yes yes no user Scratch no no yes user Store yes no no project </li> </ul> <ul> <li> <p> Backups</p> <p>There are two forms of data backup that are provided on some file systems.</p> <p> Backups</p> <p> Snapshots</p> </li> <li> <p> Cleanup</p> <p>Data retention policies and automatic cleanup of Scratch.</p> <p> Cleanup policies</p> </li> <li> <p> Quota</p> <p>Find out about quota on capacity and file counts, and how to check your quota limits.</p> <p> Quota</p> </li> <li> <p> Troubleshooting</p> <p>Answers to common issues and questions.</p> <p> common questions</p> </li> </ul> <p></p>"},{"location":"storage/filesystems/#home","title":"Home","text":"<p>The Home file system is mounted on every cluster, and is referenced by the environment variable <code>$HOME</code>. It is a relatively small storage for files such as source code or shell scripts and configuration files, provided on the VAST file system.</p> <p>Home on Daint</p> <p>The Home path for the user <code>$USER</code> is mounted at <code>/users/$USER</code>. For example, the user <code>bcumming</code> on Daint: <pre><code>$ ssh daint.alps.cscs.ch\n$ echo $HOME\n/users/bcumming\n</code></pre></p>"},{"location":"storage/filesystems/#cleanup-and-expiration","title":"Cleanup and expiration","text":"<p>There is no cleanup policy on Home, and the contents are retained for three months after your last project finishes.</p>"},{"location":"storage/filesystems/#quota","title":"Quota","text":"<p>All users get a quota of 50 GB and 500,000 inodes in Home.</p>"},{"location":"storage/filesystems/#backups","title":"Backups","text":"<p>Daily snapshots for the last seven days are provided in the hidden directory <code>$HOME/.snapshot</code>.</p> <p>Backup is not yet available on Home</p> <p>Backups to tape storage are currently being implemented for Home directories.</p> <p></p>"},{"location":"storage/filesystems/#scratch","title":"Scratch","text":"<p>The Scratch file system is a fast workspace tuned for use by parallel jobs, with an emphasis on performance over reliability, hosted on the Capstor Lustre filesystem. See the Lustre guide for some hints on how to get the best performance out of the filesystem.</p> <p>All users on Alps get their own Scratch path, <code>/capstor/scratch/cscs/$USER</code>, which is pointed to by the variable <code>$SCRATCH</code> on the HPC Platform and Climate and Weather Platform clusters Eiger, Daint and Santis.</p> <p><code>$SCRATCH</code> on MLP points to Iopsstor</p> <p>On the machine learning platform (MLP) systems clariden and bristen the <code>$SCRATCH</code> variable points to storage on Iopsstor. See the MLP docs for more information.</p>"},{"location":"storage/filesystems/#cleanup-and-expiration_1","title":"Cleanup and expiration","text":"<p>The cleanup policy is enforced on Scratch, to ensure continued performance of the file system.</p> <ul> <li>Files not accessed in the last 30 days are automatically deleted.</li> <li>When capacity grows above:<ul> <li>60%: users are asked to start removing or archiving unneeded files</li> <li>80%: CSCS will start removing files and paths without further notice.</li> </ul> </li> </ul>"},{"location":"storage/filesystems/#quota_1","title":"Quota","text":"<p>A soft quota is enforced on the Scratch file system, with a grace period to allow data transfer.</p> <p>Every user gets the following quota:</p> <ul> <li>150 TB of disk space;</li> <li>1 million inodes;</li> <li>and a soft quota grace period of two weeks.</li> </ul> <p>Important</p> <p>In order to prevent a degradation of the file system performance, please check your disk space and inode usage with the command <code>quota</code>. Even if you are not close to the quota, please endeavor to reduce usage wherever possible to improve user experience for everybody on the system.</p>"},{"location":"storage/filesystems/#backups_1","title":"Backups","text":"<p>There are no backups on Scratch. Please ensure that you move important data to a file system with backups, for example Store.</p> <p></p>"},{"location":"storage/filesystems/#store","title":"Store","text":"<p>Store is a large, medium-performance, storage on the Capstor Lustre file system for sharing data within a project, and for medium term data storage. See the Lustre guide for some hints on how to get the best performance out of the filesystem.</p> <p>Space on Store is allocated per-project, with a path created for each project. To accomodate the different customers and projects on Alps, the project paths are organised as follows:</p> <pre><code>/capstor/store/&lt;tenant&gt;/&lt;customer&gt;/&lt;group_id&gt;\n</code></pre> <ul> <li><code>tenant</code>: there are currently two tenants, <code>cscs</code> and <code>mch</code>:<ul> <li>the vast majority of projects are hosted by the <code>cscs</code> tenant.</li> </ul> </li> <li><code>customer</code>: refers to the contractual partner responsible for the project.    Examples of customers include:<ul> <li><code>userlab</code>: projects allocated in the CSCS User Lab through open calls. The majority of projects are hosted here, particularly on the HPC platform.</li> <li><code>swissai</code>: most projects allocated on the Machine Learning Platform.</li> <li><code>2go</code>: projects allocated under the CSCS2GO scheme.</li> </ul> </li> <li><code>group_id</code>: refers to the linux group created for the project.</li> </ul> Which groups and projects am I a member of? <p>Users often are part of multiple projects, and by extension their associated <code>groupd_id</code> groups. You can get a list of your groups using the <code>id</code> command in the terminal: <pre><code>$ id $USER\nuid=12345(bobsmith) gid=32819(g152) groups=32819(g152),33119(g174),32336(vasp6)\n</code></pre> Here the user <code>bobsmith</code> is in three projects (<code>g152</code>, <code>g174</code> and <code>vasp6</code>), with the project <code>g152</code> being their primary project.  In the terminal, use the following command to find your primary group: <pre><code>$ id -gn $USER\ng152\n</code></pre></p> <p>The <code>$STORE</code> environment variable</p> <p>On some clusters, for example, Eiger and Daint, the project folder for your primary project can be accessed using the <code>$STORE</code> environment variable.</p> <p>Avoid using Store for jobs</p> <p>Store is tuned for storing results and shared datasets, specifically it has fewer meta data servers assigned to it.</p> <p>Use the Scratch file systems, which are tuned for fast parallel I/O, for storing input and output for jobs.</p>"},{"location":"storage/filesystems/#cleanup-and-expiration_2","title":"Cleanup and expiration","text":"<p>There is no cleanup policy on Store, and the contents are retained for three months after the project ends.</p>"},{"location":"storage/filesystems/#quota_2","title":"Quota","text":"<p>Paths on Store is allocated per-project: a path is created for each project with a quota based on the initial resource request. Users have read and write access to the Store paths for each project that they are a member of, and you can check the quota on Store for all of your projects using the <code>quota</code> tool.</p>"},{"location":"storage/filesystems/#backups_2","title":"Backups","text":"<p>Backups are performed on Store, with the three most recent copies of every file backed up to tape every 24 hours.</p> <p></p>"},{"location":"storage/filesystems/#quota_3","title":"Quota","text":"<p>Storage quota is a limit on available storage applied to:</p> <ul> <li>capacity: the total size of files;</li> <li>and inodes: the total number of files and directories.</li> </ul> What is an inode? <p>inodes are data structures that describe Linux file system objects like files and directories - every file and directory has a corresponding inode.</p> <p>Large inode counts degrade file system performance in multiple ways. For example, Lustre file systems have separate metadata and data management. Excessive inode usage can overwhelm the metadata services, causing degradation across the file system.</p> <p>Consider compressing paths to reduce inode usage</p> <p>Consider archiving folders that you are not actively using with the tar command to reduce used capacity and the number of inodes.</p> <p>Consider compressing directories full of many small input files as SquashFS images (see the following example of generating SquashFS images for an example) - which pack many files into a single file that can be mounted to access the contents efficiently.</p> <p>Update file timestamps when unpacking tar files</p> <p>The default behavior of the <code>tar</code> command is to retain the access date of the original file when unpacking tar balls. When unpacking on a file system with cleanup policy, use the <code>--touch</code> flag with <code>tar</code> to ensure that the files won\u2019t be cleaned up prematurely. For example: <pre><code>$ tar --touch -xvf data_archive.tgz\n</code></pre></p> <p>There are two types of quota:</p> <p></p> <ul> <li>Soft quota when exceeded there is a grace period for transferring or deleting files, before it will become a hard quota.</li> <li>Hard quota when exceeded no more files can be written.</li> </ul> <p>Todo</p> <p>Storage team: can you please provide better/more complete definitions of the hard and soft quotas.</p> <p></p>"},{"location":"storage/filesystems/#checking-quota","title":"Checking quota","text":"<p>You can check your storage quotas with the command <code>quota</code> on the front-end system Ela (<code>ela.cscs.ch</code>) and the login nodes of Daint, Santis, Clariden and Eiger.</p> <p>The tool shows available capacity and used capacity for each file system that you have access to. If you are in multiple projects, information for the Store path for each project that you are a member of will be shown.</p> Checking your quota on Ela <pre><code>$ ssh user@ela.cscs.ch\n$ quota\nRetrieving data ...\n\nUser: user\nUsage data updated on: 2025-05-21 11:10:02\n+------------------------------------+--------+--------+------+---------+--------+------+-------------+----------+------+----------+-----------+------+-------------+\n|                                             |        User quota       |          Proj quota         |         User files         |    Proj files    |             |\n+------------------------------------+--------+--------+------+---------+--------+------+-------------+----------+------+----------+-----------+------+-------------+\n| Directory                          | FS     |   Used |    % |   Grace |   Used |    % | Quota limit |     Used |    % |    Grace |      Used |    % | Files limit |\n+------------------------------------+--------+--------+------+---------+--------+------+-------------+----------+------+----------+-----------+------+-------------+\n| /iopsstor/scratch/cscs/user        | lustre |  32.0G |    - |       - |      - |    - |           - |     7746 |    - |        - |         - |    - |           - |\n| /capstor/users/cscs/user           | lustre |   3.2G |  6.4 |       - |      - |    - |       50.0G |    14471 |  2.9 |        - |         - |    - |      500000 |\n| /capstor/store/cscs/director2/g33  | lustre |   1.9T |  1.3 |       - |      - |    - |      150.0T |   146254 | 14.6 |        - |         - |    - |     1000000 |\n| /capstor/store/cscs/cscs/csstaff   | 263.9T | 88.0 |      - |    - |      300.0T | 18216778 | 91.1 |         - |    - |    20000000 |\n| /capstor/scratch/cscs/user         | lustre | 243.0G |  0.2 |       - |      - |    - |      150.0T |   336479 | 33.6 |        - |         - |    - |     1000000 |\n| /vast/users/cscs/user              | vast   |  11.7G | 23.3 | Unknown |      - |    - |       50.0G |    85014 | 17.0 |  Unknown |         - |    - |      500000 |\n+------------------------------------+--------+--------+------+---------+--------+------+-------------+----------+------+----------+-----------+------+-------------+\n</code></pre> <p>Here the user is in two projects, namely <code>g33</code> and <code>csstaff</code>, for which the quota for their respective paths in <code>/capstor/store</code> are reported. Note that the path <code>/vast/users/cscs/user</code> is mounted at <code>/users/user</code> (i.e. <code>$HOME</code>) on Alps.</p> <p></p>"},{"location":"storage/filesystems/#backup","title":"Backup","text":"<p>There are two methods for retaining backup copies of data on CSCS file systems, namely backups and snapshots.</p> <p></p>"},{"location":"storage/filesystems/#backups_3","title":"Backups","text":"<p>Backups store copies of files on slow, high-capacity, tape storage. The backup process checks for modified or new files every 24 hours, and makes a copy on tape of every new or modified file.</p> <ul> <li>up to three copies of a file are stored (the three most recent copies).</li> </ul> <p>How do I restore from a backup?</p> <p>Open a service desk ticket with request type \u201cStorage and File systems\u201d to restore a file or directory.</p> <p>Please provide the following information in the request:</p> <ul> <li>the full path to restore, e.g.:<ul> <li>a file: <code>/capstor/scratch/cscs/userbob/software/data/results.tar.gz</code></li> <li>or a directory: <code>/capstor/scratch/cscs/userbob/software/data</code>.</li> </ul> </li> <li>the date to restore from:<ul> <li>the most recent backup older than the date will be used.</li> </ul> </li> </ul> <p></p>"},{"location":"storage/filesystems/#snapshots","title":"Snapshots","text":"<p>A snapshot is a full copy of a file system at a certain point in time, that can be accessed via a special hidden directory.</p> <p>Where are snapshots available?</p> <p>Currently, only the Home file system provides snapshots, with snapshots of the last 7 days available in the path <code>$HOME/.snapshot</code>.</p> Accessing snapshots on Home <p>The snapshots for Home are in the hidden <code>.snapshot</code> path in Home (the path is not visible even to <code>ls -a</code>) <pre><code>$ ls $HOME/.snapshot\nbig_catalog_2025-05-21_08_49_34_UTC\nbig_catalog_2025-05-21_09_19_34_UTC\nusers_2025-05-14_22_59_00_UTC\nusers_2025-05-15_22_59_00_UTC\nusers_2025-05-16_22_59_00_UTC\nusers_2025-05-17_22_59_00_UTC\nusers_2025-05-18_22_59_00_UTC\nusers_2025-05-19_22_59_00_UTC\nusers_2025-05-20_22_59_00_UTC\n</code></pre></p> <p></p>"},{"location":"storage/filesystems/#cleanup-policies","title":"Cleanup policies","text":"<p>The performance of Lustre file systems is affected by file system occupancy and the number of files. Ideally occupancy should not exceed 60%, with severe performance degradation for all users when occupancy exceeds 80% and when there are too many small files.</p> <p>File cleanup removes files that are not being used to ensure that occupancy and file counts do not affect file system performance.</p> <p>A daily process removes files that have not been accessed (either read or written) in the last 30 days.</p> How can I tell when a file was last accessed? <p>The access time of a file can be found using the <code>stat</code> command. For example, to get the access time of the file <code>./src/affinity.h</code>:</p> <pre><code>$ stat -c %x ./src/affinity.h\n2025-05-23 16:27:40.580767016 +0200\n</code></pre> <p>Do not artificially update the access time of files</p> <p>It is not allowed to automatically or artificially update the access time of files to avoid the cleanup policy, and CSCS scans for these activities.</p> <p>Please move data to a file system that is suitable for persistent storage instead.</p> <p>In addition to the automatic deletion of old files, if occupancy exceeds 60% the following steps are taken to maintain performance of the file system:</p> <ul> <li>Occupancy \u2265 60%: CSCS will ask users to take immediate action to remove unnecessary data.</li> <li>Occupancy \u2265 80%: CSCS will start manually removing files and folders without further notice.</li> </ul> <p>How do I ensure that important data is not cleaned up?</p> <p>File systems with cleanup, namely Scratch, are not intended for long term storage. Copy the data to a file system designed for file storage that does not have a cleanup policy, for example Store.</p> <p></p>"},{"location":"storage/filesystems/#frequently-asked-questions","title":"Frequently asked questions","text":"My files are gone, but the directories are still there <p>When the cleanup policy is applied on Lustre file systems, the files are removed, but the directories remain.</p> What do messages like <code>mkdir: cannot create directory 'test': Disk quota exceeded</code> mean? <p>You have run out of quota on the target file system. Consider deleting unneeded files, or moving data to a different file system. Specifically, if you see this message when using Home, which has a relatively small 50 GB limit, consider moving the data to your project\u2019s Store path.</p> <p>Todo</p> <p>FAQ question: writing with specific group access</p>"},{"location":"storage/longterm/","title":"Long Term Storage (LTS)","text":"<p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier. The current implementation of the LTS service addresses the first two principles of the FAIR quadrant: findable and accessible.</p> <ul> <li> Findable Data and supplementary materials have sufficiently rich metadata anda  unique and persistent identifier.</li> <li> Accessible Metadata and data are understandable to humans and machines. Data is deposited in a trusted repository.</li> <li> Interoperable Metadata use a formal, accessible, shared, and broadly applicable language for knowledge representation.</li> <li> Reusable Data and collections have a clear usage license and provide accurate information on provenance.</li> </ul> <p>These are the main features of the service:</p> <ul> <li>Storage repository with long term retention capabilities (10 years);</li> <li>Provide persistent identifiers;</li> <li>Ability to set public access to data when needed;</li> <li>Data stored in LTS easily accessible from a web browser (HTTP protocol);</li> <li>RESTful API to integrate with third party applications/portals;</li> <li>Scalable service that can cope with large volumes of data;</li> <li>Resiliency due to data protection measures against hardware/software failures;</li> <li>Clear licensing of the data.</li> </ul>"},{"location":"storage/longterm/#service-description","title":"Service Description","text":"<p>The main unit of the LTS workflow is the data collection. A data collection is a group of data files enriched with a set of metadata attributes and a persistent ID referencing the entire collection. In other contexts such an entity might be called dataset, data aggregate or data block.</p> <p>The data files are store in the CSCS Object Store thus the data collection and its associated PID handle (the specific type of persistent ID used by LTS) will contain a list of URLs. There is no need to have special clients to access the data URLs or the PID handle, standard HTTP client like a browser or the <code>curl</code> command are sufficient.</p> <p>The PID handles use for the LTS service are the one provided by the CSCS PID service. This enables the LTS service to guarantee the consistency between data collection, object store data and PID handle.</p>"},{"location":"storage/longterm/#pricing","title":"Pricing","text":"<p>As of 2021:</p> <ul> <li>Users from the free User Lab program are entitled to use 2 TB of LTS storage quota (for 10 years) free of charge per project</li> <li>Currently additional space can be purchased for CHF 600.- for each terabyte (for 10 years)</li> </ul>"},{"location":"storage/longterm/#prerequisites","title":"Prerequisites","text":"<p>In order to create collections and upload files into the LTS service, a user needs the following prerequisites:</p> <ul> <li>CSCS project with a quota on the LTS (and/or LTS-TDS) facility</li> <li>HTTP client: for example <code>curl</code> or Python requests.</li> <li>Keycloak registered client (only to access the service via RESTful API, not needed when using the web portal)</li> <li> <p>Outgoing connectivity to the following services:</p> Service URL Description LTS Prod https://lts.cscs.ch Production LTS service LTS TDS https://lts-tds.cscs.ch Test LTS service Keycloak https://auth.cscs.ch Authentication service Object Store https://object.cscs.ch Object Store service PID https://hdl.handle.net PID service </li> </ul> <p>In order to download files from the LTS service, a user needs a web browser or any other HTTP client like <code>curl</code>.</p>"},{"location":"storage/longterm/#creating-collections-and-uploading-files","title":"Creating collections and uploading files","text":"<p>LTS can be accessed in two ways:</p> <ul> <li>the web portal available at lts.cscs.ch;</li> <li>the LTS RESTful API, whose endpoints are described by the online documentation at lts.cscs.ch/api</li> </ul>"},{"location":"storage/longterm/#authentication","title":"Authentication","text":"<p>The LTS authenticates users based on the CSCS authentication service, therefore a user needs a valid CSCS account in order to access the service. The LTS web portal will guide the user through the authentication process; in order to access LTS through the RESTful API, the user will need to create a Keycloak token first.</p>"},{"location":"storage/longterm/#authorization","title":"Authorization","text":"<p>LTS data collections belong to a CSCS project: data can be stored in the LTS service after the principal investigator of the project has granted a project member the permissions to store data on behalf of the project. This is done by enabling the LTS facility for the user on the CSCS Account and Resources Management Tool.</p>"},{"location":"storage/longterm/#data-collection-creation-workflow","title":"Data collection creation Workflow","text":"<p>The typical LTS workflow will involve several steps, as described below. During the process, the data collection will go through the following states:</p> <pre><code>graph LR\n  A[NEW] --&gt; B[COMMITTED]\n  B --&gt; C[VALIDATED]\n  C --&gt; D[HANDLE ASSIGNED]\n  D --&gt; E[COMPLETED]</code></pre>"},{"location":"storage/longterm/#data-collection-definition","title":"Data collection definition","text":"<p>The workflow begins with the creation of a data collection. A minimum set of attributes is necessary to create the data collection; the most important ones are the following:</p> <ul> <li>name of the collection</li> <li>brief description</li> <li>project owning the data</li> <li>list of metadata attributes</li> <li>list of data files</li> <li>data files license</li> </ul> <p>The initial state of a data collection is <code>NEW</code>: the list of attributes and data files can be specified both at creation time or also added afterwards. The list of data files must contain the filename and the corresponding md5 checksum.</p> <p>Currently all the LTS data collections are considered containing public data: in future, the user will be able to specify whether the data collection is going to be public or private.</p> <p>If the data files are covered by copyright the user have to select an appropriate license. The default LTS license is CC BY 4.0.</p>"},{"location":"storage/longterm/#data-collection-inspection","title":"Data collection inspection","text":"<p>The collection can be inspected immediately after its creation. This is useful to check the attributes/objects already included in its definition, the object checksums, the overall state and the state of the single objects and their temporary URLs.</p>"},{"location":"storage/longterm/#data-collection-update-and-data-upload","title":"Data collection update and data upload","text":"<p>After the collection has been created, it can be modified during a transient phase: meanwhile, the collection state is <code>NEW</code> and the user can do the following:</p> <ul> <li>add additional data files</li> <li>add additional metadata attributes</li> <li>update already defined data files/attributes</li> <li>delete already defined data files/attributes</li> <li>upload the data files</li> </ul> <p>The LTS service generates a set of object store temporary URLs, one for each data file, which will be used to upload them to the object store. LTS is not involved in the upload operation, the data path comes from the user machine to the CSCS Object Store servers.</p>"},{"location":"storage/longterm/#licensing","title":"Licensing","text":"<p>When you prepare a data collection you must specify the license under which you publish your data. The default is the Creative Commons CC BY 4.0 license, but you can choose other ones. This Licensing guide provides information about the data licenses available in LTS.</p> <p>Todo</p> <p>the link to the licensing guide on Confluence KB was broken.</p>"},{"location":"storage/longterm/#commit","title":"Commit","text":"<p>At the end of the creation/update/upload phase, the user has to declare that the definition of the collection is complete. This is done by the user, who sends a collection commit request: after the commit request, the collection state changes from <code>NEW</code> to <code>COMMITTED</code> and from that point on the data collection definition cannot be modified anymore.</p>"},{"location":"storage/longterm/#validation","title":"Validation","text":"<p>Once the LTS service has received the commit request, it starts performing the validation operations needed to assess whether the files uploaded are consistent with the the checksums defined in the collection. If a mismatch is found, then the collection gets a <code>FAILED</code> status and the issue is reported back to the user. If all files are found in the object store and they have the correct checksum, the collection state is set to <code>VALIDATED</code>. At the end of the validation process, LTS sets the Object Store container ACLs in order to make the container world readable.</p>"},{"location":"storage/longterm/#handle-assignment","title":"Handle assignment","text":"<p>When the collection has entered the <code>VALIDATED</code> state, it\u2019s time for the LTS service to talk with the PID service and ask for a handle. The handle is attached to the collection and at that point the creation workflow of the collection is complete and the collection enter in the state <code>HANDLE_ASSIGNED</code> and the state COMPLETED shortly after that.</p>"},{"location":"storage/longterm/#dealing-with-failures","title":"Dealing with Failures","text":"<p>After the collection is committed by the user, LTS performs a series of checks on the uploaded data and requests an handle to the ePIC handle service. If any error occurs during this phase the collection state will be <code>FAILED</code>. Possible reasons for this state are:</p> <ul> <li>a failure in one of the LTS microservices</li> <li>a failure in one of the underneath services (object store, handle server, database server etc ..)</li> <li>one or more data files were not uploaded</li> <li>one or more data file checksums are wrong</li> </ul> <p>When in state FAILED the collection is still editable. The user will have to review the content of the collection, fix the checksum or re-upload files if he/she spots anything wrong or missing. Then the collection needs to be saved and the validation retried. The validation process can be retried clicking on \u201cRetry Data Collection\u201d from the web portal or through the <code>commit</code> endpoint if using the RESTful API.</p>"},{"location":"storage/longterm/#downloading-data-from-lts","title":"Downloading data from LTS","text":"<p>The data is publicly accessible via the HTTP protocol once the status of the collection has entered the <code>COMPLETED</code> step. Its URL can be found under the \u201cHandle\u201d attribute of the data collection.</p>"},{"location":"storage/longterm/#further-documentation","title":"Further Documentation","text":"<p>Todo</p> <p>All of the links in this section (except the video) on the Confluence KB were broken.</p> <p>The Long Term Storage webinar, held in June 2021, provides an description of the Long Term Storage service use case, its architecture and workflow, and a demonstration of the Web Portal.</p> <p>The RESTful API HowTo provides some usage examples for the LTS API with curl and Python.</p> <p>The Web portal: create a data collection page provides usage examples for the creation of a data collection in the LTS web portal.</p> <p>The Web portal: define and upload objects page provides usage examples for definition and the upload of data collection objects in the LTS web portal.</p> <p>The Licensing guide provides information about the data licenses available in LTS.</p> <p>The Landing page page provides information about the LTS landing page.</p>"},{"location":"storage/object/","title":"Object Storage","text":"<p>Note</p> <p>This page is currently incomplete and it is being updated following recent developments.</p>"},{"location":"storage/object/#s3","title":"S3","text":"<p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway. The service can be accessed from S3-compatible clients.</p>"},{"location":"storage/object/#general-information","title":"General Information","text":"<ul> <li>Endpoint: https://rgw.cscs.ch</li> <li>URL: path-style in the format <code>https://rgw.cscs.ch/%(bucket)s/key-name</code></li> <li>Publicly accessible object links: <code>https://rgw.cscs.ch/&lt;tenant&gt;:&lt;bucket-name&gt;/key-name</code><ul> <li>after setting proper bucket policy</li> </ul> </li> </ul>"},{"location":"storage/object/#usage-examples","title":"Usage Examples","text":""},{"location":"storage/object/#aws-cli","title":"AWS CLI","text":"<p>XAmzContentSHA256Mismatch error</p> <p>Due to an incompatibility between newer versions of the AWS CLI and Ceph <code>18.2.4</code>, you may see an error like <code>XAmzContentSHA256Mismatch</code>. </p> <p>This can be worked around by either pinning the AWS CLI to an older version (<code>&lt; 2.23.0</code>), or exporting the following environment variables:</p> <pre><code>export AWS_REQUEST_CHECKSUM_CALCULATION=when_required\nexport AWS_RESPONSE_CHECKSUM_VALIDATION=when_required\n</code></pre>"},{"location":"storage/object/#configuration","title":"Configuration","text":"<p>The first step is to configure the profile:</p> <pre><code>$ aws configure --profile naret-testuser\nAWS Access Key ID [None]: [REDACTED]\nAWS Secret Access Key [None]: [REDACTED]\nDefault region name [None]: cscs-zonegroup\nDefault output format [None]:\n</code></pre> <p>Then, settings such as the default endpoint and the path-style URLs can be placed in the configuration file:</p> <pre><code>[profile naret-testuser]\nendpoint_url = https://rgw.cscs.ch\nregion = cscs-zonegroup\ns3 =\n    addressing_style = path\n</code></pre>"},{"location":"storage/object/#creating-a-pre-signed-url","title":"Creating a pre-signed URL","text":"<pre><code>$ aws --profile=naret-testuser s3 presign s3://test-bucket/file.txt --expires-in 300\n\nhttps://rgw.cscs.ch/test-bucket/file.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=IA6AOCNMKPDXQ0YNA3DP%2F20241209%2Fcscs-zonegroup%2Fs3%2Faws4_request&amp;X-Amz-Date=20241209T080748Z&amp;X-Amz-Expires=300&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f2e2adb457f6fd43401124e4ea2650fba528e614ab661f9c05e2fa2e77691b5d\n</code></pre> <p>Notice that the tenant part is missing from the URL: this is because S3 doesn\u2019t natively deal with multitenancy. The correct object is retrieved based on the access key. A more thorough explanation can be found in the RGW documentation.</p>"},{"location":"storage/object/#making-a-buckets-contents-anonymously-accessible-from-the-internet","title":"Making a bucket\u2019s contents anonymously accessible from the Internet","text":"<p>First, a bucket policy needs to be written:</p> test-public-bucket-anon-from-internet.json<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": [\n        \"arn:aws:s3:::test-public-bucket/*\",\n        \"arn:aws:s3:::test-public-bucket\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Then, it can be applied to the bucket:</p> <pre><code>$ aws --profile=naret-testuser s3api put-bucket-policy \\\n      --bucket test-public-bucket --policy \\\n      file://test-public-bucket-anon-from-internet.json\n</code></pre> <p>At this point, the objects in test-public-bucket are accessible via direct links:</p> <pre><code>$ s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: [REDACTED]\nSecret Key: [REDACTED]\nDefault Region [US]: cscs-zonegroup\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: rgw.cscs.ch\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used\nif the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: rgw.cscs.ch/%(bucket)s\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: Yes\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: [REDACTED]\n  Secret Key: [REDACTED]\n  Default Region: cscs-zonegroup\n  S3 Endpoint: rgw.cscs.ch\n  DNS-style bucket+hostname:port template for accessing a bucket: rgw.cscs.ch/%(bucket)s\n  Encryption password:\n  Path to GPG program: None\n  Use HTTPS protocol: True\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n</code></pre> <p>And then confirm.</p> <p>IMPORTANT: The configuration is not complete yet.</p> <pre><code>$ s3cmd ls s3://test-bucket\nERROR: S3 error: 403 (SignatureDoesNotMatch)\n</code></pre> <p>To fix this, it is necessary to edit the <code>.s3cfg</code> file, normally located in the user\u2019s home directory, and change the <code>signature_v2</code> setting to true.</p> <pre><code>$ cat .s3cfg | grep signature_v2\nsignature_v2 = True\n\n$ s3cmd ls s3://test-bucket\n2024-12-09 08:05           15  s3://test-bucket/file.txt\n</code></pre>"},{"location":"storage/object/#cyberduck","title":"Cyberduck","text":""},{"location":"storage/object/#configuration_1","title":"Configuration","text":"<p>In order to be able to connect to the S3 endpoint using Cyberduck, a profile supporting path-style requests must be downloaded from here or copied from below.</p> S3 (Deprecated path style requests).cyberduckprofile<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n    &lt;dict&gt;\n        &lt;key&gt;Protocol&lt;/key&gt;\n        &lt;string&gt;s3&lt;/string&gt;\n        &lt;key&gt;Vendor&lt;/key&gt;\n        &lt;string&gt;s3-path-style&lt;/string&gt;\n        &lt;key&gt;Scheme&lt;/key&gt;\n        &lt;string&gt;https&lt;/string&gt;\n        &lt;key&gt;Description&lt;/key&gt;\n        &lt;string&gt;S3 (Deprecated path style requests)&lt;/string&gt;\n        &lt;key&gt;Hostname Configurable&lt;/key&gt;\n        &lt;true/&gt;\n        &lt;key&gt;Port Configurable&lt;/key&gt;\n        &lt;true/&gt;\n        &lt;key&gt;Username Configurable&lt;/key&gt;\n        &lt;true/&gt;\n        &lt;key&gt;Properties&lt;/key&gt;\n        &lt;array&gt;\n            &lt;string&gt;s3.bucket.virtualhost.disable=true&lt;/string&gt;\n        &lt;/array&gt;\n    &lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre> <p></p>"},{"location":"storage/transfer/","title":"Data Transfer","text":""},{"location":"storage/transfer/#data-transfer","title":"Data Transfer","text":""},{"location":"storage/transfer/#external-transfer","title":"External Transfer","text":"<p>CSCS currently offers the CSCS Globus online endpoint for uploading and downloading data from and to CSCS:</p> <p>The recommended way to transfer data externally occurs via the CSCS globus-online endpoint.</p> <ol> <li>Follow the official get started documentation to login<ul> <li>in case you don\u2019t have an organisation account, you can just use the option \u201cSign in with Google\u201d </li> </ul> </li> <li>Use the file manager to search for an endpoint typing \u201cCSCS\u201d<ul> <li>Please make sure that the login page belongs to the cscs.ch domain (shown in the URL)</li> <li>The CSCS endpoint requires authentication, therefore use your CSCS credentials to log in </li> </ul> </li> <li>Once logged in, you can transfer data to and from CSCS.<ul> <li>if you want to transfer the data to another endpoint, just search for it and transfer the data</li> <li>if you want to download the data to your local system, you will need the Globus Connect Personal client: the client will turn your local system into an endpoint, so you will be able to select it and transfer the data.</li> </ul> </li> </ol> <p>For more information about Globus Connect Personal, please read the official Frequently Asked Questions.</p> <p>Currently Globus provide the following mount points at CSCS:</p> Mount Point Description <code>/iopsstor/scratch/cscs</code> Iopsstor scratch <code>/capstor/scratch/cscs</code> Capstor scratch <code>/capstor/store/cscs</code> Store <code>/vast/users/cscs</code> Home <p></p>"},{"location":"storage/transfer/#internal-transfer","title":"Internal Transfer","text":"<p>The Slurm queue <code>xfer</code> is available on Alps clusters to address data transfers between internal CSCS file systems. The queue has been created to transfer files and folders from <code>/users</code>, <code>/capstor/store</code> or <code>/iopsstor/store</code> to the <code>/capstor/scratch</code> and <code>/iopsstor/scratch</code> file systems (stage-in) and vice versa (stage-out). Currently the following commands are available on the cluster supporting the queue xfer:</p> <pre><code>cp\nmv\nrm\nrsync\nrclone\n</code></pre> <p>You can adjust the Slurm batch script below to transfer your input data on <code>$SCRATCH</code>, setting the variable command to the unix command that you intend to use, choosing from the list given above:</p> <pre><code>#!/bin/bash -l\n#\n#SBATCH --time=02:00:00\n#SBATCH --ntasks=1\n#SBATCH --partition=xfer\n\ncommand=\"rsync -av\"\necho -e \"$SLURM_JOB_NAME started on $(date):\\n $command $1 $2\\n\"\nsrun -n $SLURM_NTASKS $command $1 $2\necho -e \"$SLURM_JOB_NAME finished on $(date)\\n\"\n\nif [ -n \"$3\" ]; then\n  # unset memory constraint enabled on xfer partition\n  unset SLURM_MEM_PER_CPU\n  # submit job with dependency\n  sbatch --dependency=afterok:$SLURM_JOB_ID $3\nfi\n</code></pre> <p>The template Slurm batch script above requires at least two command line arguments, which are the source and the destination files (or folders) to be copied. The stage script may take as third command line argument the name of the production Slurm batch script to be submitted after the stage job: the Slurm dependency flag <code>--dependency=afterok:$SLURM_JOB_ID</code> ensures that the production job can begin execution only after the stage job has successfully executed (i.e. ran to completion with an exit code of zero).</p> <p>You can submit the stage job with a meaningful job name as below:</p> <pre><code># stage-in and production jobs\n$ sbatch --job-name=stage_in stage.sbatch \\\n         ${PROJECT}/&lt;source&gt; ${SCRATCH}/&lt;destination&gt; \\\n         production.sbatch\n</code></pre> <p>The Slurm flag <code>--job-name</code> will set the name of the stage job that will be printed in the Slurm output file: the latter is by default the file <code>slurm-${SLURM_JOB_ID}.out</code>, unless you set a specific name for output and error using the Slurm flags <code>-e/--error</code> and/or <code>-o/--output</code> (e.g.: <code>-o %j.out -e %j.err</code>, where the Slurm symbol <code>%j</code> will be replaced by <code>$SLURM_JOB_ID</code>). l The stage script will also submit the Slurm batch script production.sbatch given as third command line argument. The production script can submit in turn a stage job to transfer the results back. E.g.:</p> <pre><code># stage-out\nsbatch --dependency=afterok:${SLURM_JOB_ID} --job-name=stage_out \\\n       stage.sbatch ${SCRATCH}/&lt;source&gt; ${PROJECT}/&lt;destination&gt;\n</code></pre>"},{"location":"tutorials/ml/","title":"Index","text":""},{"location":"tutorials/ml/#machine-learning-platform-tutorials","title":"Machine Learning Platform Tutorials","text":"<p>The LLM tutorials gradually introduce key concepts of the Machine Learning Platform in a series of hands-on examples. A particular focus is on the Container Engine for managing the runtime environment.</p> <p>In the first tutorial, you will learn how to run inference with a LLM on a single node using a container from the NVIDIA GPU Cloud (NGC). Concepts such as container environment description, layering a thin virtual environment on top of the container image, and job launching/monitoring will be introduced.</p> <p>Building on the first tutorial, in the second tutorial you will learn how to train (fine-tune) a LLM on multiple GPUs on a single node. For this purpose, you will use HuggingFace\u2019s <code>accelerate</code> and see best practices for dataset management.</p> <p>In the third tutorial, you will apply the techniques from the previous tutorials to enable distributed (pre-)training of a model in <code>nanotron</code> on multiple nodes. In particular, this tutorial makes use of model-parallelism and introduces the usage of <code>torchrun</code> to manage jobs on individual nodes.</p> <p>Note</p> <p>The focus for these tutorials is on introducing concepts of the Machine Learning Platform. As such, they do not necessarily discuss the latest advancements or steps required to obtain maximum performance. For this purpose, consult the framework-specific pages, such as the one for PyTorch. </p>"},{"location":"tutorials/ml/llm-fine-tuning/","title":"LLM Fine-tuning","text":""},{"location":"tutorials/ml/llm-fine-tuning/#llm-fine-tuning-tutorial","title":"LLM Fine-tuning Tutorial","text":"<p>This tutorial will take the model from the LLM Inference tutorial and show you how to perform fine-tuning. This means that we take the model and train it on some new custom data to change its behavior.</p> <p>To complete the tutorial, we set up some extra libraries that will help us to update the state of the machine learning model. We also write a script that will allow us to unlock more of the performance offered by the cluster, by running our fine-tuning task on two or more nodes.</p>"},{"location":"tutorials/ml/llm-fine-tuning/#fine-tuning-gemma-7b-on-the-openassistant-dataset","title":"Fine-tuning Gemma 7B on the OpenAssistant dataset","text":""},{"location":"tutorials/ml/llm-fine-tuning/#prerequisites","title":"Prerequisites","text":"<p>This tutorial assumes you\u2019ve already successfully completed the LLM Inference tutorial. For fine-tuning Gemma, we will rely on the NGC PyTorch container and the libraries we\u2019ve already installed in the Python virtual environment used previously.</p>"},{"location":"tutorials/ml/llm-fine-tuning/#set-up-trl","title":"Set up TRL","text":"<p>We will use HuggingFace TRL (Transformer Reinforcement Learning) to fine-tune Gemma-7B on the OpenAssistant dataset. First, we need to update our Python environment with some extra libraries to support TRL. To do this, we can launch an interactive shell in the PyTorch container, just like we did in the previous tutorial. Then, we install <code>peft</code>:</p> <pre><code>[clariden-lnXXX]$ cd $SCRATCH/tutorials/gemma-7b\n[clariden-lnXXX]$ srun --environment=./ngc-pytorch-gemma-24.01.toml --pty bash\nuser@nidYYYYYY$ source venv-gemma-24.01/bin/activate\n(venv-gemma-24.01) user@nidYYYYYY$ pip install peft==0.11.1\n</code></pre> <p>Next, we also need to clone and install the <code>trl</code> Git repository so that we have access to the fine-tuning scripts in it. For this purpose, we will install the package in editable mode in the virtual environment. This makes it available in python scripts independent of the current working directory and without creating a redundant copy of the files.</p> <pre><code>(venv-gemma-24.01) user@nidYYYYYY$ git clone \\\n  https://github.com/huggingface/trl -b v0.7.11\n(venv-gemma-24.01) user@nidYYYYYY$ pip install -e ./trl # (1)!\n</code></pre> <ol> <li>Installs trl in editable mode</li> </ol> <p>When this step is complete, you can exit the shell by typing <code>exit</code>.</p>"},{"location":"tutorials/ml/llm-fine-tuning/#fine-tune-gemma-7b","title":"Fine-tune Gemma-7B","text":"<p>At this point, we can set up a fine-tuning script and start training Gemma-7B. Use your favorite text editor to create the file <code>fine-tune-gemma.sh</code> just outside the <code>trl</code> and <code>venv-gemma-24.01</code> directories:</p> $SCRATCH/tutorials/gemma-7b/fine-tune-gemma.sh<pre><code>#!/bin/bash\n\nsource venv-gemma-24.01/bin/activate\n\nset -x\n\nexport HF_HOME=$SCRATCH/huggingface\nexport TRANSFORMERS_VERBOSITY=info\n\nACCEL_PROCS=$(( $SLURM_NNODES * $SLURM_GPUS_PER_NODE ))\n\nMAIN_ADDR=$(echo \"${SLURM_NODELIST}\" | sed 's/[],].*//g; s/\\[//g')\nMAIN_PORT=12802\n\naccelerate launch --config_file trl/examples/accelerate_configs/multi_gpu.yaml \\\n           --num_machines=$SLURM_NNODES --num_processes=$ACCEL_PROCS \\\n           --machine_rank $SLURM_PROCID \\\n           --main_process_ip $MAIN_ADDR --main_process_port $MAIN_PORT \\\n           trl/examples/scripts/sft.py \\\n           --model_name google/gemma-7b \\\n           --dataset_name OpenAssistant/oasst_top1_2023-08-25 \\\n           --per_device_train_batch_size 2 \\\n           --gradient_accumulation_steps 1 \\\n           --learning_rate 2e-4 \\\n           --save_steps 200 \\\n           --max_steps 400 \\\n           --use_peft \\\n           --lora_r 16 --lora_alpha 32 \\\n           --lora_target_modules q_proj k_proj v_proj o_proj \\\n           --output_dir gemma-fine-tuned-openassistant\n</code></pre> <p>This script has quite a bit more content to unpack. We use HuggingFace <code>accelerate</code> to launch the fine-tuning process, so we need to make sure that <code>accelerate</code> understands which hardware is available and where. Setting this up will be useful in the long run because it means we can tell Slurm how much hardware to reserve, and this script will setup all the details for us.</p> <p>The cluster has four GH200 chips per compute node. We can make them accessible to scripts run through <code>srun</code>/<code>sbatch</code> via the option <code>--gpus-per-node=4</code>. Then, we calculate how many processes accelerate should launch. We want to map each GPU to a separate process, this should be four processes per node. We multiply this by the number of nodes to obtain the total number of processes. Next, we use some bash magic to extract the name of the head node from Slurm environment variables. <code>accelerate</code> expects one main node and launches tasks on the other nodes from this main node. Having sourced our python environment at the top of the script, we can then launch Gemma fine-tuning. The first four lines of the launch line are used to configure <code>accelerate</code>. Everything after that configures the <code>trl/examples/scripts/sft.py</code> Python script, which we use to train Gemma.</p> <p>Dataset management and sharing</p> <p>For datasets, recommended LUSTRE settings should be used as illustrated in the tutorial on LLM Inference. As they have been set there for <code>HF_HOME</code>, which <code>huggingface_hub</code> uses for its dataset cache, they don\u2019t need to be re-applied here.</p> <p>To enable your colleagues to use also use your datasets, please refer to the storage guide.</p> <p>Make this script executable with</p> <pre><code>[clariden-lnXXX]$ chmod u+x $SCRATCH/tutorials/gemma-7b/fine-tune-gemma.sh\n</code></pre> <p>Next, we also need to create a short Slurm batch script to launch our fine-tuning script:</p> $SCRATCH/tutorials/gemma-7b/submit-fine-tune-gemma.sh<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=fine-tune-gemma\n#SBATCH --time=00:30:00\n#SBATCH --ntasks-per-node=1\n#SBATCH --gpus-per-node=4\n#SBATCH --cpus-per-task=288\n#SBATCH --output logs/slurm-%x-%j.out\n\nset -x\n\nsrun -ul --environment=./ngc-pytorch-gemma-24.01.toml fine-tune-gemma.sh\n</code></pre> <p>We set a few Slurm parameters like we already did in the previous tutorial. Note that we leave the number of nodes unspecified. This way, we can decide the number of nodes we want to use when we launch the batch job using Slurm.</p> <p>Now that we\u2019ve setup a fine-tuning script and a Slurm batch script, we can launch our fine-tuning job. We\u2019ll start out by launching it on two nodes. It should take about 10-15 minutes to fine-tune Gemma:</p> <pre><code>[clariden-lnXXX]$ sbatch --nodes=1 submit-fine-tune-gemma.sh\n</code></pre>"},{"location":"tutorials/ml/llm-fine-tuning/#compare-fine-tuned-gemma-against-default-gemma","title":"Compare fine-tuned Gemma against default Gemma","text":"<p>We can reuse our python script from the first tutorial to do inference on the Gemma model that we just fine-tuned. Let\u2019s try out a different prompt in <code>gemma-inference.py</code>:</p> <pre><code>input_text = \"What are the 5 tallest mountains in the Swiss Alps?\"\n</code></pre> <p>We can run inference using our batch script from the previous tutorial:</p> <pre><code>[clariden-lnXXX]$ sbatch submit-gemma-inference.sh\n</code></pre> <p>Inspecting the output should yield something like this:</p> <pre><code>&lt;bos&gt;What are the 5 tallest mountains in the Swiss Alps?\n\nThe Swiss Alps are home to some of the tallest mountains in the world. Here are\nthe 5 tallest mountains in the Swiss Alps:\n\n1. Mont Blanc (4,808 meters)\n2. Matterhorn (4,411 meters)\n3. Dom (4,161 meters)\n4. Jungfrau (4,158 meters)\n5. Mont Rose (4,117 meters)&lt;eos&gt;\n</code></pre> <p>Next, we can update the model line in our Python inference script to use the model that we just fine-tuned:</p> <pre><code>model = AutoModelForCausalLM.from_pretrained(\n    \"gemma-fine-tuned-openassistant/checkpoint-400\", device_map=\"auto\")\n</code></pre> <p>If we re-run inference, the output will be a bit more detailed and explanatory, similar to output we might expect from a helpful chatbot. One example looks like this:</p> <pre><code>&lt;bos&gt;What are the 5 tallest mountains in the Swiss Alps?\n\nThe Swiss Alps are home to some of the tallest mountains in Europe, and they are a popular destination for mountai\nneers and hikers. Here are the five tallest mountains in the Swiss Alps:\n\n1. Mont Blanc (4,808 m/15,774 ft): Mont Blanc is the highest mountain in the Alps and the highest mountain in Euro\npe outside of Russia. It is located on the border between France and Italy, and it is a popular destination for mo\nuntaineers and hikers.\n\n2. Dufourspitze (4,634 m/15,203 ft): Dufourspitze is the highest mountain in Switzerland and the second-highest mo\nuntain in the Alps. It is located in the Valais canton of Switzerland, and it is a popular destination for mountai\nneers and hikers.\n\n3. Liskamm (4,527 m/14,855 ft): Liskamm is a mountain in the Bernese Alps of Switzerland. It is located in the Ber\nn canton of Switzerland, and it is a popular destination for mountaineers and hikers.\n\n4. Weisshorn (4,506 m/14,783 ft): Weisshorn is a mountain in the Pennine Alps of Switzerland. It is located in the\n Valais canton of Switzerland, and it is a popular destination for mountaineers and hikers.\n\n5. Matterhorn (4,478 m/14,690 ft): Matterhorn is a mountain in the Pennine Alps of Switzerland. It is located in the Valais canton of Switzerland, and it is a popular destination for mountaineers and hikers.\n\nThese mountains are all located in the Swiss Alps, and they are a popular destination for mountaineers and hikers. If you are planning a trip to the Swiss Alps, be sure to check out these mountains and plan your itinerary accordingly.\n</code></pre> <p>Your output may look different after fine-tuning, but in general you will see that the fine-tuned model generates more verbose output. Double-checking the output reveals that the list of mountains produced by Gemma is not actually correct. These are the 5 tallest Swiss peaks according to Wikipedia:</p> <ol> <li>Dufourspitze 4,634m</li> <li>Nordend 4,609m</li> <li>Zumsteinspitz 4,563m</li> <li>Signalkuppe 4,554m</li> <li>Dom 4,545m</li> </ol> <p>This is an important reminder that machine-learning models like Gemma need extra checks to confirm any generated outputs.</p>"},{"location":"tutorials/ml/llm-inference/","title":"LLM Inference","text":""},{"location":"tutorials/ml/llm-inference/#llm-inference-tutorial","title":"LLM Inference Tutorial","text":"<p>This tutorial will guide you through the steps required to set up a PyTorch container and do ML inference. This means that we load an existing machine learning model, prompt it with some custom data, and run the model to see what output it will generate with our data.</p> <p>To complete the tutorial, we get a PyTorch container from Nvidia\u2019s GPU Cloud (NGC), customize it to suit our needs, and tell the Container Engine how to run it. Finally, we set up and run a python script to run the machine learning model and generate some output.</p> <p>The model we will be running is Google\u2019s Gemma-7B in the instruction-tuned variant. This is an LLM similar in style to popular chat assistants like ChatGPT, which can generate text responses to text prompts that we feed into it.</p>"},{"location":"tutorials/ml/llm-inference/#gemma-7b-inference-using-ngc-pytorch","title":"Gemma-7B Inference using NGC PyTorch","text":""},{"location":"tutorials/ml/llm-inference/#prerequisites","title":"Prerequisites","text":"<p>This tutorial assumes you are able to access the cluster via SSH. To set up access to CSCS systems, follow the guide here, and read through the documentation about the ML Platform.</p> <p>For clarity, we prepend all shell commands with the hostname and any active Python virtual environment they are executed in. E.g. <code>clariden-lnXXX</code> refers to a login node on Clariden, while <code>nidYYYYYY</code> is a compute node (with placeholders for numeric values). The commands listed here are run on Clariden, but can be adapted slightly to run on other vClusters as well.</p> <p>Note</p> <p>Login nodes are a shared environment for editing files, preparing and submitting SLURM jobs as well as inspecting logs. They are not intended for running significant data processing or compute work. Any memory- or compute-intensive work should instead be done on compute nodes.</p> <p>If you need to move data externally or internally, please follow the corresponding guides using Globus or the <code>xfer</code> queue, respectively.</p>"},{"location":"tutorials/ml/llm-inference/#build-a-modified-ngc-pytorch-container","title":"Build a modified NGC PyTorch Container","text":"<p>In theory, we could just go ahead and use the vanilla container image to run some PyTorch code. However, chances are that we will need some additional libraries or software. For this reason, we need to build another image on top of the one provided by Nvidia. To do this, we create a new directory for recipes to build containers in our home directory and set up a Dockerfile:</p> <pre><code>[clariden-lnXXX]$ cd $SCRATCH\n[clariden-lnXXX]$ mkdir -p tutorials/gemma-7b\n[clariden-lnXXX]$ cd tutorials/gemma-7b\n</code></pre> <p>Use your favorite text editor to create a file <code>Dockerfile</code> here. The Dockerfile should look like this:</p> $SCRATCH/tutorials/gemma-7b/Dockerfile<pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3\n\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y python3.10-venv &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre> <p>The first line specifies that we are working on top of an existing container. In this case we start <code>FROM</code> an NGC PyTorch container. Next, we set an environment variable with <code>ENV</code> that helps us run <code>apt-get</code> in the container. Finally, we <code>RUN</code> the package installer <code>apt-get</code> to install python virtual environments. This will let us install python packages later on without having to rebuild the container again and again. There\u2019s a bunch of extra commands in this line to tidy things up. If you want to understand what is happening, take a look at the Docker documentation.</p> <p>Recent changes in NGC releases</p> <p>Starting with the 24.11 release, NGC PyTorch no longer requires the installation of the Python venv module. That is, the Dockerfile simplifies to only the first line, e.g. for the <code>25.06</code> release</p> <pre><code>FROM nvcr.io/nvidia/pytorch:25.06-py3\n</code></pre> <p>The remaining steps can then be performed equivalently, replacing the version number <code>24.01</code> by the one chosen in the Dockerfile (e.g. <code>25.06</code>).</p> <p>It is generally recommended to stick to one of the most recent versions of NGC, unless there is a strong reason from your application to stick to an old version for compatibility.</p> <p>Now that we\u2019ve setup the Dockerfile, we can go ahead and pass it to Podman to build a container. Podman is a tool that enables us to fetch, manipulate, and interact with containers on the cluster. For more information, please see the Container Engine page. To use Podman, we first need to configure some storage locations for it. This step is straightforward, just create the file in your home:</p> $HOME/.config/containers/storage.conf<pre><code>[storage]\n  driver = \"overlay\"\n  runroot = \"/dev/shm/$USER/runroot\"\n  graphroot = \"/dev/shm/$USER/root\"\n\n[storage.options.overlay]\n  mount_program = \"/usr/bin/fuse-overlayfs-1.13\"\n</code></pre> <p>Warning</p> <p>If <code>$XDG_CONFIG_HOME</code> is set, place this file at <code>$XDG_CONFIG_HOME/containers/storage.conf</code> instead.</p> <p>Before building the container image, we create a dedicated directory to keep track of all images used with the CE. Since container images are large files and the filesystem is a shared resource, we need to apply best practices for LUSTRE so they are properly distributed across storage nodes.</p> Container image directory with recommended LUSTRE settings<pre><code>[clariden-lnXXX]$ mkdir -p $SCRATCH/ce-images\n[clariden-lnXXX]$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -S 4M \\\n  $SCRATCH/ce-images # (1)!\n</code></pre> <ol> <li>This makes sure that files stored subsequently end up on the same storage node (up to 4 MB), on 4 storage nodes (between 4 and 64 MB) or are striped across all storage nodes (above 64 MB)</li> </ol> <p>To build a container with Podman, we need to request a shell on a compute node from Slurm, pass the Dockerfile to Podman, and finally import the freshly built container to the dedicated directory using enroot. Slurm is a workload manager which distributes workloads on the cluster. Through Slurm, many people can use the supercomputer at the same time without interfering with one another.</p> <pre><code>[clariden-lnXXX]$ srun -A &lt;ACCOUNT&gt; --pty bash\n[nidYYYYYY]$ podman build -t ngc-pytorch:24.01 . # (1)!\n# ... lots of output here ...\n[nidYYYYYY]$ enroot import -x mount \\\n  -o $SCRATCH/ce-images/ngc-pytorch+24.01.sqsh \\\n  podman://ngc-pytorch:24.01 # (2)!\n# ... more output here ...\n</code></pre> <ol> <li>This builds the container image with the current working directory as the build context. The <code>Dockerfile</code> inside that directory is implicitly used as a recipe. If it is named differently use the <code>-f path/to/Dockerfile</code> option.  </li> <li>The newly built container image is imported and stored under <code>$SCRATCH/ce-images</code>.</li> </ol> <p>where you should replace <code>&lt;ACCOUNT&gt;</code> with your project account ID. At this point, you can exit the Slurm allocation by typing <code>exit</code>. You should be able to see a new Squashfs file in your container image directory:</p> <pre><code>[clariden-lnXXX]$ ls $SCRATCH/ce-images\nngc-pytorch+24.01.sqsh\n</code></pre> <p>This squashfs file is essentially a compressed container image, which can be run directly by the container engine. We will use our freshly-built container <code>ngc-pytorch+24.01.sqsh</code> in the following steps to run a PyTorch script that loads the Google Gemma-7B model and performs some inference with it.</p> <p>Note</p> <p>In order to import a container image from a registry without building additional layers on top of it, we can directly use <code>enroot</code> (without <code>podman</code>). This is useful in this tutorial if we want to use a more recent NGC PyTorch container that was released since <code>24.11</code>. Use the following syntax for importing the <code>25.06</code> release:</p> <pre><code>[nidYYYYYY]$ enroot import -x mount \\\n  -o $SCRATCH/ce-images/ngc-pytorch+25.06.sqsh docker://nvcr.io#nvidia/pytorch:25.06-py3\n</code></pre>"},{"location":"tutorials/ml/llm-inference/#set-up-an-edf","title":"Set up an EDF","text":"<p>We need to set up an EDF (Environment Definition File) which tells the Container Engine what container image to load, which paths to mount from the host filesystem, and what plugins to load. Use your favorite text editor to create a file <code>ngc-pytorch-gemma-24.01.toml</code> for the container engine. The EDF should look like this:</p> $SCRATCH/tutorials/gemma-7b/ngc-pytorch-gemma-24.01.toml<pre><code>image = \"${SCRATCH}/ce-images/ngc-pytorch+24.01.sqsh\" # (1)!\n\nmounts = [\n    \"/capstor\",\n    \"/iopsstor\"\n] # (2)!\n\nworkdir = \"${SCRATCH}/tutorials/gemma-7b\" # (3)!\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\" # (4)!\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nNCCL_DEBUG = \"INFO\" # (5)!\nCUDA_CACHE_DISABLE = \"1\" # (6)!\nTORCH_NCCL_ASYNC_ERROR_HANDLING = \"1\" # (7)!\nMPICH_GPU_SUPPORT_ENABLED = \"0\" # (8)!\n</code></pre> <ol> <li>It is important to use curly braces for environment variables used in the EDF</li> <li>The path <code>/users</code> is not mounted since it often contains user-specific initialization scripts for the host environment and many frameworks leave temporary data behind that can lead to non-trivial runtime errors when swapping container images. Thus, it is recommended to selectively mount specific subfolders under <code>${HOME}</code> if needed.</li> <li>You can use <code>${PWD}</code> as an alternative to use the path submitted from when the container is started</li> <li>This enables NCCL installed in the container to make effective use of the Slingshot interconnect on Alps by interfacing with the AWS OFI NCCL plugin with libfabric. While not strictly needed for single node workloads, it is good practice to keep it always on.</li> <li>This makes NCCL output debug info during initialization, which can be useful to spot communication-related issues in a distributed scenario (see later tutorials). Subsystems with debug log can be configured with <code>NCCL_DEBUG_SUBSYS</code>.</li> <li>Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.</li> <li>Async error handling when an exception is observed in NCCL watchdog: aborting NCCL communicator and tearing down process upon error</li> <li>Disable GPU support in MPICH, as it can lead to deadlocks when using together with NCCL</li> </ol> <p>If you\u2019ve decided to build the container somewhere else, make sure to supply the correct path to the <code>image</code> variable. </p> <p>The <code>image</code> variable defines which container we want to load. This could either be a container from an online docker repository, like <code>nvcr.io/nvidia/pytorch:24.01-py3</code>, or in our case, a local squashfs file which we built ourselves.</p> <p>The <code>mounts</code> variable defines which directories we want to mount where in our container. In general, it\u2019s a good idea to use a directory under <code>/capstor/scratch</code> directory to store outputs from any scientific software as this filesystem is optimized for sequential write-operations as described in Alps storage. This particularly applies to e.g. checkpoints from ML training, which we will see in the next tutorials (and there it matters also to apply good LUSTRE settings beforehand as for container images). In this tutorial, we will not generate a lot of output, but it\u2019s a good practice to stick to anyways.</p> <p>Finally, the <code>workdir</code> variable tells the container engine where to start working. If we request a shell, this is where we will find ourselves dropped initially after starting the container.</p>"},{"location":"tutorials/ml/llm-inference/#set-up-a-python-virtual-environment","title":"Set up a Python Virtual Environment","text":"<p>This will be the first time we run our modified container. To run the container, we need allocate some compute resources using Slurm and launch a shell, just like we already did to build the container. This time, we also use the <code>--environment</code> option to specify that we want to launch the shell inside the container specified by our gemma-pytorch EDF file:</p> <pre><code>[clariden-lnXXX]$ cd $SCRATCH/tutorials/gemma-7b\n[clariden-lnXXX]$ srun -A &lt;ACCOUNT&gt; \\\n  --environment=./ngc-pytorch-gemma-24.01.toml --pty bash\n</code></pre> <p>PyTorch is already setup in the container for us. We can verify this by asking pip for a list of installed packages:</p> <pre><code>user@nidYYYYYY$ python -m pip list | grep torch\npytorch-quantization      2.1.2\ntorch                     2.2.0a0+81ea7a4\ntorch-tensorrt            2.2.0a0\ntorchdata                 0.7.0a0\ntorchtext                 0.17.0a0\ntorchvision               0.17.0a0\n</code></pre> <p>However, we will need to install a few more Python packages to make it easier to do inference with Gemma-7B. While it is best practice to install stable dependencies in the container image, we can maintain frequently changing packages in a virtual environment built on top of the container image. The <code>--system-site-packages</code> option of the Python <code>venv</code> creation command ensures that we install packages in addition to the existing packages and don\u2019t accidentally re-install a new version of PyTorch shadowing the one that has been put in place by Nvidia. Next, we activate the environment and use pip to install the two packages we need, <code>accelerate</code> and <code>transformers</code>:</p> <pre><code>user@nidYYYYYY$ python -m venv --system-site-packages venv-gemma-24.01\nuser@nidYYYYYY$ source venv-gemma-24.01/bin/activate\n(venv-gemma-24.01) user@nidYYYYYY$ pip install \\\n  accelerate==0.30.1 transformers==4.38.1 huggingface_hub[cli]\n# ... pip output ...\n</code></pre> <p>Before we move on to running the Gemma-7B model, we additionally need to make an account at HuggingFace, get an API token, and accept the license agreement for the Gemma-7B model. You can save the token to <code>$SCRATCH</code> using the huggingface-cli:</p> <pre><code>(venv-gemma-24.01) user@nidYYYYYY$ export HF_HOME=$SCRATCH/huggingface\n(venv-gemma-24.01) user@nidYYYYYY$ huggingface-cli login\n</code></pre> <p>At this point, you can exit the Slurm allocation again by typing <code>exit</code>. If you <code>ls</code> the contents of the <code>gemma-inference</code> folder, you will see that the <code>venv-gemma-24.01</code> virtual environment folder persists outside of the Slurm job.</p> <p>Note</p> <p>Keep in mind that</p> <ul> <li>this virtual environment won\u2019t actually work unless you\u2019re running something from inside the PyTorch container. This is because the virtual environment ultimately relies on the resources packaged inside the container.</li> <li>every Slurm job making use of this virtual environment will need to activate it first (inside the <code>srun</code> command). </li> </ul> <p>Since <code>HF_HOME</code> will not only contain the API token, but also be the storage location for model, dataset and space caches of <code>huggingface_hub</code> (unless <code>HF_HUB_CACHE</code> is set), we also want to apply proper LUSTRE striping settings before it gets populated.</p> <pre><code>[clariden-lnXXX]$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -S 4M \\\n  $SCRATCH/huggingface\n</code></pre>"},{"location":"tutorials/ml/llm-inference/#run-inference-on-gemma-7b","title":"Run Inference on Gemma-7B","text":"<p>Cool, now you have a working container with PyTorch and all the necessary Python packages installed! Let\u2019s move on to Gemma-7B. We write a Python script to load the model and prompt it with some custom text. The Python script should look like this:</p> $SCRATCH/tutorials/gemma-7b/gemma-inference.py<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")\n\ninput_text = \"Write me a poem about the Swiss Alps.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, max_new_tokens=1024)\nprint(tokenizer.decode(outputs[0]))\n</code></pre> <p>Feel free to change the <code>input_text</code> variable to whatever prompt you like.</p> <p>All that remains is to run the python script inside the PyTorch container. There are several ways of doing this. As before, you could just use Slurm to get an interactive shell in the container. Then you would source the virtual environment and run the Python script we just wrote. There\u2019s nothing wrong with this approach per se, but consider that you might be running much more complex and lengthy Slurm jobs in the future. You\u2019ll want to document how you\u2019re calling Slurm, what commands you\u2019re running on the shell, and you might not want to (or might not be able to) keep a terminal open for the length of time the job might take. For this reason, it often makes sense to write a batch file, which enables you to document all these processes and run the Slurm job regardless of whether you\u2019re still connected to the cluster.</p> <p>Create a Slurm batch file <code>submit-gemma-inference.sh</code>. It should look like this:</p> $SCRATCH/tutorials/gemma-7b/submit-gemma-inference.sh<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=gemma-inference\n#SBATCH --time=00:15:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=288\n#SBATCH --output logs/slurm-%x-%j.out\n\nexport HF_HOME=$SCRATCH/huggingface\nexport TRANSFORMERS_VERBOSITY=info\n\ncd $SCRATCH/tutorials/gemma-7b # (1)!\n\nset -x\n\nsrun -ul --environment=./ngc-pytorch-gemma-24.01.toml bash -c \"\n    source venv-gemma-24.01/bin/activate\n    python gemma-inference.py\n\"\n</code></pre> <ol> <li>Change directory if submitted with sbatch from a different directory</li> </ol> <p>The first few lines of the batch script declare the shell we want to use to run this batch file and pass several options to the Slurm scheduler. After this, we <code>cd</code> to our working directory and <code>srun</code> the command in our container environment that <code>source</code>s our virtual environment and finally runs our inference script.</p> <p>The operations performed before the <code>srun</code> command resemble largely the operations performed on the login node above and, in fact, happen in the host environment. If you need to perform these steps in the container environment as well, you can alternatively use the <code>#SBATCH --environment=path/to/ngc-pytorch-gemma-24.01.toml</code> option instead of using <code>--environment</code> with <code>srun</code>.</p> <p>#SBATCH \u2013environment</p> <p>Use of the <code>--environment</code> option for <code>sbatch</code> is still considered experimental and could result in unexpected behavior. In particular, avoid mixing <code>#SBATCH --environment</code> and <code>srun --environment</code> in the same job.</p> <p>Use of <code>--environment</code> is currently only recommended for the <code>srun</code> command. </p> <p>Once you\u2019ve finished editing the batch file, you can save it and run it with Slurm:</p> <pre><code>[clariden-lnXXX]$ sbatch submit-gemma-inference.sh\n</code></pre> <p>This command should just finish without any output and return you to your terminal. At this point, you can follow the output in your shell using <code>tail -f logs/slurm-gemma-inference-&lt;job-id&gt;.out</code>. Besides you\u2019re free to do whatever you like; you can close the terminal, keep working, or just wait for the Slurm job to finish. You can always check on the state of your job by logging back into the cluster and running <code>squeue -l --me</code>. Once your job finishes, you will find a file in the same directory you ran it from, named something like <code>logs/slurm-gemma-inference-&lt;job-id&gt;.out</code>, and containing the output generated by your Slurm job. For this tutorial, you should see something like the following:</p> <pre><code>[clariden-lnXXX]$ cat logs/slurm-gemma-inference-543210.out\n/capstor/scratch/cscs/user/gemma-inference/venv-gemma-24.01/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nGemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:03&lt;00:00,  1.13it/s]\n/capstor/scratch/cscs/user/gemma-inference/venv-gemma-24.01/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n&lt;bos&gt;Write me a poem about the Swiss Alps.\n\nIn the heart of Switzerland, where towering peaks touch sky,\nLies a playground of beauty, beneath the watchful eye.\nThe Swiss Alps, a majestic force,\nA symphony of granite, snow, and force.\n\nSnow-laden peaks pierce the heavens above,\nTheir glaciers whisper secrets of ancient love.\nEmerald valleys bloom with flowers,\nA tapestry of colors, a breathtaking sight.\n\nHiking trails wind through meadows and woods,\nWhere waterfalls cascade, a silent song unfolds.\nThe crystal clear lakes reflect the sky above,\nA mirror of dreams, a place of peace and love.\n\nThe Swiss Alps, a treasure to behold,\nA land of wonder, a story untold.\nFrom towering peaks to shimmering shores,\nThey inspire awe, forevermore.&lt;eos&gt;\n</code></pre> <p>Congrats! You\u2019ve run Google Gemma-7B inference on four GH200 chips simultaneously. Move on to the next tutorial or try the challenge.</p> <p>Collaborating with Git</p> <p>In order to track and exchange your progress with colleagues, you can use standard <code>git</code> commands on the host, i.e. in the directory <code>$SCRATCH/tutorials/gemma-7b</code> run <pre><code>[clariden-lnXXX]$ git init .\n[clariden-lnXXX]$ git remote add origin \\\n  git@github.com:&lt;github-username&gt;/alps-mlp-tutorials-gemma-7b.git # (1)!\n[clariden-lnXXX]$ ... # git add/commit\n</code></pre></p> <ol> <li>Use any alternative Git hosting service instead of Github</li> </ol> <p>where you can replace <code>&lt;github-username&gt;</code> by the owner of the Github repository you want to push to.</p> <p>Note that for reproducibility, it is recommended to always track the Dockerfile and EDF alongside your application code in a Git repository.</p>"},{"location":"tutorials/ml/llm-inference/#challenge","title":"Challenge","text":"<p>Using the same approach as in the latter half of step 4, use pip to install the package <code>nvitop</code>. This is a tool that shows you a concise real-time summary of GPU activity. Then, run Gemma and launch <code>nvitop</code> at the same time:</p> <pre><code>(venv-gemma-24.01) user@nidYYYYYY$ python gemma-inference.py \\\n  &gt; gemma-output.log 2&gt;&amp;1 &amp; nvitop\n</code></pre> <p>Note the use of bash <code>&gt; gemma-output.log 2&gt;&amp;1</code> to hide any output from Python. Note also the use of the single ampersand <code>'&amp;'</code> which backgrounds the first command in order to run <code>nvitop</code> exclusively in the foreground.</p> <p>After a moment, you will see your Python script spawn on all four GPUs, after which the GPU activity will increase a bit and then go back to idle. At this point, you can hit <code>q</code> to quite nvitop and you will find the output of your Python script in <code>gemma-output.log</code>.</p>"},{"location":"tutorials/ml/llm-nanotron-training/","title":"LLM Pre-training","text":""},{"location":"tutorials/ml/llm-nanotron-training/#llm-nanotron-pre-training-tutorial","title":"LLM Nanotron Pre-training Tutorial","text":"<p>In this tutorial, we will build a container image to run multi-node training jobs with nanotron. We will train a 109M parameter model with ~100M wikitext tokens as a proof of concept.</p> <p>Info</p> <p>While the concepts taught here for multi-node training with PyTorch are generally portable across training frameworks, the current (August 2025) recommendation for users with a need for large-scale model-parallel training is to use <code>Megatron-LM</code> instead of <code>nanotron</code> due to significant performance advantages at scale. </p>"},{"location":"tutorials/ml/llm-nanotron-training/#prerequisites","title":"Prerequisites","text":"<p>It is recommended to follow the previous two tutorials on LLM Inference and LLM Fine-tuning first, as this will build upon them.</p>"},{"location":"tutorials/ml/llm-nanotron-training/#set-up-podman","title":"Set up Podman","text":"<p>If not already done as part of the LLM Inference tutorial, edit your podman configuration in <code>$HOME/.config/containers/storage.conf</code> as follows:</p> $HOME/.config/containers/storage.conf<pre><code>[storage]\n  driver = \"overlay\"\n  runroot = \"/dev/shm/$USER/runroot\"\n  graphroot = \"/dev/shm/$USER/root\"\n\n[storage.options.overlay]\n  mount_program = \"/usr/bin/fuse-overlayfs-1.13\"\n</code></pre> <p>Warning</p> <p>If <code>$XDG_CONFIG_HOME</code> is set, place this file at <code>$XDG_CONFIG_HOME/containers/storage.conf</code> instead.</p> <p>Create a directory to store container images used with CE and configure it with recommended LUSTRE settings:</p> Container image directory with recommended LUSTRE settings<pre><code>[clariden-lnXXX]$ mkdir -p $SCRATCH/ce-images\n[clariden-lnXXX]$ lfs setstripe -E 4M -c 1 -E 64M -c 4 -E -1 -c -1 -S 4M $SCRATCH/ce-images # (1)!\n</code></pre> <ol> <li>This makes sure that files stored subsequently end up on the same storage node (up to 4 MB), on 4 storage nodes (between 4 and 64 MB) or are striped across all storage nodes (above 64 MB)</li> </ol>"},{"location":"tutorials/ml/llm-nanotron-training/#build-a-modified-ngc-container","title":"Build a modified NGC Container","text":"<p>In this tutorial, we build a virtual environment on top of a customized NGC container image. This represents a typical task during development, where stable dependencies are captured in a static container image, whereas frequently changing packages are installed in a virtual environment on top. In contrast to the previous tutorials, the container in this case will be mostly self-contained.</p> <p>Here, we assume we are already in a compute node (run <code>srun -A &lt;ACCOUNT&gt; --pty bash</code> to get an interactive session). In this case, we create a Dockerfile with the following contents:</p> $SCRATCH/tutorials/nanotron-pretrain/Dockerfile<pre><code>FROM nvcr.io/nvidia/pytorch:24.04-py3\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y python3.10-venv &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Update flash-attn.\nRUN pip install --upgrade --no-build-isolation flash-attn==2.5.8\n\n# Install the rest of dependencies.\nRUN pip install \\\n    datasets \\\n    transformers \\\n    wandb \\\n    dacite \\\n    pyyaml \\\n    numpy \\\n    packaging \\\n    safetensors \\\n    tqdm\n</code></pre> <p>More recent NGC releases</p> <p>As discussed in the LLM Inference tutorial, starting with the 24.11 release, NGC PyTorch no longer requires the installation of the Python venv module. Furthermore, FlashAttention and several other packages were integrated into the hosted image. However, as <code>nanotron</code> as of June 2025 still requires Python 3.10 (cf. this issue), this example is restricted to NGC releases up to <code>24.10</code>.</p> $SCRATCH/tutorials/nanotron-pretrain/Dockerfile<pre><code>FROM nvcr.io/nvidia/pytorch:24.10-py3\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y python3.10-venv &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n# Update flash-attn.\nRUN pip install --upgrade --no-build-isolation flash-attn==2.5.8\n\n# Install the rest of dependencies.\nRUN pip install \\\n    datasets \\\n    transformers \\\n    wandb \\\n    dacite \\\n    pyyaml\n</code></pre> <p>The remaining steps can then be performed equivalently, replacing the version number <code>24.04</code> by the one chosen in the Dockerfile (e.g. <code>24.10</code>).</p> <p>It is generally recommended to stick to one of the most recent versions of NGC, unless there is a strong reason from your application to stick to an old version for compatibility.</p> <p>Then build and import the container.</p> <pre><code>[nidYYYYYY]$ cd $SCRATCH/tutorials/nanotron-pretrain\n[nidYYYYYY]$ podman build -f Dockerfile -t ngc-nanotron:24.04 .\n[nidYYYYYY]$ enroot import -x mount \\\n  -o $SCRATCH/ce-images/ngc-nanotron+24.04.sqsh podman://ngc-nanotron:24.04  # (1)!\n</code></pre> <ol> <li>We import container images into a canonical location under $SCRATCH.</li> </ol> <p>Debugging the container build</p> <p>If the container build fails, you can run an interactive shell using the image from the last successfully built layer with</p> <pre><code>podman run -it --rm -e NVIDIA_VISIBLE_DEVICES=void &lt;last-layer-hash&gt; bash # (1)!\n</code></pre> <ol> <li>Setting <code>NVIDIA_VISIBLE_DEVICES</code> in the environment is required specifically to run NGC containers with podman</li> </ol> <p>replacing <code>&lt;last-layer-hash&gt;</code> by the actual hash output in the build job and interactively test the failing command.  </p> <p>Now exit the interactive session by running <code>exit</code>.</p>"},{"location":"tutorials/ml/llm-nanotron-training/#set-up-an-environment-description-file-edf","title":"Set up an environment description file (EDF)","text":"<p>See the previous tutorial for context. In this case, the EDF will be co-located with the Dockerfile under <code>$SCRATCH/tutorials/nanotron-pretrain</code> and will have the following contents:</p> $SCRATCH/tutorials/nanotron-pretrain/ngc-nanotron-24.04.toml<pre><code>image = \"${SCRATCH}/ce-images/ngc-nanotron+24.04.sqsh\" # (1)!\n\nmounts = [\n    \"/capstor\",\n    \"/iopsstor\"\n] # (2)!\n\nworkdir = \"${SCRATCH}/tutorials/nanotron-pretrain/\" # (3)!\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\" # (4)!\ncom.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n[env]\nNCCL_DEBUG = \"INFO\" # (5)!\nCUDA_CACHE_DISABLE = \"1\" # (6)!\nTORCH_NCCL_ASYNC_ERROR_HANDLING = \"1\" # (7)!\nMPICH_GPU_SUPPORT_ENABLED = \"0\" # (8)!\n</code></pre> <ol> <li>It is important to use curly braces for environment variables used in the EDF</li> <li>The path <code>/users</code> is not mounted since it often contains user-specific initialization scripts for the host environment and many frameworks leave temporary data behind that can lead to non-trivial runtime errors when swapping container images. Thus, it is recommended to selectively mount specific subfolders under <code>${HOME}</code> if needed.</li> <li>You can use <code>${PWD}</code> as an alternative to use the path submitted from when the container is started</li> <li>This enables NCCL installed in the container to make effective use of the Slingshot interconnect on Alps by interfacing with the AWS OFI NCCL plugin with libfabric. While not strictly needed for single node workloads, it is good practice to keep it always on.</li> <li>This makes NCCL output debug info during initialization, which can be useful to spot communication-related issues in a distributed scenario (see later tutorials). Subsystems with debug log can be configured with <code>NCCL_DEBUG_SUBSYS</code>.</li> <li>Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.</li> <li>Async error handling when an exception is observed in NCCL watchdog: aborting NCCL communicator and tearing down process upon error</li> <li>Disable GPU support in MPICH, as it can lead to deadlocks when using together with NCCL</li> </ol> <p>Note that, if you built your container image elsewhere, you will need to modify the image path.</p>"},{"location":"tutorials/ml/llm-nanotron-training/#installing-nanotron-in-a-virtual-environment","title":"Installing nanotron in a virtual environment","text":"<p>Now let\u2019s download nanotron. In the login node run:</p> <pre><code>[clariden-lnXXX]$ cd $SCRATCH/tutorials/nanotron-pretrain\n[clariden-lnXXX]$ git clone https://github.com/huggingface/nanotron.git\n[clariden-lnXXX]$ cd nanotron\n[clariden-lnXXX]$ git checkout 5f8a52b08b702e206f31f2660e4b6f22ac328c95  # (1)!\n</code></pre> <ol> <li>This ensures the compatibility of nanotron with the following example. For general usage, there is no reason to stick to an outdated version of nanotron, though.</li> </ol> <p>We will install nanotron in a thin virtual environment on top of the container image built above. This proceeds as in the LLM Inference.</p> <pre><code>[clariden-lnXXX]$ srun  -A &lt;ACCOUNT&gt; --environment=./ngc-nanotron-24.04.toml --pty bash\nuser@nidYYYYYY$ python -m venv --system-site-packages venv-24.04\nuser@nidYYYYYY$ source venv-24.04/bin/activate\n(venv-24.04) user@nidYYYYYY$ cd nanotron/ &amp;&amp; pip install -e .\n</code></pre> <p>This creates a virtual environment on top of this container image (<code>--system-site-packages</code> ensuring access to system-installed site-packages) and installs nanotron in editable mode inside it. Because all dependencies of nanotron are already installed in the Dockerfile, no extra libraries will be installed at this point.</p> <p>Note</p> <p>Jobs making use of this virtual environment will always need to activate it first (inside the <code>srun</code> command). </p>"},{"location":"tutorials/ml/llm-nanotron-training/#preparing-a-training-job","title":"Preparing a Training Job","text":"<p>Now, with your favorite text editor, create the following nanotron configuration file:</p> $SCRATCH/tutorials/nanotron-pretrain/nanotron/examples/config_tiny_llama_wikitext.yaml<pre><code>general:\n  benchmark_csv_path: null\n  consumed_train_samples: null\n  ignore_sanity_checks: true\n  project: debug\n  run: tiny_llama_%date_%jobid\n  seed: 42\n  step: null\nmodel:\n  ddp_bucket_cap_mb: 25\n  dtype: bfloat16\n  init_method:\n    std: 0.025\n  make_vocab_size_divisible_by: 1\n  model_config:\n    bos_token_id: 1\n    eos_token_id: 2\n    hidden_act: silu\n    hidden_size: 768\n    initializer_range: 0.02\n    intermediate_size: 1536\n    is_llama_config: true\n    max_position_embeddings: 512\n    num_attention_heads: 12\n    num_hidden_layers: 12\n    num_key_value_heads: 12\n    pad_token_id: null\n    pretraining_tp: 1\n    rms_norm_eps: 1.0e-05\n    rope_scaling: null\n    tie_word_embeddings: true\n    use_cache: true\n    vocab_size: 50257\noptimizer:\n  accumulate_grad_in_fp32: true\n  clip_grad: 1.0\n  learning_rate_scheduler:\n    learning_rate: 0.001\n    lr_decay_starting_step: null\n    lr_decay_steps: null\n    lr_decay_style: cosine\n    lr_warmup_steps: 150 # 10% of the total steps\n    lr_warmup_style: linear\n    min_decay_lr: 0.00001\n  optimizer_factory:\n    adam_beta1: 0.9\n    adam_beta2: 0.95\n    adam_eps: 1.0e-08\n    name: adamW\n    torch_adam_is_fused: true\n  weight_decay: 0.01\n  zero_stage: 1\nparallelism:\n  dp: 2\n  expert_parallel_size: 1\n  pp: 1\n  pp_engine: 1f1b\n  tp: 4\n  tp_linear_async_communication: true\n  tp_mode: reduce_scatter\ndata_stages:\n  - name: stable training stage\n    start_training_step: 1\n    data:\n      dataset:\n        dataset_overwrite_cache: false\n        dataset_processing_num_proc_per_process: 32\n        hf_dataset_config_name: null\n        hf_dataset_or_datasets: wikitext\n        hf_dataset_splits: train\n        text_column_name: text\n        hf_dataset_config_name: wikitext-103-v1\n      num_loading_workers: 1\n      seed: 42\nlighteval: null\ntokenizer:\n  tokenizer_max_length: null\n  tokenizer_name_or_path: gpt2\n  tokenizer_revision: null\ntokens:\n  batch_accumulation_per_replica: 1\n  limit_test_batches: 0\n  limit_val_batches: 0\n  micro_batch_size: 64\n  sequence_length: 512\n  train_steps: 1500\n  val_check_interval: -1\ncheckpoints:\n  checkpoint_interval: 1500\n  checkpoints_path: checkpoints\n  checkpoints_path_is_shared_file_system: false\n  resume_checkpoint_path: checkpoints\n  save_initial_state: false\nprofiler: null\nlogging:\n  iteration_step_info_interval: 1\n  log_level: info\n  log_level_replica: info\n</code></pre> <p>This configuration file will train, as a proof of concept, a GPT-2-like (109M parameters) Llama model with approximately 100M tokens of wikitext with 4-way tensor parallelism (<code>tp</code>), 2-way data-parallelism (<code>dp</code>) and no pipeline-parallelism (<code>pp</code>). As a consequence, two GH200 nodes are required to train the model. The training job will require approximately 10 minutes to run.</p> <p>Now, create a submission script in <code>$SCRATCH/tutorials/nanotron-pretrain/nanotron/run_tiny_llama.sh</code> with the following content:</p> $SCRATCH/tutorials/nanotron-pretrain/run_tiny_llama.sh<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=pretrain-nanotron      # create a short name for your job\n#SBATCH --time=00:45:00\n#SBATCH --nodes=2                # total number of nodes\n#SBATCH --ntasks-per-node=1      # total number of tasks per node\n#SBATCH --gpus-per-task=4\n#SBATCH --output=logs/slurm-%x-%j.log  # if #SBATCH --error=... is not specified,\n                                 # this will also contain stderr (error messages)\n\n# Initialization.\nset -x\ncat $0\nexport HF_HOME=$SCRATCH/huggingface # (1)!\nexport CUDA_DEVICE_MAX_CONNECTIONS=1 #(2)!\n\nexport WANDB_API_KEY=&lt;api key&gt; # alternatively: export WANDB_MODE=offline\n\n# Run main script.\nsrun -ul --environment=./ngc-nanotron-24.04.toml bash -c \"\n  # activate virtual environment\n  source venv-24.04/bin/activate\n\n  # change cwd and run the training script\n  cd nanotron/\n\n  TORCHRUN_ARGS=\\\"\n   --master-addr=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n   --master-port=29500 \\\n   --node-rank=\\${SLURM_PROCID} \\\n   --nnodes=\\${SLURM_NNODES} \\\n   --nproc-per-node=\\${SLURM_GPUS_ON_NODE} \\\n  \\\"\n\n  python -m torch.distributed.run \\${TORCHRUN_ARGS} \\\n    run_train.py --config-file examples/config_tiny_llama_wikitext.yaml\n\" # (3)!\n</code></pre> <ol> <li>Location for locally stored data (incl. token and cache for models/datasets/spaces if <code>HF_HUB_CACHE</code> is not set) from <code>huggingface_hub</code> (cf. HuggingFace docs.</li> <li>This setting is specifically required by nanotron. Note that this setting can lead to faulty Nsight Systems (<code>nsys</code>) profiles that do not show overlap of compute and communication when there actually is (e.g. observed in this issue). The solution is to use a more recent version of <code>nsys</code>.</li> <li>Use <code>python -m torch.distributed.run</code> instead of <code>torchrun</code> with virtual environments.</li> </ol> <p>A few comments</p> <ul> <li>The parts outside the srun command will be run on the first node of the Slurm allocation for this job. srun commands without further specifiers execute with the settings of the sbatch script (i.e. using all nodes allocated to the job). </li> <li>Note that we are setting <code>HF_HOME</code> to a directory in scratch. This is done to place the dataset downloaded from <code>huggingface_hub</code> in your scratch, instead of your home directory. The same applies to your HuggingFace token as well as any models/spaces unless <code>HF_HUB_CACHE</code> is set (cf. HuggingFace docs). As discussed in the tutorial on LLM Inference, it is good practice to apply the recommended LUSTRE settings there.</li> <li>If instead of downloading a dataset from HuggingFace you want to re-use one managed by a colleague, please refer to the storage guide for instructions on dataset sharing.</li> <li>If you have a wandb API key and want to synchronize the training run, be sure to set the <code>WANDB_API_KEY</code> variable. Alternatively, <code>wandb</code> can write log data to the distributed filesystem with <code>WANDB_MODE=of\u200bf\u200bline</code> so that it can be uploaded with <code>wandb sync</code> (cf. Weights &amp; Biases docs) after the training run has finished.</li> </ul> <p><code>torchrun</code> with virtual environments</p> <p>When using a virtual environment on top of a base image with PyTorch, always replace <code>torchrun</code> with <code>python -m torch.distributed.run</code> to pick up the correct Python environment. Otherwise, the system Python environment will be used and virtual environment packages not available. If not using virtual environments such as with a self-contained PyTorch container, <code>torchrun</code> is equivalent to <code>python -m torch.distributed.run</code>.</p> <p>Using srun instead of <code>torchrun</code></p> <p>In many cases, workloads launched with <code>torchrun</code> can equivalently be launched purely with SLURM by setting some extra environment variables for <code>torch.distributed</code>. This simplifies the overall setup. That is, the <code>srun</code> statement in the above <code>sbatch</code> script can be rewritten as</p> $SCRATCH/tutorials/nanotron-pretrain/run_tiny_llama.sh<pre><code>#!/bin/bash\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --job-name=pretrain-nanotron      # create a short name for your job\n#SBATCH --time=00:45:00\n#SBATCH --nodes=2                # total number of nodes\n#SBATCH --ntasks-per-node=4      # total number of tasks per node\n#SBATCH --output=logs/slurm-%x-%j.log  # if #SBATCH --error=... is not specified,\n                                # this will also contain stderr (error messages)\n\n# Initialization.\nset -x\ncat $0\nexport HF_HOME=$SCRATCH/huggingface\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nexport WANDB_API_KEY=&lt;api key&gt; # alternatively: export WANDB_MODE=offline\n\n# Run main script.\nsrun -ul --environment=./ngc-nanotron-24.04.toml bash -c \"\n  # activate virtual environment\n  source venv-24.04/bin/activate\n\n  # change cwd and run the training script\n  cd nanotron/\n\n  MASTER_ADDR=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n  MASTER_PORT=29500 \\\n  RANK=\\${SLURM_PROCID} \\\n  LOCAL_RANK=\\${SLURM_LOCALID} \\\n  WORLD_SIZE=\\${SLURM_NTASKS} \\\n  python run_train.py --config-file examples/config_tiny_llama_wikitext.yaml\n\"\n</code></pre> <p>Note that, the quoted block inside the <code>srun</code> command gets executed by each task separately, i.e. 4 times per node, but all tasks on a node share the same container. This is different to the setup with <code>torchrun</code> where only one task executes the lines above the final <code>python</code> command. This is important to be aware of in order to avoid any accidental race condition (e.g. by writing to the container filesystem in one of these lines).</p>"},{"location":"tutorials/ml/llm-nanotron-training/#launch-a-training-job","title":"Launch a Training Job","text":"<p>Run:</p> <pre><code>[clariden-lnXXX]$ sbatch run_tiny_llama.sh\n</code></pre> <p>You can inspect if your job has been submitted successfully by running <code>squeue --me</code> and looking for your username. Once the run starts, there will be a new file under <code>logs/</code>. You can inspect the status of your run using:</p> <pre><code>[clariden-lnXXX]$ tail -f logs/&lt;logfile&gt;\n</code></pre> <p>In the end, the checkpoints of the model will be saved in <code>checkpoints/</code>.</p> <p>Core dump files</p> <p>In case the application crashes, it may leave behind large core dump files that contain an image of the process memory at the time of the crash. While these can be useful for debugging the reason of a specific crash (by e.g. loading them with <code>cuda-gdb</code> and looking at the stack trace with <code>bt</code>), they may accumulate over time and occupy a large space on the filesystem. For this reason, it can be useful to disable their creation by adding the line</p> <pre><code>ulimit -c 0\n</code></pre> <p>to the sbatch script.</p>"}]}